#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsbook
\begin_preamble
\usepackage{etoolbox}% http://ctan.org/pkg/etoolbox
\makeatletter
\let\old@start@align\start@align
\def\start@align{\setcounter{equation}{0}\old@start@align}
\makeatother
\allowdisplaybreaks
\newref{cor}{name=corollary~}
\newref{exa}{name=example~}
\usepackage{hyperref}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
theorems-chap
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, citecolor=blue, urlcolor=blue, plainpages=false, pdfstartview=XYZ, pdfpagelabels"
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Math 503, Spring 2015
\end_layout

\begin_layout Author
Lecture Notes
\end_layout

\begin_layout Email
\begin_inset CommandInset href
LatexCommand href
name "sdw62@georgetown.edu"
target "sdw62@georgetown.edu"
type "mailto:"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Background material
\end_layout

\begin_layout Section
Upper bounds and suprema
\end_layout

\begin_layout Standard
This section is drawn from 
\shape italic
Analysis: with an introduction to proof
\shape default
 (4th ed.) by Steven R.
 Lay.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $x,y\in\mathbb{R}$
\end_inset

 such that 
\begin_inset Formula $x\leq y+\epsilon$
\end_inset

 for every 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Then 
\begin_inset Formula $x\leq y$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "thm:inequality-of-reals"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $S$
\end_inset

 be a subset of 
\begin_inset Formula $\mathbb{R}$
\end_inset

.
 If there exists a real number 
\begin_inset Formula $m$
\end_inset

 such that 
\begin_inset Formula $m\geq s$
\end_inset

 for all 
\begin_inset Formula $s\in S$
\end_inset

, then 
\begin_inset Formula $m$
\end_inset

 is called an 
\shape italic
upper bound
\shape default
 for 
\begin_inset Formula $S$
\end_inset

, and we say that 
\begin_inset Formula $S$
\end_inset

 is bounded above.
 If 
\begin_inset Formula $m\leq s$
\end_inset

 for all 
\begin_inset Formula $s\in S$
\end_inset

, then 
\begin_inset Formula $m$
\end_inset

 is a 
\shape italic
lower bound
\shape default
 for 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 is bounded below.
 The set 
\begin_inset Formula $S$
\end_inset

 is said to be 
\shape italic
bounded
\shape default
 if it is bounded above and bounded below.
\end_layout

\begin_layout Definition
If an upper bound 
\begin_inset Formula $m$
\end_inset

 for 
\begin_inset Formula $S$
\end_inset

 is a member of 
\begin_inset Formula $S$
\end_inset

, then 
\begin_inset Formula $m$
\end_inset

 is called the 
\shape italic
maximum
\shape default
 (or largest element) of 
\begin_inset Formula $S$
\end_inset

, and we write
\begin_inset Formula 
\[
m=\max S.
\]

\end_inset

Similarly, if a lower bound of 
\begin_inset Formula $S$
\end_inset

 is a member of 
\begin_inset Formula $S$
\end_inset

, then it is called the 
\shape italic
minimum
\shape default
 (or least element) of 
\begin_inset Formula $S$
\end_inset

, denoted by 
\begin_inset Formula $\min S$
\end_inset

.
\end_layout

\begin_layout Definition
A set may have upper or lower bounds, or it may have neither.
 If 
\begin_inset Formula $m$
\end_inset

 is an upper bound for 
\begin_inset Formula $S$
\end_inset

, then any number greater than 
\begin_inset Formula $m$
\end_inset

 is also an upper bound.
 While a set may have many upper and lower bounds, if it has a maximum or
 a minimum, then those values are unique.
 Thus we speak of 
\shape italic
an
\shape default
 upper bound and 
\shape italic
the
\shape default
 maximum.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty subset of 
\begin_inset Formula $\mathbb{R}$
\end_inset

.
 If 
\begin_inset Formula $S$
\end_inset

 is bounded above, then the least upper bound of 
\begin_inset Formula $S$
\end_inset

 is called its 
\shape italic
supremum
\shape default
 and is denoted by 
\begin_inset Formula $\sup S$
\end_inset

.
 Thus 
\begin_inset Formula $m=\sup S$
\end_inset

 iff
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\backslash
setcounter{enumi}{1}
\end_layout

\end_inset


\begin_inset Formula $m\geq s$
\end_inset

, for all 
\begin_inset Formula $s\in S$
\end_inset

, and
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

if 
\begin_inset Formula $m'<m$
\end_inset

, then there exists 
\begin_inset Formula $s'\in S$
\end_inset

 such that 
\begin_inset Formula $s'>m'$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Definition
If 
\begin_inset Formula $S$
\end_inset

 is bounded below, then the greatest lower bound of 
\begin_inset Formula $S$
\end_inset

 is called its 
\shape italic
infimum
\shape default
 and is denoted by 
\begin_inset Formula $\inf S$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition
\begin_inset ERT
status open

\begin_layout Plain Layout

[completeness axiom]
\end_layout

\end_inset

Every nonempty subset 
\begin_inset Formula $S$
\end_inset

 of 
\begin_inset Formula $\mathbb{R}$
\end_inset

 that is bounded above has a least upper bound.
 That is, 
\begin_inset Formula $\sup S$
\end_inset

 exists and is a real number.
\end_layout

\begin_layout Theorem
Given nonempty subsets 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 or 
\begin_inset Formula $\mathbb{R}$
\end_inset

, let 
\begin_inset Formula $C$
\end_inset

 denote the set
\begin_inset Formula 
\[
C=\left\{ x+y:x\in A\mbox{ and }y\in B\right\} .
\]

\end_inset

If 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 have suprema, then 
\begin_inset Formula $C$
\end_inset

 has a supremum and 
\begin_inset Formula 
\[
\sup C=\sup A+\sup B.
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\sup A=a$
\end_inset

 and 
\begin_inset Formula $\sup B=b$
\end_inset

.
 If 
\begin_inset Formula $z\in C$
\end_inset

, then 
\begin_inset Formula $z=x+y$
\end_inset

 for some 
\begin_inset Formula $x\in A$
\end_inset

 and 
\begin_inset Formula $y\in B$
\end_inset

.
 Thus 
\begin_inset Formula $z=x+y\leq a+b$
\end_inset

, so 
\begin_inset Formula $a+b$
\end_inset

 is an upper bound for 
\begin_inset Formula $C$
\end_inset

.
 By the completeness axiom, 
\begin_inset Formula $C$
\end_inset

 has at least an upper bound, say 
\begin_inset Formula $\sup C=c$
\end_inset

.
 We must show that 
\begin_inset Formula $c=a+b$
\end_inset

.
 Since 
\begin_inset Formula $c$
\end_inset

 is the 
\shape italic
least
\shape default
 upper bound for 
\begin_inset Formula $C$
\end_inset

, we have 
\begin_inset Formula $c\leq a+b$
\end_inset

.
\end_layout

\begin_layout Proof
To see that 
\begin_inset Formula $a+b\leq c$
\end_inset

, choose any 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Since 
\begin_inset Formula $a=\sup A$
\end_inset

, 
\begin_inset Formula $a-\epsilon$
\end_inset

 is not an upper bound for 
\begin_inset Formula $A$
\end_inset

, and there must exist 
\begin_inset Formula $x\in A$
\end_inset

 such that 
\begin_inset Formula $a-\epsilon<x$
\end_inset

.
 Similarly, since 
\begin_inset Formula $b=\sup B$
\end_inset

, there exists 
\begin_inset Formula $y\in B$
\end_inset

 such that 
\begin_inset Formula $b-\epsilon<y$
\end_inset

.
 Combining these inequalities, we have
\begin_inset Formula 
\[
a+b-2\epsilon<x+y\leq c.
\]

\end_inset

That is, 
\begin_inset Formula $a+b<c+2\epsilon$
\end_inset

 for every 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Thus, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:inequality-of-reals"

\end_inset

, 
\begin_inset Formula $a+b\leq c$
\end_inset

.
 Finally, since 
\begin_inset Formula $c\leq a+b$
\end_inset

 and 
\begin_inset Formula $c\geq a+b$
\end_inset

, we conclude that 
\begin_inset Formula $c=a+b$
\end_inset

.
\end_layout

\begin_layout Section
Univariate transformations
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X$
\end_inset

 have cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

, let 
\begin_inset Formula $Y=g\left(X\right)$
\end_inset

, let 
\begin_inset Formula $\mathcal{X}=\left\{ x:f_{X}\left(x\right)>0\right\} $
\end_inset

, and let 
\begin_inset Formula $\mathcal{Y}=\left\{ y:y=g\left(x\right),x\in\mathcal{X}\right\} $
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $g$
\end_inset

 is an increasing function on 
\begin_inset Formula $\mathcal{X}$
\end_inset

, 
\begin_inset Formula $F_{Y}\left(y\right)=F_{X}\left(g^{-1}\left(y\right)\right)$
\end_inset

 for 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $g$
\end_inset

 is a decreasing function on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is a continuous random variable, 
\begin_inset Formula $F_{Y}\left(y\right)=1-F_{X}\left(g^{-1}\left(y\right)\right)$
\end_inset

 for 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 2.1.3 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:cdf-of-function-of-rv"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X$
\end_inset

 have pdf 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

 and let 
\begin_inset Formula $Y=g\left(X\right)$
\end_inset

, where 
\begin_inset Formula $g$
\end_inset

 is a monotone function.
 Let 
\begin_inset Formula $\mathcal{X}=\left\{ x:f_{X}\left(x\right)>0\right\} $
\end_inset

 and let 
\begin_inset Formula $\mathcal{Y}=\left\{ y:y=g\left(x\right),x\in\mathcal{X}\right\} $
\end_inset

.
 Suppose that 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

 is continuous on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and that 
\begin_inset Formula $g^{-1}\left(y\right)$
\end_inset

 has a continuous derivative on 
\begin_inset Formula $\mathcal{Y}$
\end_inset

.
 Then the pdf of 
\begin_inset Formula $Y$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{Y}\left(y\right)=\begin{cases}
f_{X}\left(g^{-1}\left(y\right)\right)\left|\dfrac{\mbox{d}}{\mbox{d}y}g^{-1}\left(y\right)\right|, & y\in\mathcal{Y}\\
0, & \mbox{otherwise}
\end{cases}.
\]

\end_inset

(This is Theorem 2.1.5 from Casella & Berger.)
\end_layout

\begin_deeper
\begin_layout Proof
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cdf-of-function-of-rv"

\end_inset

 and applying the chain rule, we have
\begin_inset Formula 
\begin{flalign*}
f_{Y}\left(y\right) & =\dfrac{\mbox{d}}{\mbox{d}y}F_{Y}\left(y\right)\\
 & =f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\mbox{d}}{\mbox{d}y}g^{-1}\left(y\right)
\end{flalign*}

\end_inset

in the case that 
\begin_inset Formula $g$
\end_inset

 is increasing and
\begin_inset Formula 
\begin{flalign*}
f_{Y}\left(y\right) & =\dfrac{\mbox{d}}{\mbox{d}y}F_{Y}\left(y\right)\\
 & =0-f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\mbox{d}}{\mbox{d}y}g^{-1}\left(y\right)\\
 & =-f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\mbox{d}}{\mbox{d}y}g^{-1}\left(y\right)
\end{flalign*}

\end_inset

in the case that 
\begin_inset Formula $g$
\end_inset

 is decreasing, which can be expressed concisely as in the theorem.
\end_layout

\end_deeper
\begin_layout Section
Sums of random variables from a random sample
\end_layout

\begin_layout Definition
The 
\shape italic
sample variance
\shape default
 is the statistic defined by
\begin_inset Formula 
\[
S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}.
\]

\end_inset

The 
\shape italic
sample standard deviation
\shape default
 is the statistic defined by 
\begin_inset Formula $S=\sqrt{S^{2}}$
\end_inset

.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 be any numbers and 
\begin_inset Formula $\bar{x}=\left(x_{1}+\cdots+x_{n}\right)/n$
\end_inset

.
 Then
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset


\begin_inset Formula $\min_{a}\sum_{i=1}^{n}\left(x_{i}-a\right)^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset


\begin_inset Formula $\left(n-1\right)s^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 5.2.4 from Casella & Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:computing-sums-rand-samples"

\end_inset


\end_layout

\begin_layout Proof
To prove part (a), add and subtract 
\begin_inset Formula $\bar{x}$
\end_inset

 to get
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}\left(x_{i}-a\right)^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}+\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)\right]\left[\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)\right]\\
 & =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)+\left(\bar{x}-a\right)\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)^{2}\right]\\
 & =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+2\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)+\left(\bar{x}-a\right)^{2}\right]\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left[2\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)\right]+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\sum_{i=1}^{n}\left(x_{i}\bar{x}-ax_{i}-\bar{x}^{2}+a\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}\sum_{i=1}^{n}x_{i}-a\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}\bar{x}^{2}+\sum_{i=1}^{n}a\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}\left(n\bar{x}\right)-a\left(n\bar{x}\right)-n\bar{x}^{2}+na\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(n\bar{x}^{2}-na\bar{x}-n\bar{x}^{2}+na\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(0\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}.
\end{flalign*}

\end_inset

It is now clear that the right-hand side is minimized at 
\begin_inset Formula $a=\bar{x}$
\end_inset

.
 To prove part (b), take 
\begin_inset Formula $a=0$
\end_inset

 in the above, i.e.,
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}\left(x_{i}-a\right)^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
\Leftrightarrow\sum_{i=1}^{n}\left(x_{i}-0\right)^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-0\right)^{2}\\
\Leftrightarrow\sum_{i=1}^{n}x_{i}^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\bar{x}^{2}\\
\Leftrightarrow\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}\bar{x}^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\\
\Leftrightarrow\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.
\end{flalign*}

\end_inset

The sample variance is defined as 
\begin_inset Formula 
\begin{flalign*}
s^{2} & =\frac{1}{n-1}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\\
\Leftrightarrow\left(n-1\right)s^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2},
\end{flalign*}

\end_inset

so the final equality of part (b) holds.
\end_layout

\begin_layout Section
Order statistics
\end_layout

\begin_layout Standard
The order statistics of a random sample 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are the sample values placed in ascending order.
 They are denoted by 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

.
 The order statistics are random variables that satisfy 
\begin_inset Formula $X_{\left(1\right)}\leq\ldots\leq X_{\left(n\right)}$
\end_inset

, and in particular, 
\begin_inset Formula $X_{\left(1\right)}=\underset{1\leq i\leq n}{\min}X_{i}$
\end_inset

 and 
\begin_inset Formula $X_{\left(n\right)}=\underset{1\leq i\leq n}{\max}X_{i}$
\end_inset

.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from a discrete distribution with pmf 
\begin_inset Formula $f_{X}\left(x_{i}\right)=p_{i}$
\end_inset

, where 
\begin_inset Formula $x_{1}<x_{2}<\cdots$
\end_inset

 are the possible values of 
\begin_inset Formula $X$
\end_inset

 in ascending order.
 Define 
\begin_inset Formula 
\begin{flalign*}
P_{0} & =0\\
P_{1} & =p_{1}\\
P_{2} & =p_{1}+p_{2}\\
 & \vdots\\
P_{i} & =p_{1}+p_{2}+\cdots+p_{i}\\
 & \vdots
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

 denote the order statistics from the sample.
 Then
\begin_inset Formula 
\[
P\left(\left\{ X_{\left(j\right)}\leq x_{i}\right\} \right)=\sum_{k=j}^{n}\binom{n}{k}P_{i}^{k}\left(1-P_{i}\right)^{n-k}
\]

\end_inset

and
\begin_inset Formula 
\[
P\left(\left\{ X_{\left(j\right)}=x_{i}\right\} \right)=\sum_{k=j}^{n}\binom{n}{k}\left[P_{i}^{k}\left(1-P_{i}\right)^{n-k}-P_{i-1}^{k}\left(1-P_{i-1}\right)^{n-k}\right].
\]

\end_inset

(This is Theorem 5.4.3 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:order-stat-discrete"

\end_inset


\end_layout

\begin_layout Proof
[add the proof]
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

 denote the order statistics of a random sample, 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

, from a continuous population with cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

 and pdf 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

.
 Then the pdf of 
\begin_inset Formula $X_{\left(j\right)}$
\end_inset

 is
\begin_inset Formula 
\[
f_{X_{\left(j\right)}}\left(x\right)=\frac{n!}{\left(j-1\right)!\left(n-j\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{j-1}\left[1-F_{X}\left(x\right)\right]^{n-j}.
\]

\end_inset

(This is Theorem 5.4.4 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:order-stat-continuous"

\end_inset


\end_layout

\begin_layout Proof
[add the proof]
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

 denote the order statistics of a random sample, 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

, from a continuous population with cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

 and pdf 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

.
 Then the joint pdf of 
\begin_inset Formula $X_{\left(i\right)}$
\end_inset

 and 
\begin_inset Formula $X_{\left(j\right)}$
\end_inset

, 
\begin_inset Formula $1\leq i<j\leq n$
\end_inset

, is
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(i\right)},X_{\left(j\right)}}\left(u,v\right) & =\frac{n!}{\left(i-1\right)!\left(j-1-i\right)!\left(n-j\right)!}f_{X}\left(u\right)f_{X}\left(v\right)\left[F_{X}\left(u\right)\right]^{i-1}\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{j-1-i}\left[1-F_{X}\left(v\right)\right]^{n-j}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $-\infty<u<v<\infty$
\end_inset

.
 (This is Theorem 5.4.6 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:order-stat-joint-pdf"

\end_inset


\end_layout

\begin_layout Proof
[add the proof]
\end_layout

\begin_layout Section
Covariance and correlation
\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are any two random variables and 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are any two constants, then
\begin_inset Formula 
\[
\Var\left(aX+bY\right)=a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right)+2ab\Cov\left(X,Y\right).
\]

\end_inset

If 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent random variables, then
\begin_inset Formula 
\[
\Var\left(aX+bY\right)=a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right).
\]

\end_inset

(This is Theorem 4.5.6 from Casella & Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:variance-of-ind-rvs"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Chapter
Common families of distributions
\end_layout

\begin_layout Section
Exponential families
\end_layout

\begin_layout Standard
A family of pdfs (or pmfs) indexed by a parameter 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is called a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family if it can be expressed as 
\begin_inset Formula 
\[
f\left(x|\boldsymbol{\theta}\right)=h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)\right\} 
\]

\end_inset

where 
\begin_inset Formula $h\left(x\right)\geq0$
\end_inset

, 
\begin_inset Formula $c\left(\boldsymbol{\theta}\right)\geq0$
\end_inset

, 
\begin_inset Formula $t_{1}\left(x\right),\ldots,t_{k}\left(x\right)$
\end_inset

 are real-valued functions of 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset Formula $\omega_{1}\left(\boldsymbol{\theta}\right),\ldots,\omega_{k}\left(\boldsymbol{\theta}\right)$
\end_inset

 are real-valued functions of the possibly vector-valued parameter 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 I.e., 
\begin_inset Formula $f\left(x|\boldsymbol{\theta}\right)$
\end_inset

 can be expressed in three parts: a part that depends only on the random
 variable(s), a part that depends only on the parameter(s), and a part that
 depends on both the random variable(s) and the parameter(s).
 Most of the parametric models you have studied in Math-501 are exponential
 families, e.g., normal, gamma, beta, binomial, negative binomial, Poisson,
 and multinomial.
 The uniform distribution is not an exponential family (see example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:exp-family-uniform"

\end_inset

 below).
\begin_inset CommandInset label
LatexCommand label
name "sec:defn-exp-family"

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Logistic regression]
\end_layout

\end_inset

For 
\begin_inset Formula $y_{1},y_{2},\ldots,y_{n}$
\end_inset

, let 
\begin_inset Formula $y_{i}\sim\text{Bernoulli}\left(p\right)$
\end_inset

, i.e., 
\begin_inset Formula 
\[
y_{i}=\begin{cases}
0, & \mbox{if no event}\\
1, & \mbox{if event.}
\end{cases}
\]

\end_inset

Then the logistic regression model is
\begin_inset Formula 
\[
\log\left(\frac{p}{1-p}\right)=\beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}
\]

\end_inset

where 
\begin_inset Formula $\log\left(p/\left(1-p\right)\right)$
\end_inset

 is called the logit link.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Binomial random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim B\left(n,p\right)$
\end_inset

, where 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

.
 Recall that 
\begin_inset Formula $X$
\end_inset

 represents the number of successes in 
\begin_inset Formula $n$
\end_inset

 i.i.d.
 Bernoulli trials and its pmf is given by 
\begin_inset Formula 
\begin{flalign*}
f\left(x|p\right) & =\binom{n}{x}p^{x}\left(1-p\right)^{n-x}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x=0,1,\ldots,n$
\end_inset

 and 
\begin_inset Formula $f\left(x|p\right)=0$
\end_inset

 otherwise.
 Express 
\begin_inset Formula $f\left(x|p\right)$
\end_inset

 in exponential family form.
\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-binomial"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{flalign*}
f\left(x|p\right) & =\binom{n}{x}p^{x}\left(1-p\right)^{n-x}\\
 & =\binom{n}{x}p^{x}\left(1-p\right)^{n}\left(1-p\right)^{-x}\\
 & =\binom{n}{x}\left(1-p\right)^{n}\left(\frac{p^{x}}{\left(1-p\right)^{x}}\right)\\
 & =\binom{n}{x}\left(1-p\right)^{n}\left(\frac{p}{1-p}\right)^{x}\\
 & =\binom{n}{x}\left(1-p\right)^{n}\exp\left\{ \log\left(\frac{p}{1-p}\right)^{x}\right\} \\
 & =\underbrace{\binom{n}{x}}_{h\left(x\right)}\underbrace{\left(1-p\right)^{n}}_{c\left(p\right)}\exp\left\{ \underbrace{x}_{t_{1}\left(x\right)}\underbrace{\log\left(\frac{p}{1-p}\right)}_{\omega_{1}\left(p\right)}\right\} 
\end{flalign*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim P\left(\lambda\right)$
\end_inset

, where 
\begin_inset Formula $\lambda>0$
\end_inset

.
 Recall that 
\begin_inset Formula $X$
\end_inset

 represents the frequency with which a specified event occurs given some
 fixed dimension, such as space or time, and its pmf is given by
\begin_inset Formula 
\[
f\left(x|\lambda\right)=\frac{e^{-\lambda}\lambda^{x}}{x!}
\]

\end_inset

for 
\begin_inset Formula $x=0,1,2,\ldots$
\end_inset

 and 
\begin_inset Formula $f\left(x|\lambda\right)=0$
\end_inset

 otherwise.
 Express 
\begin_inset Formula $f\left(x|\lambda\right)$
\end_inset

 in exponential family form.
\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-poisson"

\end_inset


\begin_inset Formula 
\begin{flalign*}
f\left(x|\lambda\right) & =\frac{e^{-\lambda}\lambda^{x}}{x!}\\
 & =\frac{1}{x!}e^{-\lambda}\exp\left\{ \log\left(\lambda^{x}\right)\right\} \\
 & =\frac{1}{x!}e^{-\lambda}\exp\left\{ x\log\lambda\right\} 
\end{flalign*}

\end_inset

Then, we have 
\begin_inset Formula $h\left(x\right)=1/x!$
\end_inset

, 
\begin_inset Formula $c\left(\lambda\right)=e^{-\lambda}$
\end_inset

, 
\begin_inset Formula $t_{1}\left(x\right)=x$
\end_inset

, and 
\begin_inset Formula $\omega_{1}\left(\lambda\right)=\log\lambda$
\end_inset

.
 In a Poisson regression, we have 
\begin_inset Formula $\log\left(\lambda\right)=\beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim N\left(\mu,\sigma^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\mu\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $\sigma>0$
\end_inset

.
 A pdf for 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right\} 
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

.
 Express 
\begin_inset Formula $f\left(x|\mu,\sigma^{2}\right)$
\end_inset

 in exponential family form.
\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-normal"

\end_inset


\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $\sigma$
\end_inset

 is known.
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu\right) & =\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{x^{2}-2\mu x+\mu^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{x^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{-2\mu x}{2\sigma^{2}}\right\} \\
 & =\underbrace{\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{x^{2}}{2\sigma^{2}}\right\} }_{h\left(x\right)}\underbrace{\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} }_{c\left(\mu\right)}\exp\left\{ \underbrace{\frac{\mu}{\sigma^{2}}}_{\omega_{1}\left(\mu\right)}\cdot\underbrace{x}_{t_{1}\left(x\right)}\right\} 
\end{flalign*}

\end_inset

Suppose 
\begin_inset Formula $\sigma$
\end_inset

 is unknown.
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{\sqrt{2\pi}}\left(\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{x^{2}-2\mu x+\mu^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{\sqrt{2\pi}}\exp\left\{ \log\left(\sigma^{2}\right)^{-1/2}\right\} \exp\left\{ -\frac{x^{2}-2\mu x}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} \\
 & =\underbrace{\frac{1}{\sqrt{2\pi}}}_{h\left(x\right)}\underbrace{\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log\sigma^{2}\right\} }_{c\left(\mu,\sigma^{2}\right)}\exp\left\{ \underbrace{\frac{1}{\sigma^{2}}}_{\omega_{1}\left(\mu,\sigma^{2}\right)}\cdot\underbrace{\left(-\frac{x^{2}}{2}\right)}_{t_{1}\left(x\right)}+\underbrace{\frac{\mu}{\sigma^{2}}}_{\omega_{2}\left(\mu,\sigma^{2}\right)}\cdot\underbrace{x}_{t_{2}\left(x\right)}\right\} 
\end{flalign*}

\end_inset

Thus, in the case that 
\begin_inset Formula $\sigma$
\end_inset

 is unknown, 
\begin_inset Formula $f\left(x|\mu,\sigma^{2}\right)$
\end_inset

 is a two-parameter exponential family, i.e., we have 
\begin_inset Formula $k=2$
\end_inset

 for 
\begin_inset Formula $\sum_{j=1}^{k}\omega_{j}\left(\theta\right)t_{j}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Definition
The 
\shape italic
indicator function
\shape default
 of a set 
\begin_inset Formula $A$
\end_inset

, most often denoted by 
\begin_inset Formula $I_{A}\left(x\right)$
\end_inset

, is the function
\begin_inset Formula 
\[
I_{A}\left(x\right)=\begin{cases}
1, & x\in A\\
0, & x\notin A
\end{cases}.
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim U\left(0,\theta\right)$
\end_inset

, where 
\begin_inset Formula $\theta>0$
\end_inset

.
 A pdf for 
\begin_inset Formula $X$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{1}{\theta-0}\\
 & =\frac{1}{\theta}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $0<x<\theta$
\end_inset

.
 Express 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 in exponential family form, if possible.
\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-uniform"

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $A=\left\{ x:x\in\left(0,\theta\right)\right\} $
\end_inset

 and let 
\begin_inset Formula $I_{A}$
\end_inset

 be the indicator function of 
\begin_inset Formula $A$
\end_inset

, i.e.,
\begin_inset Formula 
\[
I_{A}\left(x\right)=\begin{cases}
1, & \mbox{if }x\in A\\
0, & \mbox{if }x\notin A
\end{cases}.
\]

\end_inset

Then, we can write 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{1}{\theta}I_{A}\left(x\right)\\
 & =\frac{1}{\theta}I_{\left(0,\theta\right)}\left(x\right).
\end{flalign*}

\end_inset

Notice that 
\begin_inset Formula $I_{\left(0,\theta\right)}\left(x\right)$
\end_inset

 is not a function of 
\begin_inset Formula $x$
\end_inset

 exclusively, not a function of 
\begin_inset Formula $\theta$
\end_inset

 exclusively, and cannot be written as an exponential.
 Because the entire pdf must be incorporated into 
\begin_inset Formula $h\left(x\right)$
\end_inset

, 
\begin_inset Formula $c\left(\theta\right)$
\end_inset

, 
\begin_inset Formula $t_{j}\left(x\right)$
\end_inset

, and 
\begin_inset Formula $\omega_{j}\left(\theta\right)$
\end_inset

, it follows that the familiy of pdfs given by 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 is not an exponential family.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Three-parameter exponential family distribution]
\end_layout

\end_inset

Consider the family of distributions with densities
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

.
 Express 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 in exponential family form.
\begin_inset CommandInset label
LatexCommand label
name "exa:3-param-exp-family"

\end_inset


\end_layout

\begin_layout Example
Recall that the binomial theorem states that
\begin_inset Formula 
\begin{flalign*}
\left(x+y\right)^{n} & =\sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k},
\end{flalign*}

\end_inset

so we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]\\
 & =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\sum_{k=0}^{4}\binom{4}{k}x^{k}\left(-\theta\right)^{4-k}\right\} \\
 & =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\left[\binom{4}{0}x^{0}\left(-\theta\right)^{4}+\binom{4}{1}x\left(-\theta\right)^{3}+\binom{4}{2}x^{2}\left(-\theta\right)^{2}+\binom{4}{3}x^{3}\left(-\theta\right)+\binom{4}{4}x^{4}\left(-\theta\right)^{0}\right]\right\} \\
 & =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\left[1\cdot1\cdot\theta^{4}-4x\theta^{3}+6x^{2}\theta^{2}-4x^{3}\theta+1\cdot x^{4}\cdot1\right]\right\} \\
 & =\underbrace{\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -x^{4}\right\} }_{h\left(x\right)}\underbrace{\exp\left\{ -\theta^{4}\right\} }_{c\left(\theta\right)}\exp\left\{ \underbrace{4x^{3}}_{t_{1}\left(x\right)}\underbrace{\theta}_{\omega_{1}\left(\theta\right)}\underbrace{-6x^{2}}_{t_{2}\left(x\right)}\underbrace{\theta^{2}}_{\omega_{2}\left(\theta\right)}+\underbrace{4x}_{t_{3}\left(x\right)}\underbrace{\theta^{3}}_{\omega_{3}\left(\theta\right)}\right\} .
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
Random samples from 
\begin_inset Formula $k$
\end_inset

-parameter exponential families have joint distributions which are 
\begin_inset Formula $k$
\end_inset

-parameter exponential families.
\end_layout

\begin_layout Proof
Suppose that a random variable 
\begin_inset Formula $X$
\end_inset

 has a pdf 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

, and that 
\begin_inset Formula $X_{1},X_{2},\ldots,X_{n}$
\end_inset

 is a random sample from a population having the distribution of 
\begin_inset Formula $X$
\end_inset

.
 It follows that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are independent and identically distributed, and that each 
\begin_inset Formula $X_{i}$
\end_inset

 has the same cdf as 
\begin_inset Formula $X$
\end_inset

, and therefore that 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 is a pdf for each 
\begin_inset Formula $X_{i}$
\end_inset

.
 Then, the joint pdf of the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign}
f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right) & =\prod_{i=1}^{n}f\left(x_{i}|\theta\right)\label{eq:joint-distr-proof-step-1}\\
 & =\prod_{i=1}^{n}\left[h\left(x_{i}\right)c\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \right]\label{eq:joint-distr-proof-step-2}\\
 & =\left[\prod_{i=1}^{n}h\left(x_{i}\right)\right]\left[c\left(\theta\right)\right]^{n}\exp\left\{ \sum_{j=1}^{k}\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \label{eq:joint-distr-proof-step-3}
\end{flalign}

\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:joint-distr-proof-step-1"

\end_inset

 follows because the joint pdf of independent random variables is the product
 of their marginal pdfs.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:joint-distr-proof-step-2"

\end_inset

 follows because we are given that the pdf of 
\begin_inset Formula $X$
\end_inset

 is part of a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family, so it can be written in that form.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:joint-distr-proof-step-3"

\end_inset

 follows from algebra, and because the product of exponentials is an exponential
 raised to the sum of the individual powers, e.g., 
\begin_inset Formula $e^{x_{1}}\cdot e^{x_{2}}=e^{x_{1}+x_{2}}$
\end_inset

.
 Then, let 
\begin_inset Formula 
\begin{flalign*}
h^{*}\left(x\right) & =\prod_{i=1}^{n}h\left(x_{i}\right)\\
c^{*}\left(\theta\right) & =\left[c\left(\theta\right)\right]^{n},
\end{flalign*}

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right) & =\left[\prod_{i=1}^{n}h\left(x_{i}\right)\right]\left[c\left(\theta\right)\right]^{n}\exp\left\{ \sum_{j=1}^{k}\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \\
 & =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\left(\omega_{j}\left(\theta\right)\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\right)\right\} .
\end{flalign*}

\end_inset

Now, let 
\begin_inset Formula 
\begin{flalign*}
T_{j}\left(x\right) & =\sum_{i=1}^{n}t_{j}\left(x_{i}\right),
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right) & =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} .
\end{flalign*}

\end_inset

Thus, the joint pdf 
\begin_inset Formula $f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right)$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family.
\end_layout

\begin_layout Subsection
Natural parameters
\begin_inset CommandInset label
LatexCommand label
name "sec:Natural-parameters"

\end_inset


\end_layout

\begin_layout Standard
An exponential family is sometimes reparametrized as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right),
\end{flalign*}

\end_inset

where the natural parameters are defined by 
\begin_inset Formula $\eta_{j}=\omega_{j}\left(\theta\right)$
\end_inset

 and the natural parameter space is
\begin_inset Formula 
\[
\left\{ \eta=\left(\eta_{1},\ldots,\eta_{k}\right):\int h\left(x\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \mbox{d}x<\infty\right\} 
\]

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
c^{*}\left(\eta\right) & =\frac{1}{\int h\left(x\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\mbox{d}x\right\} },
\end{flalign*}

\end_inset

which ensures that the pdf integrates to 1.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Binomial random variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:natural-param-binomial"

\end_inset

Let 
\begin_inset Formula $X\sim B\left(n,p\right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-binomial"

\end_inset

, the pmf of 
\begin_inset Formula $X$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{flalign*}
f\left(x|p\right) & =\binom{n}{x}\left(1-p\right)^{n}\exp\left\{ x\log\left(\frac{p}{1-p}\right)\right\} ,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $k=1$
\end_inset

 and 
\begin_inset Formula 
\begin{flalign*}
\omega_{1}\left(p\right) & =\log\frac{p}{1-p}.
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula $\eta=\omega_{1}\left(p\right)$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
\eta & =\log\frac{p}{1-p}\\
e^{\eta} & =\exp\left(\log\frac{p}{1-p}\right)\\
e^{\eta} & =\frac{p}{1-p}\\
p & =e^{\eta}\left(1-p\right)\\
p & =e^{\eta}-e^{\eta}p\\
e^{\eta} & =p\left(1+e^{\eta}\right)\\
p & =\frac{e^{\eta}}{1+e^{\eta}}.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
c\left(p\right) & =\left(1-p\right)^{n}\\
c\left(\eta\right) & =\left(1-\frac{e^{\eta}}{1+e^{\eta}}\right)^{n}\\
 & =\left(\frac{1}{1+e^{\eta}}\right)^{n}
\end{flalign*}

\end_inset

and 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{n}{x}\left(\frac{1}{1+e^{\eta}}\right)^{n}\exp\left(x\eta\right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim P\left(\lambda\right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-poisson"

\end_inset

, the pmf of 
\begin_inset Formula $X$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\lambda\right) & =\frac{1}{x!}e^{-\lambda}\exp\left\{ x\log\lambda\right\} ,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $k=1$
\end_inset

 and
\begin_inset Formula 
\begin{flalign*}
\omega_{1}\left(\lambda\right) & =\log\lambda.
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula $\eta=\omega_{1}\left(\lambda\right)$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
\eta & =\log\lambda\\
\Leftrightarrow e^{\eta} & =\exp\left(\log\lambda\right)\\
\Leftrightarrow e^{\eta} & =\lambda.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
c\left(\lambda\right) & =e^{-\lambda}\\
c\left(\eta\right) & =\exp\left(-e^{\eta}\right)
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\frac{1}{x!}\exp\left(-e^{\eta}\right)\exp\left(x\eta\right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Bernoulli random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Bernoulli}\left(p\right)$
\end_inset

, i.e., 
\begin_inset Formula $X\sim B\left(1,p\right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:natural-param-binomial"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{n}{x}\left(\frac{1}{1+e^{\eta}}\right)^{n}\exp\left(x\eta\right).
\end{flalign*}

\end_inset

With 
\begin_inset Formula $n=1$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{1}{x}\left(\frac{1}{1+e^{\eta}}\right)\exp\left(x\eta\right)\\
 & =1\cdot\left(\frac{1}{1+e^{\eta}}\right)\exp\left(x\eta\right)\\
 & =\frac{1}{1+e^{\eta}}\exp\left(x\eta\right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim N\left(\mu,\sigma^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\sigma>0$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 is unknown.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-normal"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log\sigma^{2}\right\} \exp\left\{ -x^{2}\frac{1}{2\sigma^{2}}+x\frac{\mu}{\sigma^{2}}\right\} .
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula $\eta_{1}=\omega_{1}\left(\mu,\sigma^{2}\right)$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
\eta_{1} & =\frac{1}{\sigma^{2}}\\
\sigma^{2}\eta_{1} & =1\\
\sigma^{2} & =\frac{1}{\eta_{1}}
\end{flalign*}

\end_inset

and let 
\begin_inset Formula $\eta_{2}=\omega_{2}\left(\mu,\sigma^{2}\right)$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
\eta_{2} & =\frac{\mu}{\sigma^{2}}\\
\mu & =\sigma^{2}\eta_{2}\\
 & =\frac{\eta_{2}}{\eta_{1}}.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
c\left(\mu,\sigma^{2}\right) & =\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log\sigma^{2}\right\} \\
c^{*}\left(\eta_{1},\eta_{2}\right) & =\exp\left\{ -\frac{\left(\frac{\eta_{2}}{\eta_{1}}\right)^{2}}{2\left(\frac{1}{\eta_{1}}\right)}-\frac{1}{2}\log\frac{1}{\eta_{1}}\right\} \\
 & =\exp\left\{ -\frac{\frac{\eta_{2}^{2}}{\eta_{1}^{2}}}{\frac{2}{\eta_{1}}}-\frac{1}{2}\log\frac{1}{\eta_{1}}\right\} \\
 & =\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\left(\frac{1}{\eta_{1}}\right)^{-1/2}\right\} \\
 & =\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\left(\left(\eta_{1}\right)^{-1}\right)^{-1/2}\right\} \\
 & =\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\sqrt{\eta_{1}}\right\} 
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta_{1},\eta_{2}\right) & =\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\sqrt{\eta_{1}}\right\} \exp\left\{ -\frac{\eta_{1}x^{2}}{2}+\eta_{2}x\right\} .
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X$
\end_inset

 have density in an exponential family.
 Then,
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\E\left(t_{j}\left(X\right)\right)=-\dfrac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right)$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Var\left(t_{j}\left(X\right)\right)=-\dfrac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\eta\right)$
\end_inset

,
\end_layout

\end_deeper
\begin_layout Theorem
and the moment-generating function for 
\begin_inset Formula $\left(X_{1},\ldots,X_{k}\right)$
\end_inset

 is
\begin_inset Formula 
\[
M_{\left(X_{1},\ldots,X_{k}\right)}\left(s_{1},\ldots,s_{k}\right)=\E\left[e^{\sum_{j=1}^{k}s_{j}x_{j}}\right].
\]

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:expected-value-exp-family"

\end_inset


\end_layout

\begin_layout Proof
We begin with the pdf of an exponential family.
\begin_inset Formula 
\begin{flalign}
1 & =\int f\left(x|\theta\right)\mbox{d}x\label{eq:expected-val-proof-step-1}\\
 & =\int h\left(x\right)c\left(\theta\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\theta\right)t_{i}\left(x\right)\right)\mbox{d}x\label{eq:expected-val-proof-step-2}\\
 & =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\mbox{d}x\label{eq:expected-val-proof-step-3}\\
\frac{\partial}{\partial\eta_{j}}1 & =\frac{\partial}{\partial\eta_{j}}\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\mbox{d}x\label{eq:expected-val-proof-step-4}\\
0 & =\int\frac{\partial}{\partial\eta_{j}}\left[h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\mbox{d}x\label{eq:expected-val-proof-step-5}\\
 & =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\mbox{d}x\label{eq:expected-val-proof-step-6}\\
 & \quad+\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\left(\eta_{i}t_{i}\left(x\right)\right)\right)\mbox{d}x\nonumber \\
 & =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\mbox{d}x\label{eq:expected-val-proof-step-7}\\
 & \quad+\mbox{E}\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\left(\eta_{i}t_{i}\left(x\right)\right)\right)\nonumber \\
 & =\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\mbox{d}x\label{eq:expected-val-proof-step-8}\\
 & \quad+\mbox{E}\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\left(\eta_{i}t_{i}\left(x\right)\right)\right)\nonumber \\
 & =\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)\cdot1+\mbox{E}\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\left(\eta_{i}t_{i}\left(x\right)\right)\right)\label{eq:expected-val-proof-step-9}\\
-\frac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right) & =\mbox{E}\left(\frac{\partial}{\partial\eta_{j}}\eta_{1}t_{1}\left(x\right)+\ldots+\frac{\partial}{\partial\eta_{j}}\eta_{j}t_{j}\left(x\right)+\ldots+\frac{\partial}{\partial\eta_{j}}\eta_{k}t_{k}\left(x\right)\right)\label{eq:expected-val-proof-step-10}\\
 & =\mbox{E}\left(0\cdot t_{1}\left(x\right)+\ldots+1\cdot t_{j}\left(x\right)+\ldots+0\cdot t_{k}\left(x\right)\right)\label{eq:expected-val-proof-step-11}\\
 & =\mbox{E}\left(t_{j}\left(x\right)\right)\label{eq:expected-val-proof-step-12}
\end{flalign}

\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-1"

\end_inset

 follows from the definition of a pdf.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-2"

\end_inset

 follows by expressing 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 in exponential family form.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-3"

\end_inset

 follows from the natural reparameterization of 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-4"

\end_inset

 follows by taking the derivative of both sides of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-3"

\end_inset

 with respect to 
\begin_inset Formula $\eta_{j}$
\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-5"

\end_inset

 follows from interchanging differentiation and integration.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-6"

\end_inset

 follows from the product rule and the chain rule.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-7"

\end_inset

 follows from the fact that 
\begin_inset Formula $g'\left(x\right)=g\left(x\right)\frac{\partial}{\partial x}\log\left(g\left(x\right)\right)$
\end_inset

 and the definition of expected value.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-8"

\end_inset

 follows because we are integrating with respect to 
\begin_inset Formula $x$
\end_inset

, so we may treat functions of 
\begin_inset Formula $\eta$
\end_inset

 exclusively as constant.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-9"

\end_inset

 follows because the pdf 
\begin_inset Formula $f\left(x|\eta\right)$
\end_inset

 integrates to 1.
 We rearrange terms and expand the sum to write 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-10"

\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-proof-step-11"

\end_inset

 follows because the derivative of 
\begin_inset Formula $\eta_{i}$
\end_inset

 with respect to 
\begin_inset Formula $\eta_{j}$
\end_inset

 is equal to zero for 
\begin_inset Formula $i\neq j$
\end_inset

 and is equal to 1 in the case where 
\begin_inset Formula $i=j$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Expected value of a binomial random variable]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim B\left(n,p\right)$
\end_inset

.
 We will find the expected value of 
\begin_inset Formula $X$
\end_inset

 by applying 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:expected-value-exp-family"

\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:natural-param-binomial"

\end_inset

, the pmf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{n}{x}\left(\frac{1}{1+e^{\eta}}\right)^{n}\exp\left(x\eta\right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\eta=\log\left(p/\left(1-p\right)\right)$
\end_inset

, so that 
\begin_inset Formula $p=1/\left(1+e^{\eta}\right)$
\end_inset

.
 From the general form of a natural parameterization, we have 
\begin_inset Formula $k=1$
\end_inset

, 
\begin_inset Formula $t\left(x\right)=x$
\end_inset

, and 
\begin_inset Formula $c^{*}\left(\eta\right)=\left(1/\left(1+e^{\eta}\right)\right)^{n}$
\end_inset

.
 Then, we have 
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left(X\right) & =\mbox{E}\left(t\left(X\right)\right)\\
 & =-\frac{\partial}{\partial\eta}\log\left(\frac{1}{1+e^{\eta}}\right)^{n}\\
 & =-\frac{\partial}{\partial\eta}\log\left(1+e^{\eta}\right)^{-n}\\
 & =-\frac{\partial}{\partial\eta}\left(-n\log\left(1+e^{\eta}\right)\right)\\
 & =n\frac{\partial}{\partial\eta}\log\left(1+e^{\eta}\right)\\
 & =n\frac{e^{\eta}}{1+e^{\eta}}\\
 & =np.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $X$
\end_inset

 has a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family distribution indexed by the natural parameters,
 then for any 
\begin_inset Formula $\eta$
\end_inset

 on the interior of the natural parameter space, the mgf of 
\begin_inset Formula $\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)$
\end_inset

 exists and is given by 
\begin_inset Formula 
\begin{flalign*}
M_{\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)}\left(s_{1},\ldots,s_{k}\right) & =\frac{C^{*}\left(\eta\right)}{C^{*}\left(\eta+s\right)}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\eta+s$
\end_inset

 is the vector 
\begin_inset Formula $\left(\eta_{1}+s_{1},\ldots,\eta_{k}+s_{k}\right)$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "thm:mgf-natural-param"

\end_inset


\end_layout

\begin_layout Proof
Suppose that 
\begin_inset Formula $X$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family distribution indexed by the natural parameters.
 Then, from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Natural-parameters"

\end_inset

, it has a pdf given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right).
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign}
M_{\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)}\left(s_{1},\ldots,s_{k}\right) & =\mbox{E}\left[e^{\sum_{j=1}^{k}s_{j}t_{j}\left(x\right)}\right]\label{eq:mgf-proof-step-1}\\
 & =\int\exp\left(\sum_{j=1}^{k}s_{j}t_{j}\left(x\right)\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right)\mbox{d}x\label{eq:mgf-proof-step-2}\\
 & =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right)\mbox{d}x\label{eq:mgf-proof-step-3}\\
 & =\frac{c^{*}\left(\eta+s\right)}{c^{*}\left(\eta+s\right)}\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right)\mbox{d}x\label{eq:mgf-proof-step-4}\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\int h\left(x\right)c^{*}\left(\eta+s\right)\exp\left(\sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right)\mbox{d}x\label{eq:mgf-proof-step-5}\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\cdot1\label{eq:mgf-proof-step-6}\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}.
\end{flalign}

\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-1"

\end_inset

 follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:expected-value-exp-family"

\end_inset

 with 
\begin_inset Formula $X_{i}$
\end_inset

 replaced by 
\begin_inset Formula $t_{i}\left(X\right)$
\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-2"

\end_inset

 follows from the definition of expected value (of a function of a random
 variable, i.e., 
\begin_inset Formula $\mbox{E}\left(g\left(X\right)\right)$
\end_inset

).
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-3"

\end_inset

 follows from algebra.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-4"

\end_inset

 follows from multiplying by 
\begin_inset Formula $1=c^{*}\left(\eta+s\right)/c^{*}\left(\eta+s\right)$
\end_inset

.
 We rearrange terms to write 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-5"

\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-6"

\end_inset

 follows because the integral in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mgf-proof-step-5"

\end_inset

 is the pdf of 
\begin_inset Formula $X$
\end_inset

 with parameter 
\begin_inset Formula $\eta+s$
\end_inset

, i.e., 
\begin_inset Formula $f\left(x|\eta+s\right)$
\end_inset

 and therefore integrates to 1.
\end_layout

\begin_layout Definition
A 
\shape italic
curved exponential family
\shape default
 is a family of densities of the form given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:defn-exp-family"

\end_inset

 for which the dimension of the vector 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is equal to 
\begin_inset Formula $d<k$
\end_inset

, where 
\begin_inset Formula $k$
\end_inset

 is the number of terms in the sum in the exponent.
 If 
\begin_inset Formula $d=k$
\end_inset

, the family is a 
\shape italic
full exponential family
\shape default
.
\end_layout

\begin_layout Section
Location and scale families
\end_layout

\begin_layout Standard
Location families, scale families, and location-scale families are constructed
 by specifying a single pdf, 
\begin_inset Formula $f\left(x\right)$
\end_inset

, called the standard pdf for the family.
 Then, all other pdfs in the family are generated by transforming the standard
 pdf in a prescribed way.
\end_layout

\begin_layout Subsection
Location families
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Then, the family of pdfs indexed by 
\begin_inset Formula $\mu$
\end_inset

, 
\begin_inset Formula $f\left(x-\mu\right)$
\end_inset

, is called the 
\shape italic
location family
\shape default
 with respect to the standard pdf 
\begin_inset Formula $f$
\end_inset

, and 
\begin_inset Formula $\mu$
\end_inset

 is called the l
\shape italic
ocation parameter
\shape default
.
\end_layout

\begin_layout Standard
E.g., 
\begin_inset Formula $f\left(x\right)\sim N\left(0,1^{2}\right)$
\end_inset

, 
\begin_inset Formula $N\left(\mu,1^{2}\right)$
\end_inset

 is a location family.
 The location parameter 
\begin_inset Formula $\mu$
\end_inset

 simply shifts the pdf 
\begin_inset Formula $f\left(x\right)$
\end_inset

 so that the shape of the graph is unchanged but the point on the graph
 that was above 
\begin_inset Formula $x=0$
\end_inset

 under 
\begin_inset Formula $f\left(x\right)$
\end_inset

 is above 
\begin_inset Formula $x=\mu$
\end_inset

 for 
\begin_inset Formula $f\left(x-\mu\right)$
\end_inset

, thus
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ -1\leq X\leq2|X\sim f\left(x\right)\right\} \right) & =P\left(\left\{ \mu-1\leq X\leq\mu+2|X\sim f\left(x-\mu\right)\right\} \right).
\end{flalign*}

\end_inset

Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-of-location-normal}
\end_layout

\end_inset

 shows the normal distribution with 
\begin_inset Formula $\sigma^{2}=1^{2}$
\end_inset

 and 
\begin_inset Formula $\mu=0$
\end_inset

, 
\begin_inset Formula $\mu=2$
\end_inset

, and 
\begin_inset Formula $\mu=-2$
\end_inset

 in red, blue, and green, respectively.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-of-location-normal, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h', fig.cap='example of a normal location family'>>=
\end_layout

\begin_layout Plain Layout

x <- seq(-5,5,length=1000)
\end_layout

\begin_layout Plain Layout

par(mar=c(2,2,0.2,2))
\end_layout

\begin_layout Plain Layout

plot(x,dnorm(x, mean=0, sd=1), type="l", col="blue", ylab="f(x)", cex.lab=0.75,
 cex.axis=0.75, yaxt="n")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=2, sd=1), type="l", col="red")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=-2, sd=1), type="l", col="green")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Scale families
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Then, for any 
\begin_inset Formula $\sigma>0$
\end_inset

, the family of pdfs 
\begin_inset Formula $\left(1/\sigma\right)f\left(x/\sigma\right)$
\end_inset

, indexed by the parameter 
\begin_inset Formula $\sigma$
\end_inset

, is called the 
\shape italic
scale family
\shape default
 with standard pdf 
\begin_inset Formula $f\left(x\right)$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 is called the 
\shape italic
scale parameter
\shape default
 of the family.
\end_layout

\begin_layout Standard
E.g., 
\begin_inset Formula $f\left(x\right)\sim N\left(0,1^{2}\right)$
\end_inset

, 
\begin_inset Formula $N\left(0,\sigma^{2}\right)$
\end_inset

 is a scale family.
 The effect of introducing the scale parameter 
\begin_inset Formula $\sigma$
\end_inset

 is either to stretch (
\begin_inset Formula $\sigma>1$
\end_inset

) or to contract (
\begin_inset Formula $\sigma<1$
\end_inset

) the graph of 
\begin_inset Formula $f\left(x\right)$
\end_inset

 while still maintaining the same basic shape of the graph.
 Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-of-scale-normal}
\end_layout

\end_inset

 shows the normal distribution with 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}=1^{2}$
\end_inset

, 
\begin_inset Formula $\sigma^{2}=0.75^{2}$
\end_inset

, and 
\begin_inset Formula $\sigma^{2}=1.5^{2}$
\end_inset

 in red, blue, and green, respectively.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-of-scale-normal, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h', fig.cap='example of a normal scale family'>>=
\end_layout

\begin_layout Plain Layout

x <- seq(-5,5,length=1000)
\end_layout

\begin_layout Plain Layout

par(mar=c(2,2,0.2,2))
\end_layout

\begin_layout Plain Layout

plot(x,dnorm(x, mean=0, sd=0.75), type="l", col="blue", ylab="f(x)", cex.lab=0.75,
 cex.axis=0.75, yaxt="n")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=0, sd=1), type="l", col="red")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=0, sd=1.5), type="l", col="green")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Location-scale families
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Then, for any 
\begin_inset Formula $-\infty<\mu<\infty$
\end_inset

 and any 
\begin_inset Formula $\sigma>0$
\end_inset

, the family of pdfs 
\begin_inset Formula $\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

 is called the location-scale family with standard pdf 
\begin_inset Formula $f\left(x\right)$
\end_inset

.
 
\begin_inset Formula $\mu$
\end_inset

 is called the location parameter and 
\begin_inset Formula $\sigma$
\end_inset

 is called the scale parameter.
 E.g., 
\begin_inset Formula $f\left(x\right)\sim N\left(0,1^{2}\right)$
\end_inset

, 
\begin_inset Formula $N\left(\mu,\sigma^{2}\right)$
\end_inset

 is a location-scale family.
 The effect of introducing both the location and scale parameters is to
 stretch/contract the graph with the scale parameter and shift the graph
 with the location parameter.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Let 
\begin_inset Formula $\mu$
\end_inset

 be any real number, and let 
\begin_inset Formula $\sigma$
\end_inset

 be any positive real number.
 Then 
\begin_inset Formula $X$
\end_inset

 is a random variable with pdf 
\begin_inset Formula $\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

 if and only if there exists a random variable 
\begin_inset Formula $Z$
\end_inset

 with pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

 and 
\begin_inset Formula $X=\sigma Z+\mu$
\end_inset

.
 (This is Theorem 3.5.6 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:location-scale-family"

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $X\sim\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign}
F\left(x\right) & =P\left(\left\{ X\leq x\right\} \right)\label{eq:loc-scale-family-proof-step-1}\\
 & =\int_{-\infty}^{x}\frac{1}{\sigma}f\left(\frac{t-\mu}{\sigma}\right)\mbox{d}t\label{eq:loc-scale-family-proof-step-2}\\
 & =\int_{-\infty}^{\left(x-\mu\right)/\sigma}\frac{1}{\sigma}f\left(z\right)\sigma\mbox{d}z\label{eq:loc-scale-family-proof-step-3}\\
 & =\int_{-\infty}^{\left(x-\mu\right)/\sigma}f\left(z\right)\mbox{d}z\label{eq:loc-scale-family-proof-step-4}\\
 & =P\left(Z\leq\frac{x-\mu}{\sigma}\right)\label{eq:loc-scale-family-proof-step-5}\\
 & =P\left(\sigma Z+\mu\leq x\right).\label{eq:loc-scale-family-proof-step-6}
\end{flalign}

\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:loc-scale-family-proof-step-1"

\end_inset

 is the definition of a cdf.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:loc-scale-family-proof-step-2"

\end_inset

 follows from the definition of a pdf.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:loc-scale-family-proof-step-3"

\end_inset

 follows from making the substitution 
\begin_inset Formula $z=\left(t-\mu\right)/\sigma$
\end_inset

, so that 
\begin_inset Formula $\mbox{d}t=\sigma\mbox{d}z$
\end_inset

, 
\begin_inset Formula $f\left(z\right)$
\end_inset

 is a pdf for 
\begin_inset Formula $Z$
\end_inset

, and the upper limit of integration is 
\begin_inset Formula $z\left(x\right)=\left(x-\mu\right)/\sigma$
\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:loc-scale-family-proof-step-4"

\end_inset

 follows from algebra.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:loc-scale-family-proof-step-5"

\end_inset

 follows from the definition of a pdf, and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:loc-scale-family-proof-step-6"

\end_inset

 follows from algebra.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $Z$
\end_inset

 be a random variable with pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

.
 Suppose 
\begin_inset Formula $\E\left(Z\right)$
\end_inset

 and 
\begin_inset Formula $\Var\left(Z\right)$
\end_inset

 exist.
 If 
\begin_inset Formula $X$
\end_inset

 is a random variable with pdf 
\begin_inset Formula $\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

, then 
\begin_inset Formula $\E\left(X\right)=\sigma\E\left(Z\right)+\mu$
\end_inset

, and 
\begin_inset Formula $\Var\left(X\right)=\sigma^{2}\Var\left(Z\right)$
\end_inset

.
 (This is Theorem 3.5.7 from Casella and Berger; the version in the lecture
 slides is a slight restatement.)
\end_layout

\begin_layout Proof
By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:location-scale-family"

\end_inset

, there is a random variable 
\begin_inset Formula $Z^{*}$
\end_inset

 with pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

 and 
\begin_inset Formula $X=\sigma Z^{*}+\mu$
\end_inset

.
 So 
\begin_inset Formula 
\begin{flalign}
\E\left(X\right) & =\E\left(\sigma Z^{*}+\mu\right)\label{eq:expected-val-loc-scale-step-1}\\
 & =\E\left(\sigma Z^{*}\right)+\E\left(\mu\right)\label{eq:expected-val-loc-scale-step-2}\\
 & =\sigma\E\left(Z^{*}\right)+\mu\label{eq:expected-val-loc-scale-step-3}\\
 & =\sigma\int z\cdot f\left(z\right)\mbox{d}z+\mu\label{eq:expected-val-loc-scale-step-4}\\
 & =\sigma\E\left(Z\right)+\mu.\label{eq:expected-val-loc-scale-step-5}
\end{flalign}

\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-loc-scale-step-1"

\end_inset

 follows by substituting for 
\begin_inset Formula $X$
\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-loc-scale-step-2"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-loc-scale-step-3"

\end_inset

 follow from the linearity of expected value.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-loc-scale-step-4"

\end_inset

 follows from the definition of expected value and because 
\begin_inset Formula $Z^{*}$
\end_inset

 has the same pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

 as 
\begin_inset Formula $Z$
\end_inset

, which is a consequence of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:location-scale-family"

\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expected-val-loc-scale-step-5"

\end_inset

 follows from the definition of expected value.
 Q.E.D.
 Then, 
\begin_inset Formula 
\begin{flalign}
\Var\left(X\right) & =\Var\left(\sigma Z^{*}+\mu\right)\label{eq:variance-loc-scale-step-1}\\
 & =\E\left(\left(\sigma Z^{*}+\mu\right)-\E\left(\sigma Z^{*}+\mu\right)\right)^{2}\label{eq:variance-loc-scale-step-2}\\
 & =\E\left(\sigma Z^{*}+\mu-\left(\sigma\E\left(Z^{*}\right)+\mu\right)\right)^{2}\label{eq:variance-loc-scale-step-3}\\
 & =\E\left(\sigma Z^{*}+\mu-\sigma\E\left(Z^{*}\right)-\mu\right)^{2}\label{eq:variance-loc-scale-step-4}\\
 & =\E\left(\sigma\left(Z^{*}-\E\left(Z^{*}\right)\right)\right)^{2}\label{eq:variance-loc-scale-step-5}\\
 & =\left(\sigma\E\left(Z^{*}-\E\left(Z^{*}\right)\right)\right)^{2}\label{eq:variance-loc-scale-step-6}\\
 & =\sigma^{2}\E\left(Z^{*}-\E\left(Z^{*}\right)\right)^{2}\label{eq:variance-loc-scale-step-7}\\
 & =\sigma^{2}\Var\left(Z^{*}\right)\label{eq:variance-loc-scale-step-8}\\
 & =\sigma^{2}\Var\left(Z\right)\label{eq:variance-loc-scale-step-9}
\end{flalign}

\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-1"

\end_inset

 follows by substituting for 
\begin_inset Formula $X$
\end_inset

.
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-2"

\end_inset

 follows from the definition of variance.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-3"

\end_inset

 follows from the linearity of expected value.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-4"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-5"

\end_inset

 follow from algebra.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-6"

\end_inset

 follows from the linearity of expected value.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-7"

\end_inset

 follows from algebra.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-8"

\end_inset

 follows from the definition of variance.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:variance-loc-scale-step-9"

\end_inset

 follows because 
\begin_inset Formula $Z^{*}$
\end_inset

 has the same pdf as 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Chapter
Principles of data reduction
\end_layout

\begin_layout Section
Sufficiency
\end_layout

\begin_layout Standard
The concept of sufficiency attempts to find a statistic 
\begin_inset Formula $T\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 that contains all the information in the sample about the model parameter
 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Definition
A 
\shape italic
statistic
\shape default
 is a function 
\begin_inset Formula $T\left(X\right)$
\end_inset

 of the data, such as mean, variance, max, or min.
 
\begin_inset Formula $T\left(X\right)$
\end_inset

 is a random variable.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition
An 
\shape italic
estimate
\shape default
 is a statistic that is intended to be close to a parameter.
\end_layout

\begin_layout Subsection
Data reduction
\end_layout

\begin_layout Standard
Any statistic 
\begin_inset Formula $T\left(X\right)$
\end_inset

 defines a form of data reduction or data summary.
 An investigator who uses only the observed value of the statistic rather
 than the entire observed sample 
\series bold

\begin_inset Formula $\mathbf{X}$
\end_inset


\series default
 will treat as equal two samples 
\series bold

\begin_inset Formula $\mathbf{X}$
\end_inset


\series default
 and 
\series bold

\begin_inset Formula $\mathbf{Y}$
\end_inset


\series default
 that satisfy 
\begin_inset Formula $T\left(x\right)=T\left(y\right)$
\end_inset

 even though the actual sample values may be different in some ways.
 Data reduction in terms of a particular statistic can be thought of as
 a partition of the sample space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 of 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 
\begin_inset Formula $T\left(X\right)$
\end_inset

 reduces the data by partitioning the sample space into sets 
\begin_inset Formula $A_{t}$
\end_inset

, 
\begin_inset Formula $t\in\mathcal{T}$
\end_inset

, defined by 
\begin_inset Formula $A_{t}=\left\{ x:T\left(x\right)=t\right\} $
\end_inset

 where 
\begin_inset Formula $\mathcal{T}=\left\{ t:t=T\left(x\right)\right\} $
\end_inset

 for some 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Waiting time]
\end_layout

\end_inset

Suppose that 
\begin_inset Formula $X$
\end_inset

 represents waiting time, e.g., for a bus.
 We first collect data 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

.
 Suppose that we wish to find the mean waiting time 
\begin_inset Formula $\overline{x}=\left(1/n\right)\sum_{i=1}^{n}x_{i}$
\end_inset

, i.e., 
\begin_inset Formula $T\left(X\right)=\overline{x}$
\end_inset

.
 We calculate 
\begin_inset Formula $\overline{x}=8.32$
\end_inset

.
 Then, all sets of 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 that give 
\begin_inset Formula $\overline{x}=8.32$
\end_inset

 are a partition of the sample space.
 Many points in the sample space have this same mean, and we can consider
 them as belonging to the set 
\begin_inset Formula $\left\{ \left(x_{1},\ldots,x_{n}\right):\overline{x}=8.32\right\} $
\end_inset

, which is also the hyperplane 
\begin_inset Formula $x_{1}+\cdots+x_{n}=\left(8.32\right)n$
\end_inset

.
 In this case, the sample mean 
\begin_inset Formula $\overline{X}$
\end_inset

 (or any statistic 
\begin_inset Formula $T\left(X\right)$
\end_inset

) partitions the sample space into a collection of sets.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a binomial distribution]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sub:example-sampling-binom"

\end_inset

Let 
\begin_inset Formula $X\sim B\left(3,p\right)$
\end_inset

, where 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

.
 Then, 
\begin_inset Formula $X$
\end_inset

 represents the number of successes in 3 trials of a random experiment.
 Suppose that we draw a sample of size 2, so that the sample space consists
 of
\begin_inset Formula 
\[
\left\{ \left(0,0\right),\left(0,1\right),\left(0,2\right),\left(0,3\right),\left(1,1\right),\left(1,2\right),\left(1,3\right),\left(2,2\right),\left(2,3\right),\left(3,3\right)\right\} .
\]

\end_inset

Let 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 be the maximum of the sample, i.e., 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},X_{2}\right)$
\end_inset

.
 Then, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 reduces the data by partitioning the sample space into sets 
\begin_inset Formula $A_{t}$
\end_inset

 with 
\begin_inset Formula $\mathcal{T}=\left\{ 0,1,2,3\right\} $
\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
A_{0} & =\left\{ \left(0,0\right)\right\} \\
A_{1} & =\left\{ \left(0,1\right),\left(1,2\right)\right\} \\
A_{2} & =\left\{ \left(0,2\right),\left(1,2\right),\left(2,2\right)\right\} \\
A_{3} & =\left\{ \left(0,3\right),\left(1,3\right),\left(2,3\right),\left(3,3\right)\right\} .
\end{flalign*}

\end_inset


\end_layout

\begin_layout Subsection
Sufficiency principle
\end_layout

\begin_layout Definition
A statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a 
\shape italic
sufficient statistic
\shape default
 for 
\begin_inset Formula $\theta$
\end_inset

 if the conditional distribution of the sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

 given the value of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

, i.e., if 
\begin_inset Formula $P\left(\mathbf{X}|T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
A sufficient statistic for a parameter 
\begin_inset Formula $\theta$
\end_inset

 contains all information that is in the data about 
\begin_inset Formula $\theta$
\end_inset

.
 I.e., given the value of 
\begin_inset Formula $T$
\end_inset

, the sufficient statistic, we can gain no more knowledge about 
\begin_inset Formula $\theta$
\end_inset

 from knowing more about the probability distribution of 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{X}$
\end_inset

 are two samples and 
\begin_inset Formula $T\left(\mathbf{X}\right)=T\left(\mathbf{Y}\right)$
\end_inset

, then inference about 
\begin_inset Formula $\theta$
\end_inset

 should be the same whether 
\begin_inset Formula $\mathbf{X=x}$
\end_inset

 is observed or 
\begin_inset Formula $\mathbf{Y=\mathbf{y}}$
\end_inset

 is observed.
 Note that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 must be a one-to-one function to be a sufficient statistic.
\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)$
\end_inset

 is the joint pdf or pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $q\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

 is the pdf or pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

, then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

 if, for every 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the sample space, the ratio 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{x}|\theta\right)\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "thm:suff-stat-ratio"

\end_inset


\end_layout

\begin_layout Proof
By definition, if 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

, then 
\begin_inset Formula $P\left(\mathbf{X}|T\left(\mathbf{X}\right)\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

 .
 
\begin_inset Formula 
\begin{flalign}
P\left(\mathbf{X}|T\left(\mathbf{X}\right)\right) & =P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} |\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =\frac{P\left(\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)\cap\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{P\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{p\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{x}|\theta\right)\right)}
\end{flalign}

\end_inset

(1) is simply an expansion of the notation.
 (2) follows from the definition of conditional probability.
 (3) follows because 
\begin_inset Formula $\left\{ \mathbf{X}=\mathbf{x}\right\} $
\end_inset

 is a subset of 
\begin_inset Formula $\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} $
\end_inset

, and if we have two sets 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 such that 
\begin_inset Formula $A\subset B$
\end_inset

, then 
\begin_inset Formula $A\cap B=A$
\end_inset

, so that 
\begin_inset Formula $P\left(A\cap B\right)=P\left(A\right)$
\end_inset

.
 (4) follows from setting 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)$
\end_inset

 equal to the numerator, which is the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and setting 
\begin_inset Formula $q\left(T\left(\mathbf{X}|\theta\right)\right)$
\end_inset

 equal to the denominator, which is the pdf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 Then, if 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{x}|\theta\right)\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, i.e., does not depend on 
\begin_inset Formula $\theta$
\end_inset

, then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a binomial distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 be as in example 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:example-sampling-binom"

\end_inset

.
 If 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

, show it, and if not, explain why not.
 We have 
\begin_inset Formula $\mathbf{X}=X_{1},X_{2}$
\end_inset

, where the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are independent and identically distributed because 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a random sample (by assumption).
 Then, each 
\begin_inset Formula $X_{i}$
\end_inset

 has the same pmf as 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =P\left(\left\{ X=x\right\} \right)\\
 & =\binom{3}{x}p^{x}\left(1-p\right)^{3-x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,3\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)\\
 & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\left\{ X_{2}=x_{2}\right\} \right)\\
 & =P\left(\left\{ X_{1}=x_{1}\right\} \right)\cdot P\left(\left\{ X_{2}=x_{2}\right\} \right)\\
 & =\prod_{i=1}^{2}p_{X}\left(x_{i}\right)
\end{flalign*}

\end_inset

Each 
\begin_inset Formula $X_{i}$
\end_inset

 also has the same cdf as 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
F_{X}\left(t\right) & =P\left(\left\{ X\leq t\right\} \right)\\
 & =\sum_{k=0}^{t}p_{X}\left(k\right).
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},X_{2}\right)$
\end_inset

, which is just the order statistic 
\begin_inset Formula $X_{\left(2\right)}$
\end_inset

.
 For some maximum value 
\begin_inset Formula $T\left(\mathbf{x}\right)=t$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X_{\left(2\right)}\leq t\right\} \right) & =P\left(\left\{ X_{1}\leq t\right\} \cap\left\{ X_{2}\leq t\right\} \right)\\
 & =P\left(\left\{ X_{1}\leq t\right\} \right)\cdot P\left(\left\{ X_{2}\leq t\right\} \right)\\
 & =\prod_{i=1}^{2}P\left(\left\{ X_{i}\leq t\right\} \right)\\
 & =\prod_{i=1}^{2}F_{X}\left(t\right)\\
 & =\left[F_{X}\left(t\right)\right]^{2}
\end{flalign*}

\end_inset

so that the pmf of 
\begin_inset Formula $X_{\left(2\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{X_{\left(2\right)}}\left(t\right) & =P\left(\left\{ X_{\left(2\right)}=t\right\} \right)\\
 & =P\left(\left\{ X_{\left(2\right)}\leq t\right\} \right)-P\left(\left\{ X_{\left(2\right)}\leq t-1\right\} \right)\\
 & =F_{X_{\left(2\right)}}\left(t\right)-F_{X_{\left(2\right)}}\left(t-1\right)\\
 & =\left[F_{X}\left(t\right)\right]^{2}-\left[F_{X}\left(t-1\right)\right]^{2}\\
 & =\left[\sum_{k=0}^{t}p_{X}\left(k\right)\right]^{2}-\left[\sum_{k=0}^{t-1}p_{X}\left(k\right)\right]^{2}\\
 & =\left[\sum_{k=0}^{t}p_{X}\left(k\right)\right]^{2}-\left[\sum_{k=0}^{t}p_{X}\left(k\right)-p_{X}\left(t\right)\right]^{2}\\
 & =\left[\sum_{k=0}^{t}p_{X}\left(k\right)\right]^{2}-\left[\left(\sum_{k=0}^{t}p_{X}\left(k\right)\right)^{2}-2p_{X}\left(t\right)\sum_{k=0}^{t}p_{X}\left(k\right)+\left[p_{X}\left(t\right)\right]^{2}\right]\\
 & =2p_{X}\left(t\right)\sum_{k=0}^{t}p_{X}\left(k\right)-\left[p_{X}\left(t\right)\right]^{2}\\
 & =p_{X}\left(t\right)\left[2\sum_{k=0}^{t}p_{X}\left(k\right)-p_{X}\left(t\right)\right].
\end{flalign*}

\end_inset

Then, we have 
\begin_inset Formula 
\begin{flalign*}
P\left(\mathbf{X}|T\left(\mathbf{X}\right)\right) & =\frac{P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \cap\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{\prod_{i=1}^{2}p_{X}\left(x_{i}\right)}{p_{X}\left(t\right)\left[2\sum_{k=0}^{t}p_{X}\left(k\right)-p_{X}\left(t\right)\right]}\\
 & =\frac{\left[\binom{3}{x_{1}}p^{x_{1}}\left(1-p\right)^{3-x_{1}}\right]\left[\binom{3}{x_{2}}p^{x_{2}}\left(1-p\right)^{3-x_{2}}\right]}{\left[\binom{3}{t}p^{t}\left(1-p\right)^{3-t}\right]\left[2\sum_{k=0}^{t}\left(\binom{3}{k}p^{k}\left(1-p\right)^{3-k}\right)-\binom{3}{t}p^{t}\left(1-p\right)^{3-t}\right]}.
\end{flalign*}

\end_inset

Clearly, this expression depends on 
\begin_inset Formula $p$
\end_inset

, so 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(\mathbf{X}\right)$
\end_inset

 is not a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a uniform distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim U\left(0,\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

.
 Let 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},\ldots,X_{n}\right)$
\end_inset

.
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:suff-statistic-uniform"

\end_inset


\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x\right) & =\frac{1}{\theta-0}\\
 & =\frac{1}{\theta}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left(0,\theta\right)$
\end_inset

 and 
\begin_inset Formula $f_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)\\
 & =P\left(\left\{ X_{1}=x_{1}\right\} \right)\cdot\ldots\cdot P\left(\left\{ X_{n}=x_{n}\right\} \right)\\
 & =\prod_{i=1}^{n}f_{X}\left(x_{i}\right)\\
 & =\left(\frac{1}{\theta}\right)^{n}\\
 & =\frac{1}{\theta^{n}}.
\end{flalign*}

\end_inset

The cdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
F_{X}\left(x\right) & =\int_{-\infty}^{x}f_{X}\left(t\right)\mbox{d}t\\
 & =\int_{-\infty}^{0}0\mbox{d}t+\int_{0}^{x}\frac{1}{\theta}\mbox{d}t\\
 & =0+\left(\frac{1}{\theta}\int_{0}^{x}1\mbox{d}t\right)\\
 & =\frac{1}{\theta}\left(t\Big\rvert_{0}^{x}\right)\\
 & =\frac{1}{\theta}\left(x-0\right)\\
 & =\frac{x}{\theta}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},\ldots,X_{n}\right)$
\end_inset

, which is just the order statistic 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-continuous"

\end_inset

, we have 
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(n\right)}}\left(x\right) & =\frac{n!}{\left(j-1\right)!\left(n-j\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{j-1}\left[1-F_{X}\left(x\right)\right]^{n-j}\\
 & =\frac{n!}{\left(n-1\right)!\left(n-n\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{n-1}\left[1-F_{X}\left(x\right)\right]^{n-n}\\
 & =\frac{n!}{\left(n-1\right)!}\left(\frac{1}{\theta}\right)\left[\frac{x}{\theta}\right]^{n-1}\left[1-\frac{x}{\theta}\right]^{0}\\
 & =n\left(\frac{1}{\theta}\right)\left(\frac{x}{\theta}\right)^{n-1}\\
 & =n\left(\frac{1}{\theta}\right)\frac{x^{n-1}}{\theta^{n-1}}\\
 & =nx^{n-1}\left(\frac{1}{\theta}\right)\left(\frac{1}{\theta^{n-1}}\right)\\
 & =nx^{n-1}\left(\frac{1}{\theta^{n}}\right).
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)$
\end_inset

 be the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and let 
\begin_inset Formula $q\left(T\left(\mathbf{X}\right)|\theta\right)$
\end_inset

 be the pdf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{X}\right)|\theta\right)} & =\frac{1/\theta^{n}}{nx^{n-1}\left(1/\theta^{n}\right)}\\
 & =\frac{1}{nx^{n-1}}.
\end{flalign*}

\end_inset

This expression does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{X}\right)|\theta\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a Poisson distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim P\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},X_{2}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

.
 Let 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{1}+X_{2}$
\end_inset

.
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
 The pmf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =\frac{e^{-\lambda}\lambda^{x}}{x!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\left\{ X_{2}\cap x_{x}\right\} \right)\\
 & =P\left(\left\{ X_{2}=x_{1}\right\} \right)\cdot P\left(\left\{ X_{2}=x_{2}\right\} \right)\\
 & =p_{X_{1}}\left(x_{1}\right)\cdot p_{X_{2}}\left(x_{2}\right)\\
 & =\left(\frac{e^{-\lambda}\lambda^{x_{1}}}{x_{1}!}\right)\left(\frac{e^{-\lambda}\lambda^{x_{2}}}{x_{2}!}\right)\\
 & =\frac{e^{-2\lambda}\lambda^{x_{1}+x_{2}}}{x_{1}!x_{2}!}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $z=x_{1}+x_{2}$
\end_inset

.
 Then, the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign}
P\left(\left\{ X_{1}+X_{2}=z\right\} \right) & =\sum_{k=0}^{z}P\left(\left\{ X_{1}=k\right\} \cap\left\{ X_{2}=z-k\right\} \right)\\
 & =\sum_{k=0}^{z}P\left(\left\{ X_{1}=k\right\} \right)\cdot P\left(\left\{ X_{2}=z-k\right\} \right)\\
 & =\sum_{k=0}^{z}\left(\frac{e^{-\lambda}\lambda^{k}}{k!}\right)\left(\frac{e^{-\lambda}\lambda^{z-k}}{\left(z-k\right)!}\right)\\
 & =\sum_{k=0}^{z}\frac{e^{-2\lambda}\lambda^{z}}{k!\left(z-k\right)!}\\
 & =e^{-2\lambda}\lambda^{z}\sum_{k=0}^{z}\frac{1}{k!\left(z-k\right)!}\\
 & =\frac{e^{-2\lambda}\lambda^{z}}{z!}\sum_{k=0}^{z}\frac{z!}{k!\left(z-k\right)!}\\
 & =\frac{e^{-2\lambda}\lambda^{z}}{z!}\cdot2^{z}\\
 & =\frac{e^{-2\lambda}\left(2\lambda\right)^{z}}{z!}\\
 & =\frac{e^{-2\lambda}\left(2\lambda\right)^{x_{1}+x_{2}}}{\left(x_{1}+x_{2}\right)!}
\end{flalign}

\end_inset

so that 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{1}+X_{2}\sim P\left(2\lambda\right)$
\end_inset

.
 (1) follows from Proposition 6.18 in Weiss' text, which gives the pmf of
 the sum of two discrete random variables.
 (2) follows because 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 are independent.
 (3) follows because 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 each has the pmf of 
\begin_inset Formula $X$
\end_inset

.
 (4) and (5) follow from algebra.
 (6) follows by multiplying by 
\begin_inset Formula $1=z!/z!$
\end_inset

 and rearrangement.
 (7) follows from the binomial theorem, which gives 
\begin_inset Formula 
\begin{flalign*}
\left(x+y\right)^{n} & =\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
2^{z} & =\left(1+1\right)^{z}\\
 & =\sum_{k=0}^{z}\binom{z}{k}1^{z-k}1^{k}\\
 & =\sum_{k=0}^{z}\binom{z}{k}\\
 & =\sum_{k=0}^{z}\frac{z!}{k!\left(z-k\right)!}.
\end{flalign*}

\end_inset

(8) and (9) follow from algebra.
 Let 
\begin_inset Formula $p\left(\mathbf{x}|\lambda\right)$
\end_inset

 be the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and let 
\begin_inset Formula $q\left(T\left(\mathbf{X}\right)|\lambda\right)$
\end_inset

 be the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|\lambda\right)}{q\left(T\left(\mathbf{X}\right)|\lambda\right)} & =\frac{\frac{e^{-2\lambda}\lambda^{x_{1}+x_{2}}}{x_{1}!x_{2}!}}{\frac{e^{-2\lambda}\left(2\lambda\right)^{x_{1}+x_{2}}}{\left(x_{1}+x_{2}\right)!}}\\
 & =\frac{\lambda^{x_{1}+x_{2}}\left(x_{1}+x_{2}\right)!}{\left(2\lambda\right)^{x_{1}+x_{2}}x_{1}!x_{2}!}\\
 & =\frac{\lambda^{x_{1}+x_{2}}\left(x_{1}+x_{2}\right)!}{2^{x_{1}+x_{2}}\lambda^{x_{1}+x_{2}}x_{1}!x_{2}!}\\
 & =\frac{\left(x_{1}+x_{2}\right)!}{x_{1}!x_{2}!}\left(\frac{1}{2}\right)^{x_{1}+x_{2}}.
\end{flalign*}

\end_inset

Clearly, this expression does not depend on 
\begin_inset Formula $\lambda$
\end_inset

, so it follows that 
\begin_inset Formula $p\left(\mathbf{x}|\lambda\right)/q\left(T\left(\mathbf{X}\right)|\lambda\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\lambda$
\end_inset

, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a Bernoulli distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Bernoulli}\left(p\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

.
 Let 
\begin_inset Formula $T\left(\mathbf{X}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset

 (this is the MLE 
\begin_inset Formula $\hat{p}$
\end_inset

).
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Example
A Bernoulli random variable is a binomial random variable with 
\begin_inset Formula $n=1$
\end_inset

, so the pmf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =\binom{1}{x}p^{x}\left(1-p\right)^{1-x}\\
 & =p^{x}\left(1-p\right)^{1-x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Then, the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)\\
 & =P\left(\left\{ X_{1}=x_{1}\right\} \right)\cdot\ldots\cdot P\left(\left\{ X_{n}=x_{n}\right\} \right)\\
 & =\prod_{i=1}^{n}p_{X}\left(x_{i}\right)\\
 & =\prod_{i=1}^{n}p^{x_{i}}\left(1-p\right)^{1-x_{i}}\\
 & =p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{\sum_{i=1}^{n}\left(1-x_{i}\right)}\\
 & =p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}.
\end{flalign*}

\end_inset

Proposition 6.20 from Weiss' text states that if 
\begin_inset Formula $Y_{1},\ldots,Y_{m}$
\end_inset

 are independent random variables with 
\begin_inset Formula $Y_{j}\sim B\left(n_{j},p\right)$
\end_inset

 for 
\begin_inset Formula $1\leq j\leq m$
\end_inset

, then 
\begin_inset Formula $Y_{1}+\cdots+Y_{m}\sim B\left(n_{1}+\cdots n_{m},p\right)$
\end_inset

.
 It follows that 
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}X_{i} & \sim B\left(\sum_{i=1}^{n}1,p\right)\\
 & =B\left(n,p\right).
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $T\left(\mathbf{X}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset

, so it follows that 
\begin_inset Formula $nT\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

, so that 
\begin_inset Formula $nT\left(\mathbf{X}\right)\sim B\left(n,p\right)$
\end_inset

.
 Then, the pmf of 
\begin_inset Formula $nT\left(\mathbf{X}\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ nT\left(\mathbf{X}\right)=\sum_{i=1}^{n}x_{i}\right\} \right) & =\binom{n}{\sum_{i=1}^{n}x_{i}}p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $p\left(\mathbf{x}|p\right)$
\end_inset

 be the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and let 
\begin_inset Formula $q\left(nT\left(\mathbf{X}\right)|p\right)$
\end_inset

 be the pmf of 
\begin_inset Formula $nT\left(\mathbf{X}\right)$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

, we have 
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|p\right)}{q\left(nT\left(\mathbf{X}\right)|p\right)} & =\frac{p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}}{\binom{n}{\sum_{i=1}^{n}x_{i}}p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}}\\
 & =\frac{1}{\binom{n}{\sum_{i=1}^{n}x_{i}}}.
\end{flalign*}

\end_inset

Clearly, this expression does not depend on 
\begin_inset Formula $p$
\end_inset

, so it follows that 
\begin_inset Formula $nT\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\hat{p}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Subsection
Factorization theorem
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Factorization Theorem]
\end_layout

\end_inset

Let 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

 be the joint pdf or pmf of a sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 A statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

 if and only if there exist functions 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 such that 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

 for all sample points 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and all 
\begin_inset Formula $\theta$
\end_inset

.
 (This is Theorem 6.2.6 from Casella and Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:factorization"

\end_inset


\end_layout

\begin_layout Proof
We will give the proof only for the discrete case.
\end_layout

\begin_layout Proof
Suppose 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic.
 Choose 
\begin_inset Formula $g\left(t|\theta\right)=P_{\theta}\left(\left\{ T\left(\mathbf{X}\right)=t\right\} \right)$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)=P\left(\left\{ \mathbf{X}=\mathbf{x}|T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)$
\end_inset

.
 Because 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is sufficient, the conditional probability defining 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 Thus, this choice of 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 and 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 is legitimate, and for this choice we have
\begin_inset Formula 
\begin{flalign}
f\left(\mathbf{x}|\theta\right) & =P_{\theta}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)\\
 & =P_{\theta}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \cap\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =P_{\theta}\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\cdot P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} |\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right).
\end{flalign}

\end_inset

(1) follows from the definition of a pmf.
 (2) follows because 
\begin_inset Formula $\left\{ \mathbf{X}=\mathbf{x}\right\} $
\end_inset

 is a subset of 
\begin_inset Formula $\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} $
\end_inset

, and if we have two sets 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 such that 
\begin_inset Formula $A\subset B$
\end_inset

, then 
\begin_inset Formula $A\cap B=A$
\end_inset

, so that 
\begin_inset Formula $P\left(A\cap B\right)=P\left(A\right)$
\end_inset

.
 (3) follows from the definition of conditional probability.
 (4) follows from our definitions of 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

.
 So factorization has been exhibited.
 We also see from the last two lines above that 
\begin_inset Formula $P_{\theta}\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)=g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

, so 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

 is the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 
\end_layout

\begin_layout Proof
Now assume the factorization 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

 exists.
 Let 
\begin_inset Formula $q\left(t|\theta\right)$
\end_inset

 be the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 To now show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is sufficient, we examine the ratio 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

.
 Define 
\begin_inset Formula $A_{T\left(\mathbf{x}\right)}=\left\{ \mathbf{y}:T\left(\mathbf{y}\right)=T\left(\mathbf{x}\right)\right\} $
\end_inset

.
 Then
\begin_inset Formula 
\begin{flalign}
\frac{f\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{x}\right)|\theta\right)} & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{q\left(T\left(\mathbf{x}\right)|\theta\right)}\\
 & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{\sum_{A_{T\left(\mathbf{x}\right)}}g\left(T\left(\mathbf{y}\right)|\theta\right)h\left(\mathbf{y}\right)}\\
 & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{\sum_{A_{T\left(\mathbf{x}\right)}}g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{y}\right)}\\
 & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{g\left(T\left(\mathbf{x}\right)|\theta\right)\sum_{A_{T\left(\mathbf{x}\right)}}h\left(\mathbf{y}\right)}\\
 & =\frac{h\left(\mathbf{x}\right)}{\sum_{A_{T\left(\mathbf{x}\right)}}h\left(\mathbf{y}\right)}.
\end{flalign}

\end_inset

(1) follows from our assumption that the factorization exists.
 (2) follows from the definition of the pmf of 
\begin_inset Formula $T$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{T\left(\mathbf{X}\right)}\left(T\left(\mathbf{x}\right)\right) & =P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =\sum_{\mathbf{y}\in T^{-1}\left(\left\{ T\left(\mathbf{x}\right)\right\} \right)}P\left(\left\{ \mathbf{X}=\mathbf{y}\right\} \right)\\
 & =\sum_{\mathbf{y}\in T^{-1}\left(\left\{ T\left(\mathbf{x}\right)\right\} \right)}f_{\mathbf{X}}\left(\mathbf{y}\right)\\
 & =\sum_{\mathbf{y}\in\left\{ \mathbf{y}:T\left(\mathbf{y}\right)=T\left(\mathbf{x}\right)\right\} }f_{\mathbf{X}}\left(\mathbf{y}\right)\\
 & =\sum_{\mathbf{y}\in A_{T\left(\mathbf{x}\right)}}g\left(T\left(\mathbf{y}\right)|\theta\right)h\left(\mathbf{y}\right).
\end{flalign*}

\end_inset

(3) and (4) follow from the fact that 
\begin_inset Formula $T$
\end_inset

 is constant on 
\begin_inset Formula $A_{T\left(\mathbf{x}\right)}$
\end_inset

, i.e., 
\series bold

\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset


\series default
.
 (5) follows from algebra.
 Because (5) does not depend on 
\begin_inset Formula $\theta$
\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sufficient statistic for a Poisson random variable]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Poisson}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x|\lambda\right) & =\frac{e^{-\lambda}\lambda^{x}}{x!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\lambda\right) & =\prod_{i=1}^{n}p_{X_{i}}\left(x_{i}\right)\\
 & =\prod_{i=1}^{n}\left(\frac{e^{-\lambda}\lambda^{x_{i}}}{x_{i}!}\right)\\
 & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\left(\prod_{i=1}^{n}e^{-\lambda}\lambda^{x_{i}}\right)\\
 & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)e^{-n\lambda}\lambda^{x_{1}+\cdots+x_{n}}\\
 & =\underbrace{\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)}_{h\left(\mathbf{x}\right)}\underbrace{e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}}_{g\left(T\left(\mathbf{x}\right)|\lambda\right)}.
\end{flalign*}

\end_inset

It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{x}\right)=\sum_{i=1}^{n}x_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:suff-stat-poisson"

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sufficient statistic for a Bernoulli random variable]
\end_layout

\end_inset

Consider a sequence of independent Bernoulli random variables 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 where 
\begin_inset Formula $P\left(\left\{ X_{i}=x\right\} \right)=\theta^{x}\left(1-\theta\right)^{1-x}$
\end_inset

 where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

.
 Then, the joint pmf of the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\left(X_{1},\ldots,X_{n}|\theta\right)}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}\left(\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}\right)\\
 & =\theta^{\sum_{i=1}^{n}x_{i}}\prod_{i=1}^{n}\left[\left(1-\theta\right)^{1}\left(1-\theta\right)^{-x_{i}}\right]\\
 & =\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n}\left(1-\theta\right)^{-\sum_{i=1}^{n}x_{i}}\\
 & =\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n}\left(\frac{1}{1-\theta}\right)^{\sum_{i=1}^{n}x_{i}}\\
 & =\underbrace{\left(1-\theta\right)^{n}\left(\frac{\theta}{1-\theta}\right)^{\sum_{i=1}^{n}x_{i}}}_{g\left(T\left(\mathbf{x}\right)|\theta\right)}.
\end{flalign*}

\end_inset

We set 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

.
 Then, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{x}\right)=\sum_{i=1}^{n}x_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:suff-stat-bernoulli"

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sufficient statistic for a normal random variable]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim N\left(\mu,\sigma^{2}\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\left(x-\mu\right)^{2}/\left(2\sigma^{2}\right)}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

, where 
\begin_inset Formula $\mu\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $\sigma>0$
\end_inset

.
 Find a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:suff-stat-normal"

\end_inset


\end_layout

\begin_layout Example
A joint pdf for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\mu,\sigma^{2}\right) & =\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{1}{2}\left(\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}\right)\right\} \right]\\
 & =\prod_{i=1}^{n}\left[\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left(\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}\right)\right\} \right]\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ \sum_{i=1}^{n}-\frac{1}{2}\left(\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}\right)\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(\frac{x_{i}^{2}}{\sigma^{2}}-\frac{2\mu x_{i}}{\sigma^{2}}+\frac{\mu^{2}}{\sigma^{2}}\right)\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2}\left(\frac{\sum_{i=1}^{n}x_{i}^{2}}{\sigma^{2}}-\frac{2\mu\sum_{i=1}^{n}x_{i}}{\sigma^{2}}+\frac{n\mu^{2}}{\sigma^{2}}\right)\right\} \\
 & =\underbrace{\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{n\mu^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\sum_{i=1}^{n}x_{i}^{2}}{2\sigma^{2}}+\frac{\mu\sum_{i=1}^{n}x_{i}}{\sigma^{2}}\right\} }_{g\left(T\left(\mathbf{x}\right)|\theta\right)}.
\end{flalign*}

\end_inset

We set 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

.
 Then, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(\sum_{i=1}^{n}X_{i}^{2},\sum_{i=1}^{n}X_{i}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sufficient statistic for a uniform distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim U\left(0,\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\frac{1}{\theta}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left(0,\theta\right)$
\end_inset

 and 
\begin_inset Formula $f\left(x|\theta\right)=0$
\end_inset

 otherwise, so that a joint pdf for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}\left(\frac{1}{\theta}I_{\left\{ 0<x_{i}<\theta\right\} }\right)\\
 & =\left(\frac{1}{\theta}\right)^{n}I_{\left\{ x_{\left(1\right)}>0\right\} }I_{\left\{ x_{\left(n\right)}<\theta\right\} }.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $h\left(\mathbf{x}\right)=I_{\left\{ x_{\left(1\right)}>0\right\} }$
\end_inset

 and 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)=\theta^{-n}I_{\left\{ x_{\left(n\right)}<\theta\right\} }$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{x}\right)=x_{\left(n\right)}=\underset{1\leq i\leq n}{\max}x_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from a pdf or pmf 
\begin_inset Formula $f\left(x|\boldsymbol{\theta}\right)$
\end_inset

 that belongs to an exponential family given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\boldsymbol{\theta}\right) & =h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\boldsymbol{\theta}\right)t_{i}\left(x\right)\right)
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\boldsymbol{\theta}=\left(\theta_{1},\ldots,\theta_{d}\right)$
\end_inset

, 
\begin_inset Formula $d\leq k$
\end_inset

.
 Then
\begin_inset Formula 
\begin{flalign*}
T\left(\mathbf{X}\right) & =\left(\sum_{j=1}^{n}t_{1}\left(X_{j}\right),\ldots,\sum_{j=1}^{n}t_{k}\left(X_{j}\right)\right)
\end{flalign*}

\end_inset

is a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is called the 
\series bold
natural sufficient statistic.

\series default
 (This is Theorem 6.2.10 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:suff-statistic-exp-family"

\end_inset


\end_layout

\begin_layout Proof
The joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{j=1}^{n}\left[h\left(x_{j}\right)c\left(\theta\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\theta\right)t_{i}\left(x_{j}\right)\right)\right]\\
 & =\underbrace{c\left(\theta\right)^{n}\exp\left\{ \sum_{i=1}^{k}\left(\omega_{i}\left(\theta\right)\sum_{j=1}^{n}t_{i}\left(x_{j}\right)\right)\right\} }_{g\left(T\left(\mathbf{x}\right)|\theta\right)}\underbrace{\prod_{j=1}^{n}h\left(x_{j}\right)}_{h\left(\mathbf{x}\right)}.
\end{flalign*}

\end_inset

It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(\sum_{j=1}^{n}t_{1}\left(X_{j}\right),\ldots,\sum_{j=1}^{n}t_{k}\left(X_{j}\right)\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Subsection
Minimal sufficient statistics
\end_layout

\begin_layout Standard
In the preceding section, we found one sufficient statistic for each model
 considered.
 In fact, there are many sufficient statistics.
 It is always true that the data 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a sufficient statistic; we can factor the pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 as 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=f\left(T\left(\mathbf{X}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

, where 
\begin_inset Formula $T\left(\mathbf{x}\right)=\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

 for all 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Any one-to-one function of a sufficient statistic is sufficient.
 We might ask whether one sufficient statistic is any better than another.
\end_layout

\begin_layout Definition
A sufficient statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is called a 
\shape italic
minimal sufficient statistic
\shape default
 if, for any other sufficient statistic 
\begin_inset Formula $T'\left(\mathbf{X}\right)$
\end_inset

, 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is a function of 
\begin_inset Formula $T'\left(\mathbf{x}\right)$
\end_inset

, i.e., 
\begin_inset Formula $T\left(\mathbf{x}\right)=g\left(T'\left(\mathbf{x}\right)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Of all sufficient statistics, a minimal sufficient statistic provides the
 greatest reduction of the data.
 In terms of the partition sets described above, if 
\begin_inset Formula $\left\{ B_{t'}:t'\in\mathcal{T}'\right\} $
\end_inset

 are the partition sets of 
\begin_inset Formula $T'\left(\mathbf{x}\right)$
\end_inset

 and 
\begin_inset Formula $\left\{ A_{t}:t\in\mathcal{T}\right\} $
\end_inset

 are the partition sets for 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

, then every 
\begin_inset Formula $B_{t'}$
\end_inset

 is a subset of 
\begin_inset Formula $A_{t}$
\end_inset

.
 The partition associated with the minimal sufficient statistic is the coarest
 possible partition (among those induced by sufficient statistics).
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

 be the pmf or pdf of a sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 Suppose there exists a function 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 such that, for every two sample points 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

, the ratio 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)/f\left(\mathbf{y}|\theta\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

 if and only if 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset

.
 Then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 (This is Theorem 6.2.13 from Casella & Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:minimal-sufficient"

\end_inset


\end_layout

\begin_layout Proof
To simplify the proof, we assume 
\begin_inset Formula $f\left(x|\theta\right)>0$
\end_inset

 for all 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Proof
First, we show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic.
 Let 
\begin_inset Formula $\mathcal{T}=\left\{ t:t=T\left(\mathbf{x}\right),\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 under 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

.
 Define the partition sets induced by 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 as 
\begin_inset Formula $A_{t}=\left\{ \mathbf{x}:T\left(\mathbf{x}\right)=t\right\} $
\end_inset

.
 For each 
\begin_inset Formula $A_{t}$
\end_inset

, choose and fix one element 
\begin_inset Formula $\mathbf{x}_{t}\in A_{t}$
\end_inset

.
 For any 
\begin_inset Formula $\mathbf{x}\in\mathcal{X}$
\end_inset

, 
\begin_inset Formula $\mathbf{x}_{T\left(\mathbf{x}\right)}$
\end_inset

 is the fixed element that is in the same set, 
\begin_inset Formula $A_{t}$
\end_inset

, as 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Since 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{T\left(\mathbf{x}\right)}$
\end_inset

 are in the same set 
\begin_inset Formula $A_{t}$
\end_inset

, 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{x}_{T\left(\mathbf{x}\right)}\right)$
\end_inset

 and, hence, 
\begin_inset Formula $f\left(x|\theta\right)/f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

.
 Thus, we can define a function on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 by 
\begin_inset Formula $h\left(\mathbf{x}\right)=f\left(\mathbf{x}|\theta\right)/f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)$
\end_inset

 and 
\begin_inset Formula $h$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 Define a function on 
\begin_inset Formula $\mathcal{T}$
\end_inset

 by 
\begin_inset Formula $g\left(t|\theta\right)=f\left(\mathbf{x}_{t}|\theta\right)$
\end_inset

.
 Then it can be seen that
\begin_inset Formula 
\begin{flalign*}
f\left(\mathbf{x}|\theta\right) & =\frac{f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)f\left(\mathbf{x}|\theta\right)}{f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)}\\
 & =g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)
\end{flalign*}

\end_inset

and, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Proof
Now to show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is minimal, let 
\begin_inset Formula $T'\left(\mathbf{X}\right)$
\end_inset

 be any other sufficient statistic.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

, there exist functions 
\begin_inset Formula $g'$
\end_inset

 and 
\begin_inset Formula $h'$
\end_inset

 such that 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g'\left(T'\left(\mathbf{x}\right)|\theta\right)h'\left(\mathbf{x}\right)$
\end_inset

.
 Let 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 be any two sample points with 
\begin_inset Formula $T'\left(\mathbf{x}\right)=T'\left(\mathbf{y}\right)$
\end_inset

.
 Then
\begin_inset Formula 
\begin{flalign*}
\frac{f\left(\mathbf{x}|\theta\right)}{f\left(\mathbf{y}|\theta\right)} & =\frac{g'\left(T'\left(\mathbf{x}\right)|\theta\right)h'\left(\mathbf{x}\right)}{g'\left(T'\left(\mathbf{y}\right)|\theta\right)h'\left(\mathbf{y}\right)}\\
 & =\frac{h'\left(\mathbf{x}\right)}{h'\left(\mathbf{y}\right)}.
\end{flalign*}

\end_inset

Since this ratio does not depend on 
\begin_inset Formula $\theta$
\end_inset

, the assumptions of the theorem imply that 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is a function of 
\begin_inset Formula $T'\left(\mathbf{x}\right)$
\end_inset

 and 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is minimal.
\end_layout

\begin_layout Corollary
If the partition of the sample space induced by 
\begin_inset Formula $f\left(x|\theta\right)/f\left(y|\theta\right)$
\end_inset

 is equivalent to that induced by 
\begin_inset Formula $T$
\end_inset

, then 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient.
\begin_inset CommandInset label
LatexCommand label
name "cor:minimal-sufficient-partition"

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Minimal sufficient statistic for a Bernoulli random variable]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},X_{2}\sim\mbox{Bernoulli}\left(p\right)$
\end_inset

.
 Let 
\begin_inset Formula $V=X_{1}$
\end_inset

, 
\begin_inset Formula $T=\sum_{i}X_{i}$
\end_inset

, and 
\begin_inset Formula $U=\left(T,X_{1}\right)$
\end_inset

.
 Determine whether 
\begin_inset Formula $V$
\end_inset

, 
\begin_inset Formula $T$
\end_inset

, or 
\begin_inset Formula $U$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 
\end_layout

\begin_layout Example
The set of outcomes and the statistics are shown below.
\end_layout

\begin_layout Example
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $X_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $X_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $V$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $T$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $U$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(0,0\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1,0\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1,1\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(2,1\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $\mathcal{V}=\left\{ v:v=x_{1},\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

, the sample space of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, under 
\begin_inset Formula $V$
\end_inset

.
 From the table above, we have 
\begin_inset Formula $\mathcal{V}=\left\{ 0,1\right\} $
\end_inset

.
 Let 
\begin_inset Formula $W_{t}=\left\{ \mathbf{x}:X_{1}=v\right\} $
\end_inset

 be the partition sets induced by 
\begin_inset Formula $V$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
W_{0} & =\left\{ \left(0,0\right),\left(0,1\right)\right\} \\
W_{1} & =\left\{ \left(1,0\right),\left(1,1\right)\right\} .
\end{flalign*}

\end_inset

The joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},X_{2}$
\end_inset

 conditioned on 
\begin_inset Formula $V$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\left(\mathbf{X}|V\right)}\left(\mathbf{x}|v\right) & =P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} |\left\{ V=v\right\} \right)\\
 & =\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{V}\left(v|p\right)}\\
 & =\frac{\left(p^{x_{1}}\left(1-p\right)^{1-x_{1}}\right)\left(p^{x_{2}}\left(1-p\right)^{1-x_{2}}\right)}{p^{x_{1}}\left(1-p\right)^{1-x_{1}}}\\
 & =p^{x_{2}}\left(1-p\right)^{1-x_{2}}.
\end{flalign*}

\end_inset

Clearly, this expression depends on 
\begin_inset Formula $p$
\end_inset

, so 
\begin_inset Formula $V$
\end_inset

 is not a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $\mathcal{T}=\left\{ t:t=\sum_{i}x_{i},\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 under 
\begin_inset Formula $T$
\end_inset

.
 From the table above, we have 
\begin_inset Formula $\mathcal{T}=\left\{ 0,1,2\right\} $
\end_inset

.
 Let 
\begin_inset Formula $A_{t}=\left\{ \mathbf{x}:\sum_{i}x_{i}=t\right\} $
\end_inset

 be the partition sets induced by 
\begin_inset Formula $T$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
A_{0} & =\left\{ \left(0,0\right)\right\} \\
A_{1} & =\left\{ \left(0,1\right),\left(1,0\right)\right\} \\
A_{2} & =\left\{ \left(1,1\right)\right\} .
\end{flalign*}

\end_inset

We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

 that 
\begin_inset Formula $T$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 We will now show that 
\begin_inset Formula $T$
\end_inset

 is minimal.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, the pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 can be written as 
\begin_inset Formula $p_{\mathbf{X}}\left(\mathbf{x}|p\right)=g\left(T\left(\mathbf{x}\right)|p\right)h\left(\mathbf{x}\right)$
\end_inset

, where 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

 and 
\begin_inset Formula 
\begin{flalign*}
g\left(T\left(\mathbf{x}\right)|p\right) & =\left(1-p\right)^{n}\left(\frac{p}{1-p}\right)^{\sum_{i=1}^{n}x_{i}}\\
 & =\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{\mathbf{X}}\left(\mathbf{y}|p\right)} & =\frac{g\left(T\left(\mathbf{x}\right)|p\right)h\left(\mathbf{x}\right)}{g\left(T\left(\mathbf{y}\right)|p\right)h\left(\mathbf{y}\right)}\\
 & =\frac{\left[\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}\right]\cdot1}{\left[\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{y_{1}+y_{2}}\right]\cdot1}\\
 & =\frac{\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}}{\left(\frac{p}{1-p}\right)^{y_{1}+y_{2}}}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

, so that 
\begin_inset Formula $p/\left(1-p\right)>0$
\end_inset

, so that the ratio shown above will be defined.
 This ratio will be constant as a function of 
\begin_inset Formula $p$
\end_inset

 if and only if 
\begin_inset Formula $x_{1}+x_{2}=y_{1}+y_{2}$
\end_inset

, i.e., if 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

 that 
\begin_inset Formula $T$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $\mathcal{U}=\left\{ \mathbf{u}:\mathbf{u}=\left(T\left(\mathbf{x}\right),x_{1}\right),\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 under 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
 From the table above, we have 
\begin_inset Formula $\mathcal{U}=\left\{ \left(0,0\right),\left(1,0\right),\left(1,1\right),\left(2,1\right)\right\} $
\end_inset

.
 Let 
\begin_inset Formula $B_{t}=\left\{ \mathbf{x}:\left(T\left(\mathbf{x}\right),x_{1}\right)=\mathbf{u}\right\} $
\end_inset

 be the partition sets induced by 
\begin_inset Formula $\mathbf{U}$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
B_{\left(0,0\right)} & =\left\{ \left(0,0\right)\right\} \\
B_{\left(1,0\right)} & =\left\{ \left(1,0\right)\right\} \\
B_{\left(1,1\right)} & =\left\{ \left(0,1\right)\right\} \\
B_{\left(2,1\right)} & =\left\{ \left(1,1\right)\right\} .
\end{flalign*}

\end_inset

The pmf of 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{U}}\left(\mathbf{u}|p\right) & =P\left(\left\{ T\left(\mathbf{X}\right)=t\right\} \cap\left\{ X_{1}=x_{1}\right\} \right)\\
 & =P\left(\left\{ T\left(\mathbf{X}\right)=t\right\} |\left\{ X_{1}=x_{1}\right\} \right)\cdot P\left(\left\{ X_{1}=x_{1}\right\} \right)\\
 & =\left(\frac{p_{T\left(\mathbf{x}\right)}\left(t|p\right)}{p_{X_{1}}\left(x_{1}|p\right)}\right)\cdot p_{X_{1}}\left(x_{1}|p\right)\\
 & =p_{T\left(\mathbf{x}\right)}\left(t|p\right).
\end{flalign*}

\end_inset

To show that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

, we examine the ratio
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{x}|\theta\right)\right)} & =\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{\mathbf{U}}\left(\mathbf{u}|p\right)}\\
 & =\frac{\left(p^{x_{1}}\left(1-p\right)^{1-x_{1}}\right)\left(p^{x_{2}}\left(1-p\right)^{1-x_{2}}\right)}{\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}}\\
 & =\frac{p^{x_{1}+x_{2}}\left(1-p\right)^{2-x_{1}-x_{2}}}{\left(1-p\right)^{2}p^{x_{1}+x_{2}}\left(1-p\right)^{-x_{1}-x_{2}}}\\
 & =\frac{p^{x_{1}+x_{x}}\left(1-p\right)^{2-x_{1}-x_{2}}}{p^{x_{1}+x_{2}}\left(1-p\right)^{2-x_{1}-x_{x}}}\\
 & =1.
\end{flalign*}

\end_inset

Because the ratio 
\begin_inset Formula $p_{\mathbf{X}}\left(\mathbf{x}|p\right)/p_{\mathbf{U}}\left(\mathbf{u}|p\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $p$
\end_inset

, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

 that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 We will now check whether 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is minimal.
 We showed above that 
\begin_inset Formula 
\begin{flalign*}
\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{\mathbf{X}}\left(\mathbf{y}|p\right)} & =\frac{\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}}{\left(\frac{p}{1-p}\right)^{y_{1}+y_{2}}}.
\end{flalign*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

, if this ratio is constant if and only if 
\begin_inset Formula $\mathbf{U}\left(\mathbf{x}\right)=\mathbf{U}\left(\mathbf{y}\right)$
\end_inset

, then 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is minimal.
 We have 
\begin_inset Formula 
\begin{flalign*}
\mathbf{U}\left(\mathbf{x}\right) & =\left(T\left(\mathbf{x}\right),x_{1}\right)\\
 & =\left(x_{1}+x_{2},x_{1}\right)
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
\mathbf{U}\left(\mathbf{y}\right) & =\left(T\left(\mathbf{y}\right),y_{1}\right)\\
 & =\left(y_{1}+y_{2},y_{1}\right).
\end{flalign*}

\end_inset

As shown above, the ratio 
\begin_inset Formula $p_{\mathbf{X}}\left(\mathbf{x}|p\right)/p_{\mathbf{X}}\left(\mathbf{y}|p\right)$
\end_inset

 will be constant if and only if 
\begin_inset Formula $x_{1}+y_{1}=x_{2}+y_{2}$
\end_inset

, which does not imply 
\begin_inset Formula $x_{1}=y_{1}$
\end_inset

.
 Thus, it is not the case that the ratio will be constant if and only if
 
\begin_inset Formula $\mathbf{U}\left(\mathbf{x}\right)=\mathbf{U}\left(\mathbf{y}\right)$
\end_inset

, so it follows that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 Another way to see that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not minimal is to observe that 
\begin_inset Formula 
\begin{flalign*}
B_{\left(0,0\right)} & =\left\{ \left(0,0\right)\right\} =A_{0}\\
B_{\left(1,0\right)} & =\left\{ \left(1,0\right)\right\} \subset A_{1}\\
B_{\left(1,1\right)} & =\left\{ \left(0,1\right)\right\} \subset A_{1}\\
B_{\left(2,1\right)} & =\left\{ \left(1,1\right)\right\} =A_{2}.
\end{flalign*}

\end_inset

Thus, 
\begin_inset Formula $B_{t}\subseteq A_{t}$
\end_inset

, i.e., the partition of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 induced by 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not equivalent to that induced by 
\begin_inset Formula $T$
\end_inset

, which is minimal sufficient, so by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:minimal-sufficient-partition"

\end_inset

, 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 Conversely, note that the statistic 
\begin_inset Formula $W=17T$
\end_inset

 generates the same partition as 
\begin_inset Formula $T$
\end_inset

, so 
\begin_inset Formula $W$
\end_inset

 is also minimal sufficient.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

Minimal sufficient statistic for an exponential random variable
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\mbox{Exp}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\lambda\right) & =\lambda e^{-\lambda x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x>0$
\end_inset

 and 
\begin_inset Formula $f\left(x|\lambda\right)=0$
\end_inset

 otherwise, so that a joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right) & =\prod_{i=1}^{n}\lambda e^{-\lambda x}I_{\left\{ x_{i}>0\right\} }\\
 & =\lambda^{n}e^{-\sum_{i=1}^{n}\lambda x_{i}}I_{\left\{ x_{\left(1\right)}>0\right\} }.
\end{flalign*}

\end_inset

Find a minimal sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:minimal-sufficient-exp-rv"

\end_inset


\end_layout

\begin_layout Example
We have
\begin_inset Formula 
\begin{flalign*}
\frac{f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right)}{f_{\mathbf{X}}\left(\mathbf{y}|\lambda\right)} & =\frac{\lambda^{n}e^{-\lambda\sum_{i=1}^{n}x_{i}}I_{\left\{ x_{\left(1\right)}>0\right\} }}{\lambda^{n}e^{-\lambda\sum_{i=1}^{n}y_{i}}I_{\left\{ y_{\left(1\right)}>0\right\} }}\\
 & =e^{-\lambda\sum_{i=1}^{n}x_{i}}e^{\lambda\sum_{i=1}^{n}y_{i}}\left(\frac{I_{\left\{ x_{\left(1\right)}>0\right\} }}{I_{\left\{ y_{\left(1\right)}>0\right\} }}\right)\\
 & =e^{-\lambda\left(\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}y_{i}\right)}\left(\frac{I_{\left\{ x_{\left(1\right)}>0\right\} }}{I_{\left\{ y_{\left(1\right)}>0\right\} }}\right).
\end{flalign*}

\end_inset

This ratio will be constant as a function of 
\begin_inset Formula $\lambda$
\end_inset

 if and only if 
\begin_inset Formula $\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}y_{i}=0$
\end_inset

, i.e., if 
\begin_inset Formula $\sum_{i=1}^{n}x_{i}=\sum_{i=1}^{n}y_{i}$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Minimal sufficient statistics for an exponential family]
\end_layout

\end_inset

The set of sufficient statistics 
\begin_inset Formula $T\left(X\right)=\left\{ T_{1}\left(X\right),\ldots,T_{n}\left(X\right)\right\} $
\end_inset

 from an exponential family are minimal sufficient.
 Recall that the pmf or pdf of an exponential family can be written as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} .
\end{flalign*}

\end_inset

We have
\begin_inset Formula 
\begin{flalign*}
\frac{f\left(x|\theta\right)}{f\left(y|\theta\right)} & =\frac{h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} }{h^{*}\left(y\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(y\right)\right\} }\\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} \exp\left\{ -\sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(y\right)\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \left[\sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right]-\left[\sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(y\right)\right]\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \left[\omega_{1}\left(\theta\right)T_{1}\left(x\right)+\cdots+\omega_{k}T_{k}\left(x\right)\right]-\left[\omega_{1}\left(\theta\right)T_{1}\left(y\right)+\cdots+\omega_{k}\left(\theta\right)T_{k}\left(y\right)\right]\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \omega_{1}\left(\theta\right)\left[T_{1}\left(x\right)-T_{1}\left(y\right)\right]+\cdots+\omega_{k}\left[T_{k}\left(x\right)-T_{k}\left(y\right)\right]\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)\left[T_{j}\left(x\right)-T_{j}\left(y\right)\right]\right\} .
\end{flalign*}

\end_inset

This ratio will be constant as a function of 
\begin_inset Formula $\theta$
\end_inset

 if and only if 
\begin_inset Formula $T\left(X\right)=T\left(Y\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

 that 
\begin_inset Formula $T\left(X\right)=\left\{ T_{1}\left(X\right),\ldots,T_{n}\left(X\right)\right\} $
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:min-sufficient-exp-family"

\end_inset


\end_layout

\begin_layout Subsection
Ancillary statistics
\end_layout

\begin_layout Definition
A statistic 
\begin_inset Formula $S\left(\mathbf{X}\right)$
\end_inset

 whose distribution does not depend on the parameter 
\begin_inset Formula $\theta$
\end_inset

 is called an 
\shape italic
ancillary statistic
\shape default
.
\end_layout

\begin_layout Standard
Alone, an ancillary statistic contains no information about 
\begin_inset Formula $\theta$
\end_inset

.
 When used in conjunction with other statistics, it sometimes contains valuable
 information for inference about 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
arraystretch}{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Bivariate transformations
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 be a random vector with a known probability distribution.
 Now consider a new bivariate random vector 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 defined by 
\begin_inset Formula $U=g_{1}\left(X,Y\right)$
\end_inset

 and 
\begin_inset Formula $V=g_{2}\left(X,Y\right)$
\end_inset

, where 
\begin_inset Formula $g_{1}\left(x,y\right)$
\end_inset

 and 
\begin_inset Formula $g_{2}\left(x,y\right)$
\end_inset

 are some specified functions.
 If 
\begin_inset Formula $B$
\end_inset

 is any subset of 
\begin_inset Formula $\mathbb{R}^{2}$
\end_inset

, then 
\begin_inset Formula $\left(U,V\right)\in B$
\end_inset

 if and only if 
\begin_inset Formula $\left(X,Y\right)\in A$
\end_inset

, where 
\begin_inset Formula $A=\left\{ \left(x,y\right):\left(g_{1}\left(x,y\right),g_{2}\left(x,y\right)\right)\in B\right\} $
\end_inset

.
 Thus 
\begin_inset Formula $P\left(\left\{ \left(U,V\right)\in B\right\} \right)=P\left(\left\{ \left(X,Y\right)\in A\right\} \right)$
\end_inset

, and the probability distribution of 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 is completely determined by the probability distribution of 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 is a continuous random vector with joint pdf 
\begin_inset Formula $f_{X,Y}\left(x,y\right)$
\end_inset

, then the joint pdf of 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 can be expressed in terms of 
\begin_inset Formula $f_{X,Y}\left(x,y\right)$
\end_inset

.
 Let 
\begin_inset Formula 
\begin{flalign*}
\mathcal{A} & =\left\{ \left(x,y\right):f_{X,Y}\left(x,y\right)>0\right\} 
\end{flalign*}

\end_inset

 and 
\begin_inset Formula 
\begin{flalign*}
\mathcal{B} & =\left\{ \left(u,v\right):u=g_{1}\left(x,y\right),v=g_{2}\left(x,y\right),\left(x,y\right)\in\mathcal{A}\right\} .
\end{flalign*}

\end_inset

The joint pdf 
\begin_inset Formula $f_{U,V}$
\end_inset


\begin_inset Formula $\left(u,v\right)$
\end_inset

 will be positive on the set 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 For the simplest version of this result we assume that the transformation
 
\begin_inset Formula $u=g_{1}\left(x,y\right)$
\end_inset

 and 
\begin_inset Formula $v=g_{2}\left(x,y\right)$
\end_inset

 defines a one-to-one transformation of 
\begin_inset Formula $\mathcal{A}$
\end_inset

 onto 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 We are assuming that for each 
\begin_inset Formula $\left(u,v\right)\in\mathcal{B}$
\end_inset

, there is only one 
\begin_inset Formula $\left(x,y\right)\in\mathcal{A}$
\end_inset

 such that 
\begin_inset Formula $\left(u,v\right)=\left(g_{1}\left(x,y\right),g_{2}\left(x,y\right)\right)$
\end_inset

.
 For such a one-to-one, onto transformation, we can solve the equations
 
\begin_inset Formula $u=g_{1}\left(x,y\right)$
\end_inset

 and 
\begin_inset Formula $v=g_{2}\left(x,y\right)$
\end_inset

 for 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 in terms of 
\begin_inset Formula $u$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

.
 We will denote this inverse transformation by 
\begin_inset Formula $x=h_{1}\left(u,v\right)$
\end_inset

 and 
\begin_inset Formula $y=h_{2}\left(u,v\right)$
\end_inset

.
 The role played by the derivative in the univariate case is now played
 by quantity called the Jacobian of the transformation.
 This function of 
\begin_inset Formula $\left(u,v\right)$
\end_inset

, denoted by 
\begin_inset Formula $J$
\end_inset

, is the determinant of a matrix of partial derivatives, and is defined
 by
\begin_inset Formula 
\begin{flalign*}
J= & \left|\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial y}{\partial u}\frac{\partial x}{\partial v},
\end{flalign*}

\end_inset

where 
\begin_inset Formula 
\[
\frac{\partial x}{\partial u}=\frac{\partial h_{1}\left(u,v\right)}{\partial u},\quad\frac{\partial x}{\partial v}=\frac{\partial h_{1}\left(u,v\right)}{\partial v},\quad\frac{\partial y}{\partial u}=\frac{\partial h_{2}\left(u,v\right)}{\partial u},\quad\mbox{and}\quad\frac{\partial y}{\partial v}=\frac{\partial h_{2}\left(u,v\right)}{\partial v}.
\]

\end_inset

Then, the joint pdf of 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 is 0 outside the set 
\begin_inset Formula $\mathcal{B}$
\end_inset

 and on the set 
\begin_inset Formula $\mathcal{B}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{U,V}\left(u,v\right) & =f_{X,Y}\left(h_{1}\left(u,v\right),h_{2}\left(u,v\right)\right)\left|J\right|.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Remark*
This technique, which is easily generalized to more than two variables,
 can be used to find the pdf of some function of interest by first finding
 the joint pdf of that function and another, conveniently chosen function,
 then integrating the resulting joint pdf with respect to the second function,
 which gives the (marginal) pdf of the function of interest.
 This technique is demonstrated in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:ancillary-stat-uniform-range"

\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Distribution of the sum of Poisson variables]
\end_layout

\end_inset

If 
\begin_inset Formula $X\sim\mbox{Poisson}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $Y\sim\mbox{Poisson}\left(\lambda\right)$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent, then 
\begin_inset Formula $X+Y\sim\mbox{Poisson}\left(\theta+\lambda\right)$
\end_inset

.
 (This is Theorem 4.3.2 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:dist-of-sum-of-poisson-rv"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ancillary statistic for a uniform random variable]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid uniform observations on the interval 
\begin_inset Formula $\left(\theta,\theta+1\right)$
\end_inset

, 
\begin_inset Formula $-\infty<\theta<\infty$
\end_inset

.
 Let 
\begin_inset Formula $X_{\left(1\right)}<\cdots<X_{\left(n\right)}$
\end_inset

 be the order statistics from the sample.
 Show that the range statistic 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

 is an ancillary statistic.
 (This is example 6.2.17 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "exa:ancillary-stat-uniform-range"

\end_inset


\end_layout

\begin_layout Example
The pdf of each 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x|\theta\right) & =\frac{1}{\left(\theta+1\right)-\theta}\\
 & =1,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\theta<x<\theta+1$
\end_inset

 and 
\begin_inset Formula $f_{X_{i}}\left(x|\theta\right)=0$
\end_inset

 otherwise, so that the cdf of each 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\[
F_{X_{i}}\left(x|\theta\right)=\begin{cases}
0, & x\leq\theta\\
\int_{\theta}^{x}1\mbox{d}x=x-\theta, & \theta<x<\theta+1\\
1, & x\geq\theta+1
\end{cases}.
\]

\end_inset

Let 
\begin_inset Formula $u=x_{\left(1\right)}$
\end_inset

 and let 
\begin_inset Formula $v=x_{\left(n\right)}$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-joint-pdf"

\end_inset

, the joint pdf of 
\begin_inset Formula $X_{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(1\right)},X_{\left(n\right)}}\left(u,v\right) & =\frac{n!}{\left(1-1\right)!\left(n-1-1\right)!\left(n-n\right)!}f_{X}\left(u\right)f_{X}\left(v\right)\left[F_{X}\left(u\right)\right]^{1-1}\\
 & \quad\cdot\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{n-1-1}\left[1-F_{X}\left(v\right)\right]^{n-n}\\
 & =\frac{n!}{\left(n-2\right)!}f_{X}\left(u\right)f_{X}\left(v\right)\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{n-2}\\
 & =\frac{n!}{\left(n-2\right)!}\cdot1\cdot1\cdot\left[\left(v-\theta\right)-\left(u-\theta\right)\right]^{n-2}\\
 & =\frac{n!}{\left(n-2\right)!}\left(v-u\right)^{n-2}\\
 & =\frac{n\left(n-1\right)\left(n-2\right)!}{\left(n-2\right)!}\left(v-u\right)^{n-2}\\
 & =n\left(n-1\right)\left(v-u\right)^{n-2}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\theta<u<v<\theta+1$
\end_inset

 and 
\begin_inset Formula $f_{X_{\left(1\right)},X_{\left(n\right)}}\left(u,v\right)=0$
\end_inset

 otherwise.
 Let 
\begin_inset Formula 
\begin{flalign*}
\mathcal{A} & =\left\{ \left(u,v\right):f_{X_{\left(1\right)},X_{\left(n\right)}}\left(u,v\right)>0\right\} ,
\end{flalign*}

\end_inset

and make the transformation 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $M=\left(X_{\left(1\right)}+X_{\left(n\right)}\right)/2$
\end_inset

, so that we have 
\begin_inset Formula 
\begin{flalign*}
\mathcal{B} & =\left\{ \left(r,m\right):r=v-u,m=\left(u+v\right)/2,\left(u,v\right)\in\mathcal{A}\right\} .
\end{flalign*}

\end_inset

Then, 
\begin_inset Formula $f_{R,M}\left(r,m\right)$
\end_inset

 will be positive on 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 We have the inverse transformation 
\begin_inset Formula 
\begin{flalign*}
X_{\left(1\right)} & =2M-X_{\left(n\right)}\\
 & =2M-\left(R+X_{\left(1\right)}\right)\\
\Leftrightarrow2X_{\left(1\right)} & =2M-R\\
\Leftrightarrow X_{\left(1\right)} & =\left(2M-R\right)/2\\
\Leftrightarrow u & =\left(2m-r\right)/2
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
X_{\left(n\right)} & =2M-X_{\left(1\right)}\\
 & =2M-\left(X_{\left(n\right)}-R\right)\\
\Leftrightarrow2X_{\left(n\right)} & =2M+R\\
\Leftrightarrow X_{\left(n\right)} & =\left(2M+R\right)/2\\
\Leftrightarrow v & =\left(2m+r\right)/2.
\end{flalign*}

\end_inset

The Jacobian of the transformation is given by
\begin_inset Formula 
\begin{flalign*}
J & =\begin{vmatrix}\frac{\partial u}{\partial r} & \frac{\partial u}{\partial m}\\
\frac{\partial v}{\partial r} & \frac{\partial v}{\partial m}
\end{vmatrix}\\
 & =\begin{vmatrix}-\frac{1}{2} & 1\\
\frac{1}{2} & 1
\end{vmatrix}\\
 & =\left(-\frac{1}{2}\right)\left(1\right)-\left(\frac{1}{2}\right)\left(1\right)\\
 & =-1.
\end{flalign*}

\end_inset

We will now find the range for 
\begin_inset Formula $f_{R,M}$
\end_inset

.
 We have
\begin_inset Formula 
\begin{flalign*}
u>\theta & \Rightarrow m-\frac{r}{2}>\theta\\
 & \Rightarrow m>\theta+\frac{r}{2}\\
v>u & \Rightarrow m+\frac{r}{2}>m-\frac{r}{2}\\
 & \Rightarrow\frac{r}{2}>-\frac{r}{2}\\
 & \Rightarrow r>0\\
v<\theta+1 & \Rightarrow\frac{2m+r}{2}<\theta+1\\
 & \Rightarrow m<\theta+1-\frac{r}{2}
\end{flalign*}

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3.5, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,4,0.5,4))
\end_layout

\begin_layout Plain Layout

r <- seq(-1, 4, length=5000) 
\end_layout

\begin_layout Plain Layout

# set xaxs="i" to remove the default 4% padding beyond the ylim
\end_layout

\begin_layout Plain Layout

plot(r, 1 + r / 2, type = "l", xlab = "r", ylab = "m", yaxt = "n", xaxt
 = "n", xaxs = "i", xlim = c(0, 4), ylim = c(0,3), cex.lab = 0.75)
\end_layout

\begin_layout Plain Layout

lines(r, 2 - r / 2, type="l")
\end_layout

\begin_layout Plain Layout

polygon(c(-1,1,-1), c(2.5,1.5,0.5), density=20, angle=0)
\end_layout

\begin_layout Plain Layout

axis(1, at = c(0,1), labels = c(0,1), cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

axis(2, at = c(1,2), labels = c(expression(theta), expression(theta + 1)),
 cex.axis = 0.75, las = 2)
\end_layout

\begin_layout Plain Layout

# use las to rotate the strings and padj to move them along the axis
\end_layout

\begin_layout Plain Layout

mtext(expression(m == theta + frac(r, 2)), side=4, line=0.5, las=2, padj=-1.5,
 cex=0.75)
\end_layout

\begin_layout Plain Layout

mtext(expression(m == theta + 1 - frac(r, 2)), side=4, line=0.5, las=2, padj=2.5,
 cex=0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
From the figure above, we see that the region where 
\begin_inset Formula $f_{R,M}$
\end_inset

 is positive is bounded by 
\begin_inset Formula $r>0$
\end_inset

, 
\begin_inset Formula $m>\theta+\left(r/2\right)$
\end_inset

, and 
\begin_inset Formula $m<\theta+1-\left(r/2\right)$
\end_inset

.
 The upper limit of integration for 
\begin_inset Formula $r$
\end_inset

 is given by the intersection of 
\begin_inset Formula $m=\theta+\left(r/2\right)$
\end_inset

 and 
\begin_inset Formula $m=\theta+1-\left(r/2\right)$
\end_inset

.
 Setting them equal, we have
\begin_inset Formula 
\begin{flalign*}
\theta+\frac{r}{2} & =\theta+1-\frac{r}{2}\\
\Leftrightarrow r & =1.
\end{flalign*}

\end_inset

Then, the joint pdf of 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $M$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{R,M}\left(r,m\right) & =f_{X_{\left(1\right)},X_{\left(n\right)}}\left(\frac{2m-r}{2},\frac{2m+r}{2}\right)\left|J\right|\\
 & =n\left(n-1\right)\left(\frac{2m+r}{2}-\frac{2m-r}{2}\right)^{n-2}\left|-1\right|\\
 & =n\left(n-1\right)\left(\frac{2m+r-2m+r}{2}\right)^{n-2}\\
 & =n\left(n-1\right)\left(\frac{2r}{2}\right)^{n-2}\\
 & =n\left(n-1\right)r^{n-2}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $0<r<1$
\end_inset

 and 
\begin_inset Formula $\theta+\left(r/2\right)<m<\theta+1-\left(r/2\right)$
\end_inset

 and 
\begin_inset Formula $f_{R,M}\left(r,m\right)=0$
\end_inset

 otherwise.
 We will find the (marginal) pdf of 
\begin_inset Formula $R$
\end_inset

 by integrating the joint pdf with respect to 
\begin_inset Formula $m$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
f_{R}\left(r|\theta\right) & =\int_{\theta+\left(r/2\right)}^{\theta+1-\left(r/2\right)}n\left(n-1\right)r^{n-2}\mbox{d}m\\
 & =n\left(n-1\right)r^{n-2}\left[m\Big\rvert_{\theta+\left(r/2\right)}^{\theta+1-\left(r/2\right)}\right]\\
 & =n\left(n-1\right)r^{n-2}\left[\theta+1-\frac{r}{2}-\left(\theta+\frac{r}{2}\right)\right]\\
 & =n\left(n-1\right)r^{n-2}\left[1-\frac{r}{2}-\frac{r}{2}\right]\\
 & =n\left(n-1\right)r^{n-2}\left(1-r\right)
\end{flalign*}

\end_inset

where 
\begin_inset Formula $0<r<1$
\end_inset

.
 This expression is independent of 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $R$
\end_inset

 is ancillary.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ancillary statistic for location family]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from a location parameter family with cdf 
\begin_inset Formula $F\left(x-\theta\right)$
\end_inset

, 
\begin_inset Formula $-\infty<\theta<\infty$
\end_inset

.
 We will show that the range, 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

, is an ancillary statistic.
 (This is example 6.2.18 from Casella & Berger.)
\end_layout

\begin_layout Example
We will use 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:location-scale-family"

\end_inset

 and work with 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 iid observations from 
\begin_inset Formula $F\left(x\right)$
\end_inset

 (corresponding to 
\begin_inset Formula $\theta=0$
\end_inset

) with 
\begin_inset Formula $X_{1}=Z_{1}+\theta,\ldots,X_{n}=Z_{n}+\theta$
\end_inset

.
 Thus, the cdf of the range statistic, 
\begin_inset Formula $R$
\end_inset

, is
\begin_inset Formula 
\begin{flalign*}
F_{R}\left(r|\theta\right) & =P_{\theta}\left(\left\{ R\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ \max_{i}X_{i}-\min_{i}X_{i}\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ \max\left(X_{1},\ldots,X_{n}\right)-\min\left(X_{1},\ldots,X_{n}\right)\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ \max\left(Z_{1}+\theta,\ldots,Z_{n}+\theta\right)-\min\left(Z_{1}+\theta,\ldots,Z_{n}+\theta\right)\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z_{\left(n\right)}+\theta-\left(Z_{\left(1\right)}+\theta\right)\right\} \leq r\right)\\
 & =P_{\theta}\left(\left\{ Z_{\left(n\right)}-Z_{\left(1\right)}\leq r\right\} \right).
\end{flalign*}

\end_inset

This expression does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows that the cdf of 
\begin_inset Formula $R$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

 and therefore 
\begin_inset Formula $R$
\end_inset

 is an ancillary statistic.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ancillary statistic for scale family]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from a scale parameter family with cdf 
\begin_inset Formula $F\left(x/\sigma\right)$
\end_inset

, 
\begin_inset Formula $\sigma>0$
\end_inset

.
 Then, any statistic that depends on the sample only through the 
\begin_inset Formula $n-1$
\end_inset

 values 
\begin_inset Formula $X_{1}/X_{n},\ldots,X_{n-1}/X_{n}$
\end_inset

 is an ancillary statistic.
 (This is example 6.2.19 from Casella & Berger.)
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 be iid observations from 
\begin_inset Formula $F\left(x\right)$
\end_inset

 (corresponding to 
\begin_inset Formula $\sigma=1$
\end_inset

) with 
\begin_inset Formula $X_{i}=\sigma Z_{i}$
\end_inset

.
 The joint cdf of 
\begin_inset Formula $X_{1}/X_{n},\ldots,X_{n-1}/X_{n}$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
F\left(y_{1},\ldots,y_{n-1}|\sigma\right) & =P_{\sigma}\left(\left\{ \frac{X_{1}}{X_{n}}\leq y_{1}\right\} \cap\cdots\cap\left\{ \frac{X_{n-1}}{X_{n}}\leq y_{n-1}\right\} \right)\\
 & =P_{\sigma}\left(\left\{ \frac{\sigma Z_{1}}{\sigma Z_{n}}\leq y_{1}\right\} \cap\cdots\cap\left\{ \frac{\sigma Z_{n-1}}{\sigma Z_{n}}\leq y_{n-1}\right\} \right)\\
 & =P_{\sigma}\left(\left\{ \frac{Z_{1}}{Z_{n}}\leq y_{1}\right\} \cap\cdots\cap\left\{ \frac{Z_{n-1}}{Z_{n}}\leq y_{n-1}\right\} \right).
\end{flalign*}

\end_inset

The last probability does not depend on 
\begin_inset Formula $\sigma$
\end_inset

 because the distribution of 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 does not depend on 
\begin_inset Formula $\sigma$
\end_inset

.
 So the distribution of 
\begin_inset Formula $X_{1}/X_{n},\ldots,X_{n-1}/X_{n}$
\end_inset

 is independent of 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Mixture of normal distributions]
\end_layout

\end_inset

Sometimes, an ancillary statistic is viewed in conjunction with another
 statistic, and together they form a minimal sufficient statistic, i.e., 
\begin_inset Formula $S=\left(T,C\right)$
\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is ancillary for 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient conditional on 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $Y$
\end_inset

 is a mixture of normal distributions 
\begin_inset Formula $N\left(\mu,\sigma_{0}^{2}\right)$
\end_inset

 and 
\begin_inset Formula $N\left(\mu,\sigma_{1}^{2}\right)$
\end_inset

 with 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 and 
\begin_inset Formula $\sigma_{1}^{2}$
\end_inset

 known (for example, any two of the three distributions in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-of-scale-normal}
\end_layout

\end_inset

).
 Let 
\begin_inset Formula $C$
\end_inset

 be defined as
\begin_inset Formula 
\[
C=\begin{cases}
0, & \mbox{if }Y\sim N\left(\mu,\sigma_{0}^{2}\right)\\
1, & \mbox{if }Y\sim N\left(\mu,\sigma_{1}^{2}\right)
\end{cases}.
\]

\end_inset


\begin_inset Formula $Y$
\end_inset

 is equally likely to be 
\begin_inset Formula $N\left(\mu,\sigma_{0}^{2}\right)$
\end_inset

 or 
\begin_inset Formula $N\left(\mu,\sigma_{1}^{2}\right)$
\end_inset

, so 
\begin_inset Formula $P\left(\left\{ Y\sim N\left(\mu,\sigma_{0}^{2}\right)\right\} \right)=P\left(\left\{ Y\sim N\left(\mu,\sigma_{1}^{2}\right)\right\} \right)=1/2$
\end_inset

.
 Then, the joint pdf of 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $C$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{C,Y}\left(c,y\right) & =P\left(\left\{ C=c\right\} \cap\left\{ Y=y\right\} \right)\\
 & =P\left(\left\{ Y=y\right\} |\left\{ C=c\right\} \right)\cdot P\left(\left\{ C=c\right\} \right)\\
 & =\frac{1}{2}\frac{1}{\sqrt{2\pi\sigma_{c}^{2}}}e^{-\left(y-\mu\right)^{2}/\left(2\sigma_{c}^{2}\right)}\\
 & =\frac{1}{2}\frac{1}{\sqrt{2\pi\sigma_{c}^{2}}}\exp\left\{ -\frac{y^{2}}{2\sigma_{c}^{2}}-\frac{\mu^{2}}{2\sigma_{c}^{2}}+\frac{\mu y}{\sigma_{c}^{2}}\right\} \\
 & =\underbrace{\frac{1}{2}\frac{1}{\sqrt{2\pi\sigma_{c}^{2}}}\exp\left\{ -\frac{y^{2}}{2\sigma_{c}^{2}}\right\} }_{h\left(y\right)}\exp\left\{ \underbrace{-\frac{\mu^{2}}{2}}_{\omega_{1}\left(\mu\right)}\underbrace{\frac{1}{\sigma_{c}^{2}}}_{t_{1}\left(y\right)}+\underbrace{\mu}_{\omega_{2}\left(\mu\right)}\underbrace{\frac{y}{\sigma_{c}^{2}}}_{t_{2}\left(y\right)}\right\} .
\end{flalign*}

\end_inset

This last expression has the form of an exponential family, with 
\begin_inset Formula $c\left(\mu\right)=1$
\end_inset

.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:min-sufficient-exp-family"

\end_inset

, we showed that a minimal sufficient statistic for an exponential family
 is given by 
\begin_inset Formula $T\left(Y\right)=\left(T_{1}\left(Y\right),\ldots,T_{n}\left(Y\right)\right)$
\end_inset

.
 It follows that 
\begin_inset Formula 
\begin{flalign*}
T\left(Y\right) & =\left(\frac{1}{\sigma_{c}^{2}},\frac{Y}{\sigma_{c}^{2}}\right)
\end{flalign*}

\end_inset

is a minimal sufficient statistic for 
\begin_inset Formula $\mu$
\end_inset

 conditioned on 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Expanded definition of ancillarity
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $\theta=\left(\psi,\lambda\right)$
\end_inset

, where 
\begin_inset Formula $\lambda$
\end_inset

 is not of direct interest (
\begin_inset Formula $\lambda$
\end_inset

 is a nuisance parameter).
 Suppose that 
\begin_inset Formula $S=\left(T,C\right)$
\end_inset

 is minimal sufficient for 
\begin_inset Formula $\theta$
\end_inset

, where the pdf of 
\begin_inset Formula $C$
\end_inset

 depends on 
\begin_inset Formula $\lambda$
\end_inset

 but not on 
\begin_inset Formula $\psi$
\end_inset

, and the conditional pdf of 
\begin_inset Formula $T$
\end_inset

 given 
\begin_inset Formula $C$
\end_inset

 depends on 
\begin_inset Formula $\psi$
\end_inset

 but not 
\begin_inset Formula $\lambda$
\end_inset

.
 Then 
\begin_inset Formula $C$
\end_inset

 is called ancillary in the extended sense.
\end_layout

\begin_layout Subsection
Complete statistics
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $f\left(t|\theta\right)$
\end_inset

 be a family of pdfs or pmfs for a statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 The family of probability distributions is called 
\shape italic
complete
\shape default
 if 
\begin_inset Formula $\mbox{E}_{\theta}\left[g\left(T\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

 implies 
\begin_inset Formula $P_{\theta}\left(\left\{ g\left(T\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

.
 Equivalently, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is called a 
\shape italic
complete statistic
\shape default
.
 I.e., 
\begin_inset Formula $T$
\end_inset

 is complete if 
\begin_inset Formula 
\[
\mbox{E}_{\theta}\left[g\left(T\right)\right]=0\quad\forall\theta\quad\Rightarrow\quad P_{\theta}\left(\left\{ g\left(T\right)=0\right\} \right)=1\quad\forall g.
\]

\end_inset


\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $T\left(X\right)$
\end_inset

 is sufficient and complete for 
\begin_inset Formula $\theta$
\end_inset

, then 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Complete statistics in the exponential family]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from an exponential family with pdf or pmf of the form
\begin_inset Formula 
\begin{flalign*}
f\left(x|\boldsymbol{\theta}\right) & =h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left(\sum_{j=1}^{k}\omega_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)\right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\boldsymbol{\theta}=\left(\theta_{1},\theta_{2},\ldots,\theta_{k}\right)$
\end_inset

.
 Then the statistic
\begin_inset Formula 
\[
T\left(\mathbf{X}\right)=\left(\sum_{i=1}^{n}t_{1}\left(X_{i}\right),\sum_{i=1}^{n}t_{2}\left(X_{i}\right),\ldots,\sum_{i=1}^{n}t_{k}\left(X_{i}\right)\right)
\]

\end_inset

is complete if 
\begin_inset Formula $\left\{ \left(\omega_{1}\left(\boldsymbol{\theta}\right),\ldots,\omega_{k}\left(\boldsymbol{\theta}\right)\right):\boldsymbol{\theta}\in\Theta\right\} $
\end_inset

 contains an open set in 
\begin_inset Formula $\mathbb{R}^{k}$
\end_inset

.
 (This is Theorem 6.2.25 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:complete-stat-exp-family"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
If a minimal sufficient statistic exists, then any complete sufficient statistic
 is also a minimal sufficient statistic.
 (This is Theorem 6.2.28 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Remark
Minimal sufficiency does not imply completeness.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:3-param-exp-family"

\end_inset

, we expressed the family of distributions with densities 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\underbrace{\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -x^{4}\right\} }_{h\left(x\right)}\underbrace{\exp\left\{ -\theta^{4}\right\} }_{c\left(\theta\right)}\exp\left\{ \underbrace{4x^{3}}_{t_{1}\left(x\right)}\underbrace{\theta}_{\omega_{1}\left(\theta\right)}\underbrace{-6x^{2}}_{t_{2}\left(x\right)}\underbrace{\theta^{2}}_{\omega_{2}\left(\theta\right)}+\underbrace{4x}_{t_{3}\left(x\right)}\underbrace{\theta^{3}}_{\omega_{3}\left(\theta\right)}\right\} ,
\end{flalign*}

\end_inset

which is in exponential form, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(4x^{3},-6x^{2},4x\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:min-sufficient-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 The parameter space for this distribution is given by 
\begin_inset Formula $\left\{ \omega_{1}\left(\theta\right),\omega_{2}\left(\theta\right),\omega_{3}\left(\theta\right)\right\} =\left\{ \theta,\theta^{2},\theta^{3}\right\} $
\end_inset

, i.e., 
\begin_inset Formula $k=3$
\end_inset

.
 We have 
\begin_inset Formula $d=1$
\end_inset

, so by definition this familiy of densities is a curved exponential family.
 Its graph in 
\begin_inset Formula $\mathbb{R}^{3}$
\end_inset

 is the curve shown in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
figref{curved-exp-family}
\end_layout

\end_inset

.
 This graph does not have positive length (volume in 
\begin_inset Formula $\mathbb{R}^{3}$
\end_inset

), i.e., 
\begin_inset Formula $\left\{ \theta,\theta^{2},\theta^{3}\right\} $
\end_inset

 does not contain an open set in 
\begin_inset Formula $\mathbb{R}^{3}$
\end_inset

, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is not complete.
\end_layout

\begin_layout Remark
\begin_inset ERT
status open

\begin_layout Plain Layout

<<curved-exp-family, echo=FALSE, fig.height=3, fig.align='center', fig.pos='!htb',
 fig.cap='graph of $x=
\backslash

\backslash
theta,y=
\backslash

\backslash
theta^{2},z=
\backslash

\backslash
theta^{3}$'>>=
\end_layout

\begin_layout Plain Layout

library(lattice)
\end_layout

\begin_layout Plain Layout

t <- seq(-2, 2, length.out=500)
\end_layout

\begin_layout Plain Layout

cloud(z~x+y, data.frame(x=t, y=t^2, z=t^3), pch=".", ylim=c(0,1.5), xlim=c(-1,1),
 zlim=c(-2,2))
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Remark
Although the condition in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that the set 
\begin_inset Formula $\left\{ \left(\omega_{1}\left(\boldsymbol{\theta}\right),\ldots,\omega_{k}\left(\boldsymbol{\theta}\right):\boldsymbol{\theta}\in\Theta\right)\right\} $
\end_inset

 contain an open set is sufficient to guarantee completeness, it is not
 necessary.
 That is, the failure of this condition does not show that a statistic is
 not complete, as shown in the following example.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X\sim\mbox{Bernoulli}\left(p\right)$
\end_inset

 and the parameter space consists of the two points 
\begin_inset Formula $p=1/4$
\end_inset

 and 
\begin_inset Formula $p=3/4$
\end_inset

.
 The pmf for 
\begin_inset Formula $X$
\end_inset

 can be written as
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =p^{x}\left(1-p\right)^{1-x}\\
 & =p^{x}\left(1-p\right)^{-x}\left(1-p\right)\\
 & =\left(1-p\right)\left(\frac{p}{1-p}\right)^{x}\\
 & =\left(1-p\right)\exp\left\{ \log\left(\left(\frac{p}{1-p}\right)^{x}\right)\right\} \\
 & =\underbrace{\left(1-p\right)}_{c\left(p\right)}\exp\left\{ \underbrace{x}_{t_{1}\left(x\right)}\underbrace{\log\left(\frac{p}{1-p}\right)}_{\omega_{1}\left(p\right)}\right\} 
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, i.e., 
\begin_inset Formula $X$
\end_inset

 has an exponential family distribution.
 Then, the parameter space is given by
\begin_inset Formula 
\begin{flalign*}
\left\{ \omega_{1}\left(p\right):p=\left\{ \frac{1}{4},\frac{3}{4}\right\} \right\}  & =\left\{ \log\left(\frac{1/4}{1-\left(1/4\right)}\right),\log\left(\frac{3/4}{1-\left(3-4\right)}\right)\right\} \\
 & =\left\{ \log\left(\frac{1/4}{3/4}\right),\log\left(\frac{3/4}{1/4}\right)\right\} \\
 & =\left\{ \log\frac{1}{3},\log3\right\} 
\end{flalign*}

\end_inset

The condition of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 is not satisfied because the parameter space has only two points in it,
 and hence the range of 
\begin_inset Formula $\log\left(p/\left(1-p\right)\right)$
\end_inset

 as 
\begin_inset Formula $p$
\end_inset

 varies over the parameter space does not contain an open set (an interval)
 in 
\begin_inset Formula $\mathbb{R}^{1}$
\end_inset

.
\end_layout

\begin_layout Example
Yet, the distribution of 
\begin_inset Formula $X$
\end_inset

 is complete.
 To see this, suppose 
\begin_inset Formula $g$
\end_inset

 is a function defined on the sample space such that 
\begin_inset Formula $\mbox{E}\left[g\left(X\right)\right]=0$
\end_inset

.
 To show that the distribution of 
\begin_inset Formula $X$
\end_inset

 is complete, we need to show that the only function satisfying this equality
 for all 
\begin_inset Formula $p$
\end_inset

 in the parameter space is the function which is identically zero.
 The sample space consists of the two points 
\begin_inset Formula $X=0$
\end_inset

 and 
\begin_inset Formula $X=1$
\end_inset

, so we need to show that this implies 
\begin_inset Formula $g\left(0\right)=g\left(1\right)=0$
\end_inset

.
\end_layout

\begin_layout Example
The expected value of 
\begin_inset Formula $g\left(X\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[g\left(X\right)\right] & =\sum_{x=0}^{1}g\left(x\right)\cdot p_{X}\left(x\right)\\
 & =\sum_{x=0}^{1}g\left(x\right)\cdot p^{x}\left(1-p\right)^{1-x}\\
 & =g\left(0\right)\cdot p^{0}\left(1-p\right)^{1-0}+g\left(1\right)\cdot p^{1}\left(1-p\right)^{1-1}\\
 & =\left(1-p\right)g\left(0\right)+pg\left(1\right)
\end{flalign*}

\end_inset

so that if 
\begin_inset Formula $p=1/4$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
0 & =\mbox{E}_{p=1/4}\left[g\left(X\right)\right]\\
 & =\frac{3}{4}g\left(0\right)+\frac{1}{4}g\left(1\right)
\end{flalign*}

\end_inset

and if 
\begin_inset Formula $p=3/4$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
0 & =\mbox{E}_{p=3/4}\left[g\left(X\right)\right]\\
 & =\frac{1}{4}g\left(0\right)+\frac{3}{4}g\left(1\right).
\end{flalign*}

\end_inset

So, we have 
\begin_inset Formula $g\left(1\right)=-3g\left(0\right)$
\end_inset

, which gives
\begin_inset Formula 
\begin{flalign*}
0 & =\frac{1}{4}g\left(0\right)+\frac{3}{4}g\left(1\right)\\
 & =\frac{1}{4}g\left(0\right)+\frac{3}{4}\left(-3g\left(0\right)\right)\\
 & =-2g\left(0\right)\\
\Leftrightarrow0 & =g\left(0\right)
\end{flalign*}

\end_inset

and thus 
\begin_inset Formula $g\left(1\right)=-3\left(0\right)=0$
\end_inset

.
 Thus, the only solution is 
\begin_inset Formula $g\left(1\right)=g\left(0\right)=0$
\end_inset

, and with just two points in the parameter space we have that the family
 of distributions is complete.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mbox{Bernoulli}\left(p\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =p^{x}\left(1-p\right)^{1-x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Show that 
\begin_inset Formula $T_{1}=X_{2}-X_{1}$
\end_inset

 is not a complete statistic.
\begin_inset CommandInset label
LatexCommand label
name "exa:complete-stat-bernoulli-part-a"

\end_inset


\end_layout

\begin_layout Example
By definition, 
\begin_inset Formula $T_{1}$
\end_inset

 will be a complete statistic if 
\begin_inset Formula $\mbox{E}\left[g\left(T_{1}\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

 implies 
\begin_inset Formula $P\left(\left\{ g\left(T_{1}\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 Suppose that 
\begin_inset Formula $g\left(T_{1}\right)=T_{1}$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[g\left(T_{1}\right)\right] & =\mbox{E}\left[T_{1}\right]\\
 & =\mbox{E}\left[X_{2}-X_{1}\right]\\
 & =\mbox{E}\left[X_{2}\right]-\mbox{E}\left[X_{1}\right]\\
 & =\sum_{x_{2}=0}^{1}\left[x_{2}\cdot p^{x_{2}}\left(1-p\right)^{1-x_{2}}\right]-\sum_{x_{1}=0}^{1}\left[x_{1}\cdot p^{x_{1}}\left(1-p\right)^{1-x_{1}}\right]\\
 & =\left[0\cdot p^{0}\left(1-p\right)^{1-0}+1\cdot p^{1}\left(1-p\right)^{1-1}\right]-\left[0\cdot p^{0}\left(1-p\right)^{1-0}+1\cdot p^{1}\left(1-p\right)^{1-1}\right]\\
 & =\left[0+p\right]-\left[0+p\right]\\
 & =0.
\end{flalign*}

\end_inset

So, this choice of 
\begin_inset Formula $g$
\end_inset

 satisfies 
\begin_inset Formula $\mbox{E}\left[g\left(T_{1}\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ g\left(T_{1}\right)=0\right\} \right) & =P\left(\left\{ T_{1}=0\right\} \right)\\
 & =P\left(\left\{ X_{2}-X_{1}=0\right\} \right).
\end{flalign*}

\end_inset

This probability will be equal to 1 if and only if 
\begin_inset Formula $X_{1}=X_{2}$
\end_inset

, but this will not always be the case, e.g., suppose that 
\begin_inset Formula $X_{1}=0$
\end_inset

 and 
\begin_inset Formula $X_{2}=1$
\end_inset

.
 Even though our choice of 
\begin_inset Formula $g$
\end_inset

satisfies 
\begin_inset Formula $\mbox{E}\left[g\left(T_{1}\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

, it does not imply 
\begin_inset Formula $P\left(\left\{ g\left(T_{1}\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 The condition of completeness applies to all functions (all choices of
 
\begin_inset Formula $g$
\end_inset

), so it follows that 
\begin_inset Formula $T_{1}$
\end_inset

 is not complete.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:complete-stat-bernoulli-part-a"

\end_inset

.
 Show that 
\begin_inset Formula $T_{2}=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a complete statistic.
 (This is example 6.2.22 from Casella & Berger.)
\end_layout

\begin_layout Example
\begin_inset Formula $T_{2}$
\end_inset

 is the sum of 
\begin_inset Formula $n$
\end_inset

 independent Bernoulli random variables each having the same success probability
 
\begin_inset Formula $p$
\end_inset

.
 It follows that 
\begin_inset Formula $T_{2}\sim\mbox{Binomial}\left(n,p\right)$
\end_inset

, so that the pmf of 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{T_{2}}\left(t\right) & =\binom{n}{t}p^{t}\left(1-p\right)^{n-t}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $t\in\left\{ 0,1,2,\ldots,n\right\} $
\end_inset

 and 
\begin_inset Formula $p_{T_{2}}\left(t\right)=0$
\end_inset

 otherwise.
 Suppose 
\begin_inset Formula $g\left(T_{2}\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\mbox{E}\left[g\left(T_{2}\right)\right]=0$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
0 & =\mbox{E}\left[g\left(T_{2}\right)\right]\\
 & =\sum_{k=0}^{n}\left[g\left(k\right)\cdot p_{T_{2}}\left(k\right)\right]\\
 & =\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}p^{k}\left(1-p\right)^{n-k}\right]\\
 & =\left(1-p\right)^{n}\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}p^{k}\left(1-p\right)^{-k}\right]\\
 & =\left(1-p\right)^{n}\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}\left(\frac{p}{1-p}\right)^{k}\right].
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

, so that 
\begin_inset Formula $\left(1-p\right)^{n}>0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 It follows that this expression will be equal to zero if and only if 
\begin_inset Formula 
\[
\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}\left(\frac{p}{1-p}\right)^{k}\right]=0.
\]

\end_inset

Let 
\begin_inset Formula $r=\left(p/\left(1-p\right)\right)$
\end_inset

.
 Then, this sum is a polynomial function of 
\begin_inset Formula $r$
\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}r^{k}\right] & =g\left(0\right)\cdot\binom{n}{0}r^{0}+\cdots+g\left(n\right)\cdot\binom{n}{n}r^{n}\\
 & =g\left(0\right)+g\left(1\right)\cdot nr^{1}+g\left(2\right)\binom{n}{2}r^{2}+\cdots+g\left(n-1\right)\cdot nr^{n-1}+g\left(n\right)r^{n}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $r=p/\left(1-p\right)>0$
\end_inset

, so that for the 
\begin_inset Formula $k\mbox{th}$
\end_inset

 term, we will have 
\begin_inset Formula $\binom{n}{k}>0$
\end_inset

 and 
\begin_inset Formula $r^{k}>0$
\end_inset

.
 It follows that this sum is equal to zero if and only if 
\begin_inset Formula $g\left(k\right)=0$
\end_inset

 for 
\begin_inset Formula $k=\left\{ 0,1,\ldots,n\right\} $
\end_inset

.
 
\begin_inset Formula $T_{2}$
\end_inset

 takes on the values 
\begin_inset Formula $0,1,\ldots,n$
\end_inset

 with probability 1 (recall that 
\begin_inset Formula $T_{2}$
\end_inset

 represents the probability of 
\begin_inset Formula $k$
\end_inset

 successes in 
\begin_inset Formula $n$
\end_inset

 trials), so that 
\begin_inset Formula $P\left(\left\{ g\left(T_{2}\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 It follows that 
\begin_inset Formula $T_{2}$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mbox{Poisson}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =\frac{e^{-\lambda}\lambda^{x}}{x!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Example
It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:dist-of-sum-of-poisson-rv"

\end_inset

 that 
\begin_inset Formula $X_{1}+X_{2}\sim\mbox{Poisson}\left(\lambda+\lambda\right)=\mbox{Poisson}\left(2\lambda\right)$
\end_inset

, that 
\begin_inset Formula $\left(X_{1}+X_{2}\right)+X_{3}\sim\mbox{Poisson}\left(2\lambda+\lambda\right)=\mbox{Poisson}\left(3\lambda\right)$
\end_inset

, and therefore that 
\begin_inset Formula 
\[
T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}\sim\mbox{Poisson}\left(n\lambda\right),
\]

\end_inset

so that the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{T\left(\mathbf{X}\right)}\left(t\right) & =\frac{e^{-n\lambda}\left(n\lambda\right)^{t}}{t!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $t\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{T\left(\mathbf{X}\right)}\left(t\right)=0$
\end_inset

 otherwise.
 Suppose that 
\begin_inset Formula $g\left(T\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\mbox{E}\left[g\left(T\right)\right]=0$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
0 & =\mbox{E}\left[g\left(T\right)\right]\\
 & =\sum_{t=0}^{\infty}\left[g\left(t\right)\cdot\frac{e^{-n\lambda}\left(n\lambda\right)^{t}}{t!}\right]\\
 & =e^{-n\lambda}\sum_{t=0}^{\infty}g\left(t\right)\cdot\frac{\left(n\lambda\right)^{t}}{t!}.
\end{flalign*}

\end_inset

We will have 
\begin_inset Formula $e^{-n\lambda}>0$
\end_inset

 for all 
\begin_inset Formula $n$
\end_inset

 and all 
\begin_inset Formula $\lambda\geq0$
\end_inset

.
 It follows that this expression will be equal to zero if and only if
\begin_inset Formula 
\[
\sum_{t=0}^{\infty}g\left(t\right)\cdot\frac{\left(n\lambda\right)^{t}}{t!}=0.
\]

\end_inset

This sum is a polynomial function of 
\begin_inset Formula $\left(n\lambda\right)$
\end_inset

.
 For the sum to be equal to zero, the coefficient 
\begin_inset Formula $g\left(t\right)/t!$
\end_inset

 must be equal to zero for all 
\begin_inset Formula $t$
\end_inset

.
 Because 
\begin_inset Formula $t!\geq1$
\end_inset

, it follows that 
\begin_inset Formula $g\left(t\right)=0$
\end_inset

 for all 
\begin_inset Formula $t$
\end_inset

.
 Thus, we have 
\begin_inset Formula 
\[
\mbox{E}\left[g\left(T\right)\right]=0\quad\forall\lambda\quad\Rightarrow\quad P\left(\left\{ g\left(T\right)=0\right\} \right)=1\quad\forall\lambda,
\]

\end_inset

so by definition 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim U\left(0,\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x\right) & =\frac{1}{\theta}I_{\left\{ 0<x<\theta\right\} }.
\end{flalign*}

\end_inset

Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}$
\end_inset

 is a complete statistic.
 (This is example 6.2.23 from Casella & Berger.)
\end_layout

\begin_layout Example
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-statistic-uniform"

\end_inset

, the cdf of 
\begin_inset Formula $X$
\end_inset

 is given by 
\begin_inset Formula $F_{X}\left(x\right)=x/\theta$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-continuous"

\end_inset

, the pdf of 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(n\right)}}\left(x\right) & =\frac{n!}{\left(n-1\right)!\left(n-n\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{n-1}\left[1-F_{X}\left(x\right)\right]^{n-n}\\
 & =\frac{n!}{\left(n-1\right)!0!}\left(\frac{1}{\theta}\right)\left(\frac{x}{\theta}\right)^{n-1}\left(1-\frac{x}{\theta}\right)^{0}I_{\left\{ 0<x<\theta\right\} }\\
 & =n\left(\frac{\left(n-1\right)!}{\left(n-1\right)!}\right)\left(\frac{1}{\theta}\right)\left(\frac{x^{n-1}}{\theta^{n-1}}\right)I_{\left\{ 0<x<\theta\right\} }\\
 & =\frac{nx^{n-1}}{\theta^{n}}I_{\left\{ 0<x<\theta\right\} }.
\end{flalign*}

\end_inset

Suppose 
\begin_inset Formula $g\left(T\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\mbox{E}\left[g\left(T\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
0 & =\mbox{E}\left[g\left(T\right)\right]\\
 & =\int_{0}^{\theta}g\left(t\right)\cdot f_{X_{\left(n\right)}}\left(t\right)\mbox{d}t\\
 & =\int_{0}^{\theta}g\left(t\right)\cdot nt^{n-1}\theta^{-n}\mbox{d}t.
\end{flalign*}

\end_inset

By assumption, 
\begin_inset Formula $\mbox{E}\left[g\left(T\right)\right]=0$
\end_inset

, i.e., it is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, so that its derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 is zero.
 Then, we have
\begin_inset Formula 
\begin{flalign}
0 & =\frac{\mbox{d}}{\mbox{d}\theta}\left[\int_{0}^{\theta}g\left(t\right)\cdot nt^{n-1}\theta^{-n}\mbox{d}t\right]\\
 & =\frac{\mbox{d}}{\mbox{d}\theta}\left[n\theta^{-n}\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\mbox{d}t\right]\\
 & =n\theta^{-n}\frac{\mbox{d}}{\mbox{d}\theta}\left[\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\mbox{d}t\right]+\left(\frac{\mbox{d}}{\mbox{d}\theta}n\theta^{-n}\right)\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\mbox{d}t\\
 & =n\theta^{-n}\left[g\left(\theta\right)\cdot\theta^{n-1}-g\left(0\right)\cdot0^{n-1}\right]+n\left(-n\theta^{-n-1}\right)\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\mbox{d}t\\
 & =n\theta^{-1}g\left(\theta\right)+\left[-n\theta^{-1}\int_{0}^{\theta}g\left(t\right)\cdot nt^{n-1}\theta^{-n}\mbox{d}t\right]\\
 & =n\theta^{-1}g\left(\theta\right)+\left(-n\theta^{-1}\cdot0\right)\\
 & =n\theta^{-1}g\left(\theta\right).
\end{flalign}

\end_inset

(2) follows from algebra.
 (3) follows from the product rule for differentiation.
 (4) the first term follows from the Fundamental Theorem of Calculus.
 We rearrange terms in (5) so that (6) follows from our assumption that
 
\begin_inset Formula $\mbox{E}\left[g\left(T\right)\right]=0$
\end_inset

.
 (7) follows from algebra.
\end_layout

\begin_layout Example
We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $\theta>0,$
\end_inset

 so that 
\begin_inset Formula $\theta^{-1}>0$
\end_inset

.
 It follows that 
\begin_inset Formula $n\theta^{-1}g\left(\theta\right)$
\end_inset

 is equal to zero if and only if 
\begin_inset Formula $g\left(\theta\right)=0$
\end_inset

, which is true for for all 
\begin_inset Formula $\theta>0$
\end_inset

.
 Noting that we will always have 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}>0$
\end_inset

, it follows that 
\begin_inset Formula $P\left(\left\{ g\left(T\right)=0\right\} \right)=1$
\end_inset

, and thus that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Remark
The Fundamental Theorem of Calculus applies only to functions that are Riemann-i
ntegrable.
 Thus, the equation 
\begin_inset Formula 
\begin{flalign*}
\frac{\mbox{d}}{\mbox{d}\theta}\int_{0}^{\theta}g\left(t\right)\mbox{d}t & =g\left(\theta\right)
\end{flalign*}

\end_inset

is valid only at points of continuity of Riemann-integrable 
\begin_inset Formula $g$
\end_inset

.
 The condition of completeness applies to all functions, not just Riemann-integr
able ones, so the argument above does not, strictly speaking, show that
 
\begin_inset Formula $T$
\end_inset

 is a complete statistic.
 From a more practical view, though, this distinction is not of concern
 since the condition of Riemann-integrability is so general that it includes
 virtually any function we could think of.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Basu's Theorem]
\end_layout

\end_inset

If 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete and minimal sufficient statistic, then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is independent of every ancillary statistic.
 (This is Theorem 6.2.24 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:basu"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
That is, if 
\begin_inset Formula $T$
\end_inset

 depends on some ancillary statistic and 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient, then 
\begin_inset Formula $T$
\end_inset

 cannot be complete.
 Basu's Theorem allows us to deduce the independence of two statistics without
 finding their joint distribution.
 To use Basu's Theorem, we need to show that a statistic is complete.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mbox{Exp}\left(\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\theta e^{-\theta x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\geq0$
\end_inset

, so that the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}f_{X_{i}}\left(x_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}\theta e^{-\theta x_{i}}\\
 & =\theta^{n}e^{\sum_{i=1}^{n}-\theta x_{i}}\\
 & =\theta^{n}e^{-\theta\sum_{i=1}^{n}x_{i}}.
\end{flalign*}

\end_inset

Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is independent of 
\begin_inset Formula $Y=X_{n}/\sum_{i=1}^{n}X_{i}$
\end_inset

.
\end_layout

\begin_layout Example
We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:minimal-sufficient-exp-rv"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 We can express 
\begin_inset Formula $f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)$
\end_inset

 in exponential family form as
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\underbrace{\theta^{n}}_{c\left(\theta\right)}\exp\left\{ \underbrace{-\theta}_{\omega_{1}\left(\theta\right)}\underbrace{\sum_{i=1}^{n}x_{i}}_{t_{1}\left(\mathbf{x}\right)}\right\} 
\end{flalign*}

\end_inset

with 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

.
 The parameter space for this distribution is given by 
\begin_inset Formula $\left\{ \omega_{1}\left(\theta\right)=-\theta:\theta>0\right\} $
\end_inset

, i.e., 
\begin_inset Formula $k=1$
\end_inset

.
 The representation of this parameter space in 
\begin_inset Formula $\mathbb{R}^{1}$
\end_inset

 is the interval 
\begin_inset Formula $\left(-\infty,0\right)$
\end_inset

, which has positive length, i.e., it is an open set, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is complete.
\end_layout

\begin_layout Example
The cdf of 
\begin_inset Formula $Y$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y\leq y\right\} \right) & =P\left(\left\{ \frac{X_{n}}{\sum_{i=1}^{n}X_{i}}\leq y\right\} \right).
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 be iid observations from 
\begin_inset Formula $F_{Y}\left(y\right)$
\end_inset

 (corresponding to 
\begin_inset Formula $\theta=1$
\end_inset

) with 
\begin_inset Formula $X_{i}=\theta Z_{i}$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y\leq y\right\} \right) & =P\left(\left\{ \frac{\theta Z_{n}}{\sum_{i=1}^{n}\theta Z_{i}}\leq y\right\} \right)\\
 & =P\left(\left\{ \frac{Z_{n}}{\sum_{i=1}^{n}Z_{i}}\leq y\right\} \right).
\end{flalign*}

\end_inset

This expression does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $Y$
\end_inset

 is an ancillary statistic.
 Because 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete and minimal sufficient statistic, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:basu"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim U\left(\theta,\theta+1\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid.
 Is 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(X_{\left(1\right)},X_{\left(n\right)}\right)$
\end_inset

 a complete statistic?
\end_layout

\begin_layout Example
Any one-to-one function of a minimal sufficient statistic is also minimal
 sufficient, so it follows that 
\begin_inset Formula $\left(X_{\left(1\right)},X_{\left(n\right)}-X_{\left(1\right)}\right)$
\end_inset

 is also minimal sufficient.
 We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:ancillary-stat-uniform-range"

\end_inset

 that the range statistic 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

 is an ancillary statistic for a uniform random variable.
 (More generally, we have 
\begin_inset Formula $X\sim U\left(\theta,\theta+1\right)$
\end_inset

.
 Suppose that 
\begin_inset Formula $Z=U\left(0,1\right)$
\end_inset

, so that 
\begin_inset Formula $X=Z+\theta$
\end_inset

.
 It follows that 
\begin_inset Formula $X\sim U\left(\theta,\theta+1\right)$
\end_inset

 is a location family, and range is ancillary for a location family.) Thus,
 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 depends on an ancillary statistic, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:basu"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is not complete.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-normal"

\end_inset

 that the 
\begin_inset Formula $N\left(\mu,\sigma^{2}\right)$
\end_inset

 family is an exponential family, with 
\begin_inset Formula $\omega_{1}\left(\boldsymbol{\theta}\right)=1/\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\omega_{2}\left(\boldsymbol{\theta}\right)=\mu/\sigma^{2}$
\end_inset

.
 The parameter space for this family is given by 
\begin_inset Formula $\Theta=\left\{ \left(\mu,\sigma^{2}\right):-\infty<\mu<\infty,\sigma>0\right\} $
\end_inset

, i.e., 
\begin_inset Formula $k=2$
\end_inset

.
 The representation of this parameter space in 
\begin_inset Formula $\mathbb{R}^{2}$
\end_inset

 is the set 
\begin_inset Formula 
\[
\left\{ \left(1/\sigma^{2},\mu/\sigma^{2}\right):\left(\mu,\sigma\right)\in\Theta\right\} =\left\{ \left(x,y\right):x>0,-\infty<y<\infty\right\} ,
\]

\end_inset

which clearly contains an open set, e.g., the rectangle 
\begin_inset Formula $\left\{ \left(x,y\right):1<x<2,1<y<2\right\} $
\end_inset

.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-normal"

\end_inset

, we found that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(\sum_{i=1}^{n}X_{i},\sum_{i=1}^{n}X_{i}^{2}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Chapter
Point estimation
\end_layout

\begin_layout Standard
Many inferential problems fall into one of three types: point estimation,
 confidence estimation, or hypothesis testing.
 Point estimation refers to providing a single 
\begin_inset Quotes eld
\end_inset

best guess
\begin_inset Quotes erd
\end_inset

 of some quantity of interest, e.g., a model parameter, a cdf 
\begin_inset Formula $F$
\end_inset

, a pdf 
\begin_inset Formula $f$
\end_inset

, a regression function, a prediction for a future value 
\begin_inset Formula $Y$
\end_inset

 of a random variable.
\end_layout

\begin_layout Definition
A 
\shape italic
point estimator
\shape default
 is any function 
\begin_inset Formula $W\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 of a sample; that is, any statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a point estimator.
 An 
\shape italic
estimate
\shape default
 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is the observed value of an estimator 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 (that is, a number) that is obtained when a sample is actually taken.
\end_layout

\begin_layout Standard
Often, we are interested in estimating some function 
\begin_inset Formula $T\left(\theta\right)$
\end_inset

, e.g., if 
\begin_inset Formula $X\sim N\left(\mu,\sigma^{2}\right)$
\end_inset

, then the parameter is 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
 If our goal is to estimate 
\begin_inset Formula $\mu$
\end_inset

, then 
\begin_inset Formula $\mu=T\left(\boldsymbol{\theta}\right)$
\end_inset

 is called the 
\shape italic
parameter of interest
\shape default
 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is called a 
\shape italic
nuisance parameter
\shape default
.
\end_layout

\begin_layout Section
Methods of finding estimators
\end_layout

\begin_layout Subsection
Method of moments estimators
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X\sim f\left(x|\theta\right)$
\end_inset

.
 The 
\begin_inset Formula $r\mbox{th}$
\end_inset

 moment of 
\begin_inset Formula $X$
\end_inset

 is denoted 
\begin_inset Formula $\mu_{r}=\mbox{E}\left[X^{r}\right]$
\end_inset

.
 Note that 
\begin_inset Formula $\mu_{r}$
\end_inset

 is a function of 
\begin_inset Formula $\theta$
\end_inset

, e.g., 
\begin_inset Formula $\mbox{E}\left[X\right]=\mu_{1}$
\end_inset

, 
\begin_inset Formula $\mbox{E}\left[X^{2}\right]=\mu_{2}$
\end_inset

, 
\begin_inset Formula $\mbox{Var}\left(X\right)=\mu_{2}-\mu_{1}^{2}$
\end_inset

.
 A method of moments estimator for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is obtained by solving
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{r} & =\sum_{i=1}^{n}\frac{X_{i}^{r}}{n}.
\end{flalign*}

\end_inset

Moment estimators are not invariant to transformation of the data (a function
 of 
\begin_inset Formula $x$
\end_inset

) or to reparameterization of 
\begin_inset Formula $\theta$
\end_inset

 (a function of 
\begin_inset Formula $\theta$
\end_inset

).
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mbox{Exp}\left(\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\theta e^{-\theta x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\geq0$
\end_inset

.
 Find a moment estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:mom-exp-rv"

\end_inset


\end_layout

\begin_layout Example
We will find 
\begin_inset Formula $\hat{\mu}_{1}=\mbox{E}\left[X\right]$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\mbox{E}\left[X\right]\\
 & =\int_{0}^{\infty}x\cdot\theta e^{-\theta x}\mbox{d}x\\
 & =\lim_{t\rightarrow\infty}\int_{0}^{t}x\cdot\theta e^{-\theta x}\mbox{d}x
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $u=x$
\end_inset

 and 
\begin_inset Formula $v'=\theta e^{-\theta x}$
\end_inset

, so that 
\begin_inset Formula $u'=1$
\end_inset

 and 
\begin_inset Formula $v=-e^{-\theta x}$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\int_{0}^{t}uv'\mbox{d}x & =\left[uv\right|_{0}^{t}-\int_{0}^{t}vu'\mbox{d}x\\
 & =-xe^{-\theta x}\Big\rvert_{0}^{t}-\int_{0}^{t}-e^{-\theta x}\mbox{d}x\\
 & =\left(-te^{-\theta t}-0\right)-\left(\frac{1}{\theta}e^{-\theta x}\right|_{0}^{t}\\
 & =-te^{-\theta t}-\left(\frac{1}{\theta}e^{-\theta t}-\frac{1}{\theta}e^{0}\right)\\
 & =-te^{-\theta t}-\frac{1}{\theta}\left(e^{-\theta t}-1\right)
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\lim_{t\rightarrow\infty}\left[-te^{-\theta t}-\frac{1}{\theta}\left(e^{-\theta t}-1\right)\right]\\
 & =0-\lim_{t\rightarrow\infty}\frac{1}{\theta}\left(e^{-\theta t}-1\right)\\
 & =-\frac{1}{\theta}\lim_{t\rightarrow\infty}\left(e^{-\theta t}-1\right)\\
 & =-\frac{1}{\theta}\left(0-1\right)\\
 & =\frac{1}{\theta}.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\frac{\sum_{i=1}^{n}X_{i}}{n}\\
\Leftrightarrow\frac{1}{\theta} & =\bar{x}\\
\Leftrightarrow\hat{\theta} & =\frac{1}{\bar{x}}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Remark
Had we used the parameterization 
\begin_inset Formula $f_{X}\left(x|\theta\right)=\left(1/\theta\right)e^{-x/\theta}$
\end_inset

, we would instead have found 
\begin_inset Formula $\hat{\theta}=\bar{x}$
\end_inset

.
\end_layout

\begin_layout Example
Suppose we have a population with 
\begin_inset Formula $\theta$
\end_inset

 members labeled 
\begin_inset Formula $1,\ldots,\theta$
\end_inset

 from which we sample 
\begin_inset Formula $n$
\end_inset

 observations with replacement and record their labels 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 Find a moment estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:mom-sampling-replacement"

\end_inset


\end_layout

\begin_layout Example
We are (randomly) sampling with replacement, so an equal-likelihood model
 is appropriate.
 Then, it follows that the pmf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{X_{i}}\left(x_{i}|\theta\right) & =P\left(\left\{ X_{i}=x_{i}\right\} \right)\\
 & =\frac{1}{\theta}I_{\left\{ x_{i}\in\left\{ 1,\ldots,\theta\right\} \right\} },
\end{flalign*}

\end_inset

so that the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}\left[\frac{1}{\theta}I_{\left\{ x_{i}\in\left\{ 1,\ldots,\theta\right\} \right\} }\right]\\
 & =\frac{1}{\theta^{n}}I_{\left\{ x_{\left(1\right)\geq1}\right\} }I_{\left\{ x_{\left(n\right)}\leq\theta\right\} }.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\mbox{E}\left[X\right]\\
 & =\sum_{x=1}^{\theta}x\cdot p_{X}\left(x|\theta\right)\\
 & =\sum_{x=1}^{\theta}x\cdot\frac{1}{\theta}\\
 & =\frac{1}{\theta}\sum_{x=1}^{\theta}x\\
 & =\frac{1}{\theta}\left[1+2+\cdots+\left(\theta-1\right)+\theta\right]\\
 & =\frac{1}{\theta}\left(\frac{\theta\left(\theta+1\right)}{2}\right)\\
 & =\frac{\theta+1}{2}.
\end{flalign*}

\end_inset

We must solve 
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\frac{\sum_{i=1}^{n}X_{i}}{n}\\
\Leftrightarrow\frac{\hat{\theta}+1}{2} & =\bar{x}\\
\Leftrightarrow\hat{\theta}+1 & =2\bar{x}\\
\Leftrightarrow\hat{\theta} & =2\bar{x}-1.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Remark
The technique used above to find the sum of the first 
\begin_inset Formula $n$
\end_inset

 numbers involves taking the sums of the respective extremes, e.g., 
\begin_inset Formula $1+n$
\end_inset

, 
\begin_inset Formula $2+\left(n-1\right)=1+n$
\end_inset

, 
\begin_inset Formula $3+\left(n-2\right)=1+n$
\end_inset

, and so on.
 There are 
\begin_inset Formula $n/2$
\end_inset

 such pairs, so it follows that the sum of the first 
\begin_inset Formula $n$
\end_inset

 numbers is given by
\begin_inset Formula 
\begin{flalign*}
\sum_{x=1}^{n}x & =\frac{n\left(n+1\right)}{2}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Example
Suppose
\begin_inset Formula 
\[
X_{1},\ldots,X_{n}\sim F\left(x|\alpha,\beta\right)=\begin{cases}
0, & x<0\\
\left(x/\beta\right)^{\alpha}, & 0\leq x\leq\beta\\
1 & x>\beta
\end{cases}.
\]

\end_inset

Find moment estimators for 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
 
\end_layout

\begin_layout Example
We must find estimators for two parameters, so we will need two equations,
 so we will take the first and second moments.
 The pdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x\right) & =\frac{\mbox{d}}{\mbox{d}x}F_{X}\\
 & =\frac{\mbox{d}}{\mbox{d}x}\left(\frac{x}{\beta}\right)^{\alpha}\\
 & =\alpha\left(\frac{x}{\beta}\right)^{\alpha-1}\left(\frac{1}{\beta}\right)\\
 & =\frac{\alpha}{\beta}\left(\frac{x^{\alpha-1}}{\beta^{\alpha-1}}\right)\\
 & =\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left[0,\beta\right]$
\end_inset

 and 
\begin_inset Formula $f_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Then, the first moment of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[X\right] & =\int_{0}^{\beta}x\cdot f_{X}\left(x\right)\mbox{d}x\\
 & =\int_{0}^{\beta}x\cdot\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}\mbox{d}x\\
 & =\frac{\alpha}{\beta^{\alpha}}\int_{0}^{\beta}x^{\alpha}\mbox{d}x\\
 & =\frac{\alpha}{\beta^{\alpha}}\left[\frac{1}{\alpha+1}x^{\alpha+1}\right|_{0}^{\beta}\\
 & =\frac{\alpha}{\beta^{\alpha}}\left(\frac{\beta^{\alpha+1}}{\alpha+1}-\frac{0}{\alpha+1}\right)\\
 & =\frac{\alpha\beta}{\alpha+1}
\end{flalign*}

\end_inset

and the second moment is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[X^{2}\right] & =\int_{0}^{\beta}x^{2}\cdot\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}\mbox{d}x\\
 & =\frac{\alpha}{\beta^{\alpha}}\int_{0}^{\beta}x^{\alpha+1}\mbox{d}x\\
 & =\frac{\alpha}{\beta^{\alpha}}\left[\frac{1}{\alpha+2}x^{\alpha+2}\right|_{0}^{\beta}\\
 & =\frac{\alpha}{\beta^{\alpha}}\left(\frac{\beta^{\alpha+2}}{\alpha+2}-\frac{0}{\alpha+2}\right)\\
 & =\frac{\alpha\beta^{2}}{\alpha+2}.
\end{flalign*}

\end_inset

We will solve the first equation for 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 
\begin_inset Formula 
\begin{flalign*}
\frac{\hat{\alpha}\hat{\beta}}{\hat{\alpha}+1} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
 & =\bar{x}\\
\Leftrightarrow\hat{\alpha}\hat{\beta} & =\bar{x}\left(\hat{a}+1\right)\\
\Leftrightarrow\hat{\beta} & =\frac{\bar{x}\left(\hat{\alpha}+1\right)}{\hat{\alpha}}
\end{flalign*}

\end_inset

We will solve the section equation for 
\begin_inset Formula $\hat{\alpha}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\hat{\alpha}\hat{\beta}^{2}}{\hat{\alpha}+2} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\\
\Leftrightarrow\hat{\alpha}\hat{\beta}^{2} & =\left(\hat{\alpha}+2\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)\\
\Leftrightarrow\hat{\alpha}\left(\frac{\bar{x}\left(\hat{\alpha}+1\right)}{\hat{\alpha}}\right)^{2} & =\left(\hat{\alpha}+2\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)\\
\Leftrightarrow\frac{\bar{x}^{2}\left(\hat{\alpha}+1\right)^{2}}{\hat{\alpha}} & =\left(\hat{\alpha}+2\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)\\
\Leftrightarrow\frac{\left(\hat{\alpha}+1\right)^{2}}{\hat{\alpha}\left(\hat{\alpha}+2\right)} & =\frac{1}{n\bar{x}^{2}}\sum_{i=1}^{n}x_{i}^{2}
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $c=\left(1/n\bar{x}^{2}\right)\sum_{i=1}^{n}x_{i}^{2}$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
\frac{\left(\hat{\alpha}+1\right)^{2}}{\hat{\alpha}\left(\hat{\alpha}+2\right)} & =c\\
\Leftrightarrow\hat{\alpha}^{2}+2\hat{\alpha}+1 & =c\hat{\alpha}\left(\hat{\alpha}+2\right)\\
\Leftrightarrow\hat{\alpha}^{2}+2\hat{\alpha}+1 & =c\hat{\alpha}^{2}+2c\hat{\alpha}\\
\Leftrightarrow\hat{\alpha}^{2}\left(1-c\right)+\hat{\alpha}\left(2-2c\right)+1 & =0.
\end{flalign*}

\end_inset

Then, the quadratic formula gives
\begin_inset Formula 
\begin{flalign*}
x & =\frac{-b\pm\sqrt{b^{2}-4ac}}{2a}\\
\hat{\alpha} & =\frac{-\left(2-2c\right)\pm\sqrt{\left(2-2c\right)^{2}-4\left(1-c\right)\cdot1}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm\sqrt{\left(2\left(1-c\right)\right)^{2}-4\left(1-c\right)}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm\sqrt{4\left(1-c\right)^{2}-4\left(1-c\right)}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm\sqrt{4\left(\left(1-c\right)^{2}-\left(1-c\right)\right)}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm2\sqrt{1-2c+c^{2}-1+c}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm2\sqrt{c^{2}-c}}{2\left(1-c\right)}\\
 & =\frac{c-1\pm\sqrt{c^{2}-c}}{1-c},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\hat{\beta} & =\frac{\bar{x}\left(\hat{\alpha}+1\right)}{\hat{\alpha}}\\
 & =\frac{\bar{x}\hat{\alpha}+\bar{x}}{\hat{\alpha}}\\
 & =\bar{x}+\frac{\bar{x}}{\frac{c-1\pm\sqrt{c^{2}-c}}{1-c}}\\
 & =\bar{x}\left(1+\frac{1-c}{c-1\pm\sqrt{c^{2}-c}}\right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Standard
Method of moments estimators may not be uniquely defined, as can be seen
 in the following example.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mbox{Poisson}\left(\lambda\right)$
\end_inset

.
 Recall that 
\begin_inset Formula $\mbox{E}\left[X\right]=\mbox{Var}\left(X\right)=\lambda$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[X\right] & =\hat{\mu}_{1}\\
 & =\frac{1}{n}\sum_{i=1}^{n}X_{i}\\
\Leftrightarrow\hat{\lambda} & =\bar{x}
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(X\right) & =\mbox{E}\left[X^{2}\right]-\mbox{E}\left[X\right]^{2}\\
\Leftrightarrow\hat{\lambda} & =\hat{\mu}_{2}-\hat{\mu}_{1}^{2}\\
 & =\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}\right)-\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\bar{X}^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\frac{n\bar{X}^{2}}{n}\\
 & =\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}^{2}-n\bar{X}^{2}\right)\\
 & =\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2},
\end{flalign*}

\end_inset

where the final equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:computing-sums-rand-samples"

\end_inset

.
 
\end_layout

\begin_layout Standard
Standard method of moments may not work.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim f\left(x|\theta\right)=\theta x^{-2}$
\end_inset

, 
\begin_inset Formula $0<\theta\leq x<\infty$
\end_inset

.
 Then,
\begin_inset Formula 
\begin{flalign*}
\mbox{E}_{\theta}\left[X\right] & =\int_{\theta}^{\infty}x\frac{\theta}{x^{2}}\mbox{d}x\\
 & =\theta\int_{\theta}^{\infty}\frac{1}{x}\mbox{d}x\\
 & =\theta\lim_{c\rightarrow\infty}\int_{\theta}^{c}\frac{1}{x}\mbox{d}x\\
 & =\theta\lim_{c\rightarrow\infty}\left(\ln x\Big\rvert_{\theta}^{c}\right)\\
 & =\theta\lim_{c\rightarrow\infty}\left(\ln c-\ln\theta\right)\\
 & =\infty,
\end{flalign*}

\end_inset

so that 
\begin_inset Formula $\mu_{1}\left(\theta\right)=\hat{\mu}_{1}$
\end_inset

 has no solution.
 If we consider
\begin_inset Formula 
\begin{flalign*}
\mbox{E}_{\theta}\left[\frac{1}{X}\right] & =\int_{\theta}^{\infty}\frac{\theta}{x^{3}}\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\int_{\theta}^{c}\frac{\theta}{x^{3}}\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\left(-\theta\frac{1}{2x^{2}}\Bigr\rvert_{\theta}^{c}\right)\\
 & =\lim_{c\rightarrow\infty}\left(-\theta\frac{1}{2c^{2}}-\left(-\theta\frac{1}{2\theta^{2}}\right)\right)\\
 & =0+\frac{1}{2\theta}\\
 & =\frac{1}{2\theta},
\end{flalign*}

\end_inset

then setting 
\begin_inset Formula $u_{-1}\left(\theta\right)=\hat{\mu}_{-1}=\left(1/n\right)\sum_{i=1}^{n}1/X_{i}$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{1}{2\hat{\theta}}=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{X_{i}}\quad\Rightarrow\quad\hat{\theta}=\frac{n}{2\sum_{i=1}^{n}\frac{1}{X_{i}}}.
\]

\end_inset

If we consider instead
\begin_inset Formula 
\begin{flalign*}
\mbox{E}_{\theta}\left[X^{1/2}\right] & =\int_{\theta}^{\infty}x^{1/2}\frac{\theta}{x^{2}}\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\left[\theta\int_{\theta}^{c}x^{-3/2}\mbox{d}x\right]\\
 & =\lim_{c\rightarrow\infty}\left[\theta\left(-2x^{-1/2}\Bigr\rvert_{\theta}^{c}\right)\right]\\
 & =\lim_{c\rightarrow\infty}\left[\theta\left(-2c^{-1/2}-\left(-2\theta^{-1/2}\right)\right)\right]\\
 & =\theta\left(0+2\theta^{-1/2}\right)\\
 & =2\sqrt{\theta},
\end{flalign*}

\end_inset

then setting 
\begin_inset Formula $\mu_{1/2}\left(\theta\right)=\hat{\mu}_{1/2}$
\end_inset

 gives
\begin_inset Formula 
\[
2\sqrt{\theta}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{1/2}\quad\Rightarrow\quad\hat{\theta}=\frac{\left(\sum_{i=1}^{n}X_{i}^{1/2}\right)^{2}}{4n^{2}}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Maximum likelihood estimators
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are independent random variables from 
\begin_inset Formula $f_{i}\left(x|\theta_{1},\ldots,\theta_{k}\right)$
\end_inset

, then the likelihood function is given by
\begin_inset Formula 
\[
L\left(\theta|\mathbf{x}\right)=L\left(\theta_{1},\ldots,\theta_{k}|x_{1},\ldots,x_{n}\right)=\prod_{i=1}^{n}f_{i}\left(x_{i}|\theta_{1},\ldots,\theta_{k}\right)=f\left(\mathbf{x}|\theta\right),
\]

\end_inset

i.e., the likelihood function is just the joint density of the data, except
 that we treat it as a function of the parameter 
\begin_inset Formula $\theta$
\end_inset

.
 The likelihood function is not a density function, i.e., it does not integrate
 to 1.
 If 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a discrete random vector, then 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)=P_{\theta}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)$
\end_inset

.
 If we compare the likelihood function at two parameter points and find
 that
\begin_inset Formula 
\[
P_{\theta_{1}}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)=L\left(\theta_{1}|\mathbf{x}\right)>L\left(\theta_{2}|\mathbf{x}\right)=P_{\theta_{2}}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right),
\]

\end_inset

then the sample we actually observed is more likely to have occurred if
 
\begin_inset Formula $\theta=\theta_{1}$
\end_inset

 than if 
\begin_inset Formula $\theta=\theta_{2}$
\end_inset

, which can be interpreted as saying that 
\begin_inset Formula $\theta_{1}$
\end_inset

 is a more plausible value for the true value of 
\begin_inset Formula $\theta$
\end_inset

 than is 
\begin_inset Formula $\theta_{2}$
\end_inset

.
\end_layout

\begin_layout Definition
For each sample point 
\begin_inset Formula $\mathbf{x}$
\end_inset

, let 
\begin_inset Formula $\hat{\theta}\left(\mathbf{x}\right)$
\end_inset

 be a parameter value at which 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 attains its maximum as a function of 
\begin_inset Formula $\theta$
\end_inset

, with 
\begin_inset Formula $\mathbf{x}$
\end_inset

 held fixed.
 A 
\shape italic
maximum likelihood estimator
\shape default
 (MLE) of the parameter 
\begin_inset Formula $\theta$
\end_inset

 based on a sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is 
\begin_inset Formula $\hat{\theta}\left(\mathbf{X}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The MLE is the parameter point for which the observed sample is most likely.
 Let 
\begin_inset Formula $L\left(\hat{\theta}|X\right)$
\end_inset

 be the maximum of all likelihood functions evaluated at 
\begin_inset Formula $\theta$
\end_inset

, i.e.,
\begin_inset Formula 
\[
L\left(\hat{\theta}|X\right)=\max\left\{ L\left(\theta|X\right):\theta\in\Theta\right\} \Leftrightarrow\hat{\theta}\mbox{ is an MLE}.
\]

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<likelihood-generic, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h', fig.cap='maximizing the likelihood function'>>=
\end_layout

\begin_layout Plain Layout

x <- seq(0, 2, length = 1000)
\end_layout

\begin_layout Plain Layout

par(mgp = c(0.2,1,0), mar = c(1,2,0.1,2))
\end_layout

\begin_layout Plain Layout

plot(x, (-x^2)+2*x, type = "l", ylab = expression(paste("L(", lambda, ")")),
 xlab = expression(lambda), xaxt = "n", yaxt = "n", cex.lab = 0.75)
\end_layout

\begin_layout Plain Layout

points(x = 1, y = 1, pch = 19)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
If the likelihood is differentiable in 
\begin_inset Formula $\theta$
\end_inset

, possible candidates for 
\begin_inset Formula $\hat{\theta}$
\end_inset

 are those that solve
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\log L\left(\theta|X\right) & =0,
\end{flalign*}

\end_inset

which gives solutions that find interior extrema, some of which may be minima.
 To check whether a solution is a maximum, verify that 
\begin_inset Formula 
\begin{flalign*}
\dfrac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|X\right) & <0,
\end{flalign*}

\end_inset

i.e., concave.
 It is often easier to work with 
\begin_inset Formula $\log L\left(\theta|X\right)$
\end_inset

 than with 
\begin_inset Formula $L\left(\theta|X\right)$
\end_inset

, and whatever maximizes the likelihood will also maximize log-likelihood.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mbox{Exp}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid.
 Find the MLE of 
\begin_inset Formula $\lambda$
\end_inset

 and compare it to the MOM estimator.
\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\lambda\right) & =\frac{1}{\lambda}e^{-x/\lambda}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\geq0$
\end_inset

, so that the pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right) & =\prod_{i=1}^{n}f_{X}\left(x_{i}|\lambda\right)\\
 & =\prod_{i=1}^{n}\frac{1}{\lambda}e^{-x_{i}/\lambda}\\
 & =\frac{1}{\lambda^{n}}e^{-\sum_{i=1}^{n}x_{i}/\lambda}.
\end{flalign*}

\end_inset

Then, we have 
\begin_inset Formula $L\left(\lambda|\mathbf{x}\right)=f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right)$
\end_inset

, so that 
\begin_inset Formula 
\begin{flalign*}
\ln L\left(\lambda|\mathbf{x}\right) & =\ln\left(\frac{1}{\lambda^{n}}e^{-\sum_{i=1}^{n}x_{i}/\lambda}\right)\\
 & =\ln\left(\frac{1}{\lambda^{n}}\right)+\ln\left(e^{-\sum_{i=1}^{n}x_{i}/\lambda}\right)\\
 & =-n\ln\lambda-\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

We will take the derivative of 
\begin_inset Formula $\ln L\left(\lambda|\mathbf{x}\right)$
\end_inset

 with respect to 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\lambda}\ln L\left(\lambda|\mathbf{x}\right) & =\frac{\partial}{\partial\lambda}\left[-n\ln\lambda-\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}\right]\\
 & =-\frac{n}{\lambda}+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{n}{\lambda} & =\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow n\lambda^{2} & =\lambda\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\hat{\lambda} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
 & =\bar{x}.
\end{flalign*}

\end_inset

We will evaluate the second derivative of 
\begin_inset Formula $\ln L\left(\lambda|\mathbf{x}\right)$
\end_inset

 at 
\begin_inset Formula $\lambda=\hat{\lambda}$
\end_inset

 to verify that this is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\lambda^{2}}\ln L\left(\lambda|\mathbf{x}\right)\Bigr\rvert_{\lambda=\hat{\lambda}} & =\frac{\partial^{2}}{\partial\lambda^{2}}\left[-\frac{n}{\lambda}+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}\right|_{\lambda=\hat{\lambda}}\\
 & =\frac{n}{\lambda^{2}}-\frac{2}{\lambda^{3}}\sum_{i=1}^{n}x_{i}\Bigr\rvert_{\lambda=\hat{\lambda}}\\
 & =\frac{n}{\bar{x}^{2}}-\frac{2}{\bar{x}^{3}}\cdot n\bar{x}\\
 & =\frac{n}{\bar{x}^{2}}-\frac{2n}{\bar{x}^{2}}\\
 & =-\frac{n}{\bar{x}^{2}}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $\bar{x}^{2}>0$
\end_inset

, so it follows that 
\begin_inset Formula $-n/\bar{x}^{2}<0$
\end_inset

, therefore 
\begin_inset Formula $\hat{\lambda}=\bar{x}$
\end_inset

 is the MLE.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:mom-exp-rv"

\end_inset

, we found that 
\begin_inset Formula $\hat{\lambda}_{MOM}=\bar{x}$
\end_inset

, so the two estimators agree.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let's reconsider 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:mom-sampling-replacement"

\end_inset

, where we found that the pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 was given by
\begin_inset Formula 
\[
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\frac{1}{\theta^{n}}I_{\left\{ x_{\left(1\right)\geq1}\right\} }I_{\left\{ x_{\left(n\right)}\leq\theta\right\} }.
\]

\end_inset

Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
We have
\begin_inset Formula 
\[
L\left(\theta|\mathbf{x}\right)=\begin{cases}
\frac{1}{\theta^{n}}I_{\left\{ x_{\left(1\right)\geq1}\right\} }, & \theta\geq x_{\left(n\right)}\\
0, & \theta<x_{\left(n\right)}
\end{cases},
\]

\end_inset

whose graph is shown below.
 Clearly, the maximum value of 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 occurs at 
\begin_inset Formula $x_{\left(n\right)}$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}=X_{\left(n\right)}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<likelihood-sampling, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(2, 4, length=2000) 
\end_layout

\begin_layout Plain Layout

plot(z, 1/(z^10), type="l", xlab = expression(theta), ylab=expression(paste(L,
 "(",theta,"|",bold(x),")")), yaxt="n", xaxt="n", xlim = c(1, 4), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at=c(1,2), labels=c(expression(x[(1)]), expression(x[(n)])), cex.axis
 = 0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Remark*
When the support of the likelihood function depends on the parameter, it
 may not be necessary to take the derivative of 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

, e.g., the MLE may be found graphically.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are sampled from a multinomial distribution with 3 categories and probabilities
, e.g., the genotypes 
\begin_inset Formula $AA$
\end_inset

, 
\begin_inset Formula $Aa$
\end_inset

, and 
\begin_inset Formula $aa$
\end_inset

, so that 
\begin_inset Formula $X_{i}$
\end_inset

 represents the category of the 
\begin_inset Formula $i\mbox{th}$
\end_inset

 observation.
 Suppose that the pmf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X_{i}=1|\theta\right\} \right) & =\theta^{2}\\
P\left(\left\{ X_{i}=2|\theta\right\} \right) & =2\theta\left(1-\theta\right)\\
P\left(\left\{ X_{i}=3|\theta\right\} \right) & =\left(1-\theta\right)^{2}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\theta\in\left(0,1\right)$
\end_inset

.
 We observe 
\begin_inset Formula $n_{k}=\sum_{i=1}^{n}I_{\left\{ X_{i}=k\right\} }$
\end_inset

 individuals of type 
\begin_inset Formula $k\in\left\{ 1,2,3\right\} $
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
Note that the pmf specified for 
\begin_inset Formula $X_{i}$
\end_inset

 is legitimate, i.e., is it non-negative and
\begin_inset Formula 
\[
\theta^{2}+2\theta\left(1-\theta\right)+\left(1-\theta\right)^{2}=\theta^{2}+2\theta-2\theta^{2}+1-2\theta+\theta^{2}=1.
\]

\end_inset

 We can write the pmf as
\begin_inset Formula 
\begin{flalign*}
p_{X_{i}}\left(x_{i}|\theta\right) & =\left(\theta^{2}\right)^{I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{I_{\left\{ x_{i}=3\right\} }}
\end{flalign*}

\end_inset

where each 
\begin_inset Formula $I_{\left\{ x_{i}=k\right\} }$
\end_inset

 is equal to 0 or 1.
 Then, the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{i},\ldots,X_{n}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}p_{X_{i}}\left(x_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}\left(\theta^{2}\right)^{I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{I_{\left\{ x_{i}=3\right\} }}\\
 & =\left(\theta^{2}\right)^{\sum_{i=1}^{n}I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=3\right\} }}
\end{flalign*}

\end_inset

so that the likelihood function is given by 
\begin_inset Formula 
\begin{flalign*}
L\left(\theta|\mathbf{x}\right) & =p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)\\
 & =\left(\theta^{2}\right)^{\sum_{i=1}^{n}I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=3\right\} }}\\
 & =\left(\theta^{2}\right)^{n_{1}}\left[2\theta\left(1-\theta\right)\right]^{n_{2}}\left[\left(1-\theta\right)^{2}\right]^{n_{3}}\\
 & =\theta^{2n_{1}}\left[2\theta\left(1-\theta\right)\right]^{n_{2}}\left(1-\theta\right)^{2n_{3}}\\
 & =2^{n_{2}}\theta^{2n_{1}+n_{2}}\left(1-\theta\right)^{n_{2}+2n_{3}}\\
\Leftrightarrow\ln L\left(\theta|\mathbf{x}\right) & =\ln\left[2^{n_{2}}\theta^{2n_{1}+n_{2}}\left(1-\theta\right)^{n_{2}+2n_{3}}\right]\\
 & =\ln2^{n_{2}}+\ln\theta^{2n_{1}+n_{2}}+\ln\left(1-\theta\right)^{n_{2}+2n_{3}}\\
 & =n_{2}\ln2+\left(2n_{1}+n_{2}\right)\ln\theta+\left(n_{2}+2n_{3}\right)\ln\left(1-\theta\right).
\end{flalign*}

\end_inset

We will take the derivative of the log-likelihood with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\ln L\left(\theta|\mathbf{x}\right) & =\frac{\partial}{\partial\theta}\left[n_{2}\ln2+\left(2n_{1}+n_{2}\right)\ln\theta+\left(n_{2}+2n_{3}\right)\ln\left(1-\theta\right)\right]\\
 & =0+\frac{2n_{1}+n_{2}}{\theta}+\frac{n_{2}+2n_{3}}{1-\theta}\cdot-1\\
 & =\frac{2n_{1}+n_{2}}{\theta}-\frac{n_{2}+2n_{3}}{1-\theta}
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{2n_{1}+n_{2}}{\theta}-\frac{n_{2}+2n_{3}}{1-\theta} & =0\\
\Leftrightarrow\frac{n_{2}+2n_{3}}{1-\theta} & =\frac{2n_{1}+n_{2}}{\theta}\\
\Leftrightarrow\theta\left(n_{2}+2n_{3}\right) & =\left(1-\theta\right)\left(2n_{1}+n_{2}\right)\\
\Leftrightarrow\frac{1-\theta}{\theta} & =\frac{n_{2}+2n_{3}}{2n_{1}+n_{2}}\\
\Leftrightarrow\frac{1}{\theta}-1 & =\frac{n_{2}+2n_{3}}{2n_{1}+n_{2}}\\
\Leftrightarrow\frac{1}{\theta} & =\frac{n_{2}+2n_{3}}{2n_{1}+n_{2}}+1\\
 & =\frac{n_{2}+2n_{3}+2n_{1}+n_{2}}{2n_{1}+n_{2}}\\
 & =\frac{2\left(n_{1}+n_{2}+n_{3}\right)}{2n_{1}+n_{2}}\\
 & =\frac{2n}{2n_{1}+n_{2}}\\
\Leftrightarrow\hat{\theta} & =\frac{2n_{1}+n_{2}}{2n}.
\end{flalign*}

\end_inset

We will evaluate the second derivative of the log-likelihood at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\theta^{2}}\ln L\left(\theta|\mathbf{x}\right)\Bigr\rvert_{\theta=\hat{\theta}} & =\frac{\partial}{\partial\theta}\left[\frac{2n_{1}+n_{2}}{\theta}-\frac{n_{2}-2n_{3}}{1-\theta}\right|_{\theta=\hat{\theta}}\\
 & =-\frac{2n_{1}+n_{2}}{\theta^{2}}-\left(\frac{n_{2}-2n_{3}}{\left(1-\theta\right)^{2}}\cdot-1\cdot-1\right)\\
 & =-\frac{2n_{1}+n_{2}}{\theta^{2}}-\frac{n_{2}-2n_{3}}{\left(1-\theta\right)^{2}}\\
 & =-\left[\frac{2n_{1}+n_{2}}{\theta^{2}}+\frac{n_{2}-2n_{3}}{\left(1-\theta\right)^{2}}\right]
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n_{k}\geq0$
\end_inset

 and 
\begin_inset Formula $n>0$
\end_inset

, i.e., at least one of 
\begin_inset Formula $n_{k}$
\end_inset

 is positive, and we have 
\begin_inset Formula $\theta^{2}>0$
\end_inset

 and 
\begin_inset Formula $\left(1-\theta\right)^{2}>0$
\end_inset

, so that the sum above is positive, so that the expression above is negative.
 It follows that 
\begin_inset Formula $\hat{\theta}=\left(2n_{1}+n_{2}\right)/2n$
\end_inset

 is the MLE.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\mbox{iid}}{\sim}\mbox{Poisson}\left(\lambda\right)$
\end_inset

, where 
\begin_inset Formula $\lambda>0$
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-poisson"

\end_inset

, we found that the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 was given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\lambda\right) & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}},
\end{flalign*}

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\ln L\left(\lambda|\mathbf{x}\right) & =\ln\left[\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)e^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\right]\\
 & =\ln\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)+\ln e^{-n\lambda}+\ln\lambda^{\sum_{i=1}^{n}x_{i}}\\
 & =\left(\ln\frac{1}{x_{1}!}+\cdots+\ln\frac{1}{x_{n}!}\right)-n\lambda+\left(\sum_{i=1}^{n}x_{i}\right)\ln\lambda\\
 & =\left[\left(\ln1-\ln x_{1}!\right)+\cdots+\left(\ln1-\ln x_{n}!\right)\right]-n\lambda+\left(\sum_{i=1}^{n}x_{i}\right)\ln\lambda\\
 & =\left[\left(0-\ln x_{1}!\right)+\cdots+\left(0-\ln x_{n}!\right)\right]-n\lambda+\ln\lambda\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =\sum_{i=1}^{n}-\ln x_{i}!-n\lambda+\ln\lambda\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =-n\lambda+\ln\lambda\left(\sum_{i=1}^{n}x_{i}\right)-\sum_{i=1}^{n}\ln x_{i}!.
\end{flalign*}

\end_inset

We will take the derivative of the log-likelihood with respect to 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\lambda}\ln L\left(\lambda|\mathbf{x}\right) & =\frac{\partial}{\partial\lambda}\left[-n\lambda+\ln\lambda\left(\sum_{i=1}^{n}x_{i}\right)-\sum_{i=1}^{n}\ln x_{i}!\right]\\
 & =-n+\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
-n+\frac{1}{\lambda}\sum_{i=1}^{n}x_{i} & =0\\
\Leftrightarrow\frac{1}{\lambda}\sum_{i=1}^{n}x_{i} & =n\\
\Leftrightarrow\hat{\lambda} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
 & =\bar{x}.
\end{flalign*}

\end_inset

We will evaluate the second derivative of the log-likelihood at 
\begin_inset Formula $\lambda=\hat{\lambda}$
\end_inset

 to verify that 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\lambda^{2}}\ln L\left(\lambda|\mathbf{x}\right)\Bigr\rvert_{\lambda=\hat{\lambda}} & =\frac{\partial}{\partial\lambda}\left[-n+\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}\right|_{\lambda=\hat{\lambda}}\\
 & =-\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}\Bigr\rvert_{\lambda=\hat{\lambda}}\\
 & =-\frac{1}{\bar{x}^{2}}\sum_{i=1}^{n}x_{i}\\
 & =-\frac{1}{\bar{x}^{2}}n\bar{x}\\
 & =-\frac{n}{\bar{x}}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $\bar{x}\geq0$
\end_inset

 (because the 
\begin_inset Formula $x_{i}\mbox{'s}$
\end_inset

 are each non-negative), so that 
\begin_inset Formula $-n/\bar{x}<0$
\end_inset

 (assuming that 
\begin_inset Formula $\bar{x}>0$
\end_inset

).
 It follows that 
\begin_inset Formula $\hat{\lambda}=\bar{x}$
\end_inset

 is the MLE.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\mbox{iid}}{\sim}U\left(\theta-\frac{1}{2},\theta+\frac{1}{2}\right)$
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x_{i}\right) & =\frac{1}{\left(\theta+\frac{1}{2}\right)-\left(\theta-\frac{1}{2}\right)}I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} }\\
 & =\frac{1}{1}I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} }\\
 & =I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} },
\end{flalign*}

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} }\\
 & =I_{\left\{ \theta-\frac{1}{2}<x_{1}<\theta+\frac{1}{2}\right\} }\cdot\ldots\cdot I_{\left\{ \theta-\frac{1}{2}<x_{n}<\theta+\frac{1}{2}\right\} }\\
 & =I_{\left\{ x_{\left(1\right)}>\theta-\frac{1}{2}\right\} }I_{\left\{ x_{\left(n\right)}<\theta+\frac{1}{2}\right\} }.
\end{flalign*}

\end_inset

Then, the likelihood function is given by 
\begin_inset Formula 
\begin{flalign*}
L\left(\theta|\mathbf{x}\right) & =\begin{cases}
1, & x_{\left(n\right)}-\frac{1}{2}<\theta<x_{\left(1\right)}+\frac{1}{2}\\
0, & \mbox{otherwise}
\end{cases},
\end{flalign*}

\end_inset

and its graph is shown below.
 Clearly, 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 is constant between 
\begin_inset Formula $x_{\left(n\right)}-\frac{1}{2}$
\end_inset

 and 
\begin_inset Formula $x_{\left(1\right)}+\frac{1}{2}$
\end_inset

 and 0 elsewhere, so that the MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is given by any point in the interval 
\begin_inset Formula $\left(x_{\left(n\right)}-\frac{1}{2},x_{\left(1\right)}+\frac{1}{2}\right)$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<likelihood-uniform, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,1,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(1, 2, length=1000) 
\end_layout

\begin_layout Plain Layout

plot(z, rep(1,1000), type="l", xlab = expression(theta), ylab=expression(paste(L
, "(",theta,"|",bold(x),")")), yaxt="n", xaxt="n", xlim = c(0, 3), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at=c(1,2), labels=c(expression(x[(n)]-frac(1,2)), expression(x[(1)]+frac
(1,2))), cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

axis(2, at=1, labels=1, cex.axis=0.75, las=2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots X_{n}\stackrel{\mbox{iid}}{\sim}f\left(x|\theta\right)=\theta/x^{2}$
\end_inset

, 
\begin_inset Formula $0<\theta\leq x<\infty$
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
The support of 
\begin_inset Formula $X$
\end_inset

 depends on the parameter 
\begin_inset Formula $\theta$
\end_inset

, so we will rewrite the pdf of 
\begin_inset Formula $X$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\frac{\theta}{x^{2}}I_{\left\{ x\geq\theta\right\} }.
\end{flalign*}

\end_inset

Then, the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}f_{X_{i}}\left(x_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}\frac{\theta}{x_{i}^{2}}I_{\left\{ x_{i}\geq\theta\right\} }\\
 & =\frac{\theta^{n}}{\prod_{i=1}^{n}x_{i}^{2}}I_{\left\{ x_{\left(1\right)}\geq\theta\right\} },
\end{flalign*}

\end_inset

so that the likelihood function is given by
\begin_inset Formula 
\begin{flalign*}
L\left(\theta|\mathbf{x}\right) & =\begin{cases}
\theta^{n}/\prod_{i=1}^{n}x_{i}^{2}, & \theta\leq x_{\left(1\right)}\\
0, & \theta>x_{\left(1\right)}
\end{cases}.
\end{flalign*}

\end_inset

The graph of the likelihood function is shown below.
 Clearly, the maximum value of 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 occurs at 
\begin_inset Formula $x_{\left(1\right)}$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}=X_{\left(1\right)}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(0, 1, length=1000) 
\end_layout

\begin_layout Plain Layout

plot(z, z^3, type="l", xlab = expression(theta), ylab=expression(paste(L,
 "(",theta,"|",bold(x),")")), yaxt="n", xaxt="n", xlim = c(0, 2), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at = 1, labels = expression(x[(1)]), cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\mbox{iid}}{\sim}U\left(\mu-\sqrt{3}\sigma,\mu+\sqrt{3}\sigma\right)$
\end_inset

.
 Find the MLEs of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x_{i}|\mu,\sigma\right) & =\frac{1}{\left(\mu+\sqrt{3}\sigma\right)-\left(\mu-\sqrt{3}\sigma\right)}I_{\left\{ \mu-\sqrt{3}\sigma<x_{i}<\mu+\sqrt{3}\sigma\right\} }\\
 & =\frac{1}{2\sqrt{3}\sigma}I_{\left\{ \mu-\sqrt{3}\sigma<x_{i}<\mu+\sqrt{3}\sigma\right\} },
\end{flalign*}

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\mu,\sigma\right) & =\prod_{i=1}^{n}f_{X_{i}}\left(x_{i}|\mu,\sigma\right)\\
 & =\prod_{i=1}^{n}\frac{1}{2\sqrt{3}\sigma}I_{\left\{ \mu-\sqrt{3}\sigma<x_{i}<\mu+\sqrt{3}\sigma\right\} }\\
 & =\left(2\sqrt{3}\sigma\right)^{-n}I_{\left\{ x_{\left(1\right)}>\mu-\sqrt{3}\sigma\right\} }I_{\left\{ x_{\left(n\right)}<\mu+\sqrt{3}\sigma\right\} }.
\end{flalign*}

\end_inset

To maximize 
\begin_inset Formula $L$
\end_inset

, we note that 
\begin_inset Formula $\left(2\sqrt{3}\sigma\right)^{-n}$
\end_inset

 increases as 
\begin_inset Formula $\sigma$
\end_inset

 decreases.
 Thus, we must find the minimum value of 
\begin_inset Formula $\sigma$
\end_inset

 such that 
\begin_inset Formula $L$
\end_inset

 is positive, which is true when 
\begin_inset Formula $\sigma>\left(\mu-x_{\left(1\right)}\right)/\sqrt{3}$
\end_inset

 and 
\begin_inset Formula $\sigma>\left(x_{\left(n\right)}-\mu\right)/\sqrt{3}$
\end_inset

.
 In the graph below, the region of positivity of 
\begin_inset Formula $L$
\end_inset

 is shaded.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3.5, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,4,0.1,4))
\end_layout

\begin_layout Plain Layout

z <- seq(-1, 3, length=2000) 
\end_layout

\begin_layout Plain Layout

# set yaxs="i" to remove the default 4% padding beyond the ylim
\end_layout

\begin_layout Plain Layout

plot(z, (2-z)/sqrt(3), type = "l", xlab = expression(mu), ylab = expression(sigm
a), yaxt = "n", xaxt = "n", yaxs = "i", xlim = c(0, 3), ylim = c(0,1.5),
 cex.lab = 0.75)
\end_layout

\begin_layout Plain Layout

lines(z, (z-1)/sqrt(3), type="l")
\end_layout

\begin_layout Plain Layout

polygon(c(-1, 1.5, 4), c(3/sqrt(3), 1/(2*sqrt(3)), 3/sqrt(3)), density=20,
 angle=0)
\end_layout

\begin_layout Plain Layout

axis(1, at = c(1,2), labels = c(expression(x[(1)]), expression(x[(n)])),
 cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

# use las to rotate the strings and padj to move them along the axis
\end_layout

\begin_layout Plain Layout

mtext(expression(sigma == frac(mu-x[(1)], sqrt(3))), side=4, line=0.5, las=2,
 padj=-1, cex=0.75)
\end_layout

\begin_layout Plain Layout

mtext(expression(sigma == frac(x[(n)]-mu, sqrt(3))), side=2, line=0.5, las=2,
 padj=-1, cex=0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
The minimum value of 
\begin_inset Formula $\sigma$
\end_inset

 such that 
\begin_inset Formula $L$
\end_inset

 is positive occurs precisely when the two lines intersect, i.e., when
\begin_inset Formula 
\begin{flalign*}
\frac{\mu-x_{\left(1\right)}}{\sqrt{3}} & =\frac{x_{\left(n\right)}-\mu}{\sqrt{3}}\\
\Leftrightarrow\mu-x_{\left(1\right)} & =x_{\left(n\right)}-\mu\\
\Leftrightarrow2\mu & =x_{\left(n\right)}+x_{\left(1\right)}\\
\Leftrightarrow\hat{\mu} & =\frac{x_{\left(n\right)}+x_{\left(1\right)}}{2}.
\end{flalign*}

\end_inset

We substitute into one of the line equations to find 
\begin_inset Formula $\hat{\sigma}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\hat{\sigma} & =\frac{\hat{\mu}-x_{\left(1\right)}}{\sqrt{3}}\\
 & =\frac{\frac{1}{2}\left(x_{\left(n\right)}+x_{\left(1\right)}\right)-x_{\left(1\right)}}{\sqrt{3}}\\
 & =\frac{x_{\left(n\right)}+x_{\left(1\right)}-2x_{\left(1\right)}}{2\sqrt{3}}\\
 & =\frac{x_{\left(n\right)}-x_{\left(1\right)}}{2\sqrt{3}}
\end{flalign*}

\end_inset

Thus, the MLEs for 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 are given by 
\begin_inset Formula $\hat{\mu}=\left(x_{\left(n\right)}+x_{\left(1\right)}\right)/2$
\end_inset

 and 
\begin_inset Formula $\hat{\sigma}=\left(x_{\left(n\right)}-x_{\left(1\right)}\right)/2\sqrt{3}$
\end_inset

, respectively.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Example
Supose that 
\begin_inset Formula $X_{i}$
\end_inset

 and 
\begin_inset Formula $Y_{i}$
\end_inset

 are independent for 
\begin_inset Formula $i=1,\ldots,n$
\end_inset

, where 
\begin_inset Formula $X_{i}\sim f\left(x|\lambda\right)=\left(1/\lambda\right)e^{-x/\lambda}$
\end_inset

 and 
\begin_inset Formula $Y_{i}\sim f\left(y|\mu\right)=\left(1/\mu\right)e^{-y/\mu}$
\end_inset

.
 We observe 
\begin_inset Formula 
\[
Z_{i}=\min\left(X_{i},Y_{i}\right)\quad\mbox{and}\quad W_{i}=\begin{cases}
1, & \mbox{if }Z_{i}=X_{i}\\
0, & \mbox{if }Z_{i}=Y_{i}
\end{cases}.
\]

\end_inset

Find the MLEs of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
In the case where 
\begin_inset Formula $W_{i}=1$
\end_inset

, we have 
\begin_inset Formula $Z_{i}=\min\left(X_{i},Y_{i}\right)=X_{i}$
\end_inset

, so that our information about 
\begin_inset Formula $Y_{i}$
\end_inset

 is limited to 
\begin_inset Formula $Y_{i}>Z_{i}$
\end_inset

, and we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=1\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =P\left(\left\{ X_{i}=z_{i}\right\} \cap\left\{ Y_{i}>z_{i}\right\} \right)\\
 & =P\left(\left\{ X_{i}=z_{i}\right\} \right)\cdot P\left(\left\{ Y_{i}>z_{i}\right\} \right)\\
 & =f_{X_{i}}\left(z_{i}|\lambda\right)\cdot\left(1-F_{Y_{i}}\left(z_{i}|\mu\right)\right).
\end{flalign*}

\end_inset

In the case where 
\begin_inset Formula $W_{i}=0$
\end_inset

, we have 
\begin_inset Formula $Z_{i}=\min\left(X_{i},Y_{i}\right)=Y_{i}$
\end_inset

, so that our information about 
\begin_inset Formula $X_{i}$
\end_inset

 is limited to 
\begin_inset Formula $X_{i}>Z_{i}$
\end_inset

, and we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=0\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =P\left(\left\{ Y_{i}=z_{i}\right\} \cap\left\{ X_{i}>z_{i}\right\} \right)\\
 & =P\left(\left\{ Y_{i}=z_{i}\right\} \right)\cdot P\left(\left\{ X_{i}>z_{i}\right\} \right)\\
 & =f_{Y_{i}}\left(z_{i}|\mu\right)\cdot\left(1-F_{X_{i}}\left(z_{i}|\lambda\right)\right).
\end{flalign*}

\end_inset

The cdf of 
\begin_inset Formula $Y_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
F_{Y_{i}}\left(y|\mu\right) & =\int_{0}^{y}f_{Y_{i}}\left(t|\mu\right)\mbox{d}t\\
 & =\int_{0}^{y}\frac{1}{\mu}e^{-t/\mu}\mbox{d}t\\
 & =\left[-e^{-t/\mu}\right|_{0}^{y}\\
 & =-e^{-y/\mu}-\left(-e^{0}\right)\\
 & =1-e^{-y/\mu}.
\end{flalign*}

\end_inset

By symmetry, the cdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula $F_{X_{i}}\left(x|\lambda\right)=1-e^{-x/\lambda}$
\end_inset

.
 So, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=1\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =\frac{1}{\lambda}e^{-z_{i}/\lambda}\cdot e^{-z_{i}/\mu}
\end{flalign*}

\end_inset

and 
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=0\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =\frac{1}{\mu}e^{-z_{i}/\mu}\cdot e^{-z_{i}/\lambda}.
\end{flalign*}

\end_inset

Then, the joint pdf of 
\begin_inset Formula $W_{i}$
\end_inset

 and 
\begin_inset Formula $Z_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{W_{i},Z_{i}}\left(w_{i},z_{i}|\lambda,\mu\right) & =P\left(\left\{ Z_{i}=z_{i}\right\} \cap\left\{ W_{i}=w_{i}\right\} \right)\\
 & =\left[\frac{1}{\lambda}e^{-z_{i}/\lambda}e^{-z_{i}/\mu}\right]^{w_{i}}\left[\frac{1}{\mu}e^{-z_{i}/\mu}e^{-z_{i}/\lambda}\right]^{1-w_{i}}\\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}e^{\left(-z_{i}/\lambda\right)w_{i}}e^{\left(-z_{i}/\mu\right)w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}e^{\left(-z_{i}/\mu\right)\left(1-w_{i}\right)}e^{\left(-z_{i}/\lambda\right)\left(1-w_{i}\right)}\\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}\exp\left\{ -w_{i}\frac{z_{i}}{\lambda}-w_{i}\frac{z_{i}}{\mu}-\frac{z_{i}}{\mu}+w_{i}\frac{z_{i}}{\mu}-\frac{z_{i}}{\lambda}+w_{i}\frac{z_{i}}{\lambda}\right\} \\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}\exp\left\{ -\frac{z_{i}}{\mu}-\frac{z_{i}}{\lambda}\right\} \\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}e^{-z_{i}/\mu}e^{-z_{i}/\lambda},
\end{flalign*}

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{W}=W_{1},\ldots,W_{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Z}=Z_{1},\ldots,Z_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{W},\mathbf{Z}}\left(\mathbf{w},\mathbf{z}|\lambda,\mu\right) & =\prod_{i=1}^{n}f_{W_{i},Z_{i}}\left(w_{i},z_{i}|\lambda,\mu\right)\\
 & =\prod_{i=1}^{n}\left[\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}e^{-z_{i}/\mu}e^{-z_{i}/\lambda}\right]\\
 & =\prod_{i=1}^{n}\left[\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}\exp\left\{ -z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\right\} \right]\\
 & =\lambda^{-\sum_{i=1}^{n}w_{i}}\mu^{-\sum_{i=1}^{n}\left(1-w_{i}\right)}\exp\left\{ -\sum_{i=1}^{n}z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\right\} .
\end{flalign*}

\end_inset

Then, the log-likelihood is given by 
\begin_inset Formula 
\begin{flalign*}
\ln L\left(\lambda,\mu|\mathbf{w},\mathbf{z}\right) & =\ln\left[\lambda^{-\sum_{i=1}^{n}w_{i}}\mu^{-\sum_{i=1}^{n}\left(1-w_{i}\right)}\exp\left\{ -\sum_{i=1}^{n}z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\right\} \right]\\
 & =\ln\lambda^{-\sum_{i=1}^{n}w_{i}}+\ln\mu^{-\sum_{i=1}^{n}\left(1-w_{i}\right)}-\sum_{i=1}^{n}z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\\
 & =-\ln\lambda\sum_{i=1}^{n}w_{i}-\ln\mu\sum_{i=1}^{n}\left(1-w_{i}\right)-\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\sum_{i=1}^{n}z_{i}.
\end{flalign*}

\end_inset

We will take the derivative of 
\begin_inset Formula $\ln L$
\end_inset

 with respect to 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\lambda}\ln L\left(\lambda,\mu|\mathbf{w},\mathbf{z}\right) & =\frac{\partial}{\partial\lambda}\left[-\ln\lambda\sum_{i=1}^{n}w_{i}-\ln\mu\sum_{i=1}^{n}\left(1-w_{i}\right)-\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\sum_{i=1}^{n}z_{i}\right]\\
 & =-\frac{1}{\lambda}\sum_{i=1}^{n}w_{i}-0+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}z_{i}\\
 & =-\frac{1}{\lambda}\sum_{i=1}^{n}w_{i}+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}z_{i}
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{1}{\hat{\lambda}^{2}}\sum_{i=1}^{n}z_{i} & =\frac{1}{\hat{\lambda}}\sum_{i=1}^{n}w_{i}\\
\Leftrightarrow\hat{\lambda}^{2}\sum_{i=1}^{n}w_{i} & =\hat{\lambda}\sum_{i=1}^{n}z_{i}\\
\Leftrightarrow\hat{\lambda} & =\frac{\sum_{i=1}^{n}z_{i}}{\sum_{i=1}^{n}w_{i}}.
\end{flalign*}

\end_inset

We will now take the derivative of 
\begin_inset Formula $\ln L$
\end_inset

 with respect to 
\begin_inset Formula $\mu$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\mu}\ln L\left(\lambda,\mu|\mathbf{w},\mathbf{z}\right) & =\frac{\partial}{\partial\mu}\left[-\ln\lambda\sum_{i=1}^{n}w_{i}-\ln\mu\sum_{i=1}^{n}\left(1-w_{i}\right)-\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\sum_{i=1}^{n}z_{i}\right]\\
 & =0-\frac{1}{\mu}\sum_{i=1}^{n}\left(1-w_{i}\right)+\frac{1}{\mu^{2}}\sum_{i=1}^{n}z_{i}\\
 & =-\frac{1}{\mu}\sum_{i=1}^{n}\left(1-w_{i}\right)+\frac{1}{\mu^{2}}\sum_{i=1}^{n}z_{i}.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{1}{\hat{\mu}^{2}}\sum_{i=1}^{n}z_{i} & =\frac{1}{\hat{\mu}}\sum_{i=1}^{n}\left(1-w_{i}\right)\\
\Leftrightarrow\hat{\mu}^{2}\sum_{i=1}^{n}\left(1-w_{i}\right) & =\hat{\mu}\sum_{i=1}^{n}z_{i}\\
\Leftrightarrow\hat{\mu} & =\frac{\sum_{i=1}^{n}z_{i}}{\sum_{i=1}^{n}\left(1-w_{i}\right)}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Remark*
The above example is a common setting for survival (or failure) analysis.
 For example, 
\begin_inset Formula $X_{i}$
\end_inset

 might be the time of cure for the 
\begin_inset Formula $i\mbox{th}$
\end_inset

 subject, and 
\begin_inset Formula $Y_{i}$
\end_inset

 the follow-up time.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Invariance property of MLEs]
\end_layout

\end_inset

If 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the MLE of 
\begin_inset Formula $\theta$
\end_inset

, then for any function 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, the MLE of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

 is 
\begin_inset Formula $\tau\left(\hat{\theta}\right)$
\end_inset

.
 (This is Theorem 7.2.10 from Casella & Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:invariance-mle"

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\hat{\eta}$
\end_inset

 denote the value the maximizes 
\begin_inset Formula $L^{*}\left(\eta|\mathbf{x}\right)$
\end_inset

.
 We must show that 
\begin_inset Formula $L^{*}\left(\hat{\eta}|\mathbf{x}\right)=L^{*}\left[\tau\left(\hat{\theta}\right)|\mathbf{x}\right]$
\end_inset

.
 Now, the maxima of 
\begin_inset Formula $L$
\end_inset

 and 
\begin_inset Formula $L^{*}$
\end_inset

 coincide, so we have
\begin_inset Formula 
\begin{flalign*}
L^{*}\left(\hat{\eta}|\mathbf{x}\right) & =\sup_{\eta}\sup_{\left\{ \theta:\tau\left(\theta\right)=\eta\right\} }L\left(\theta|\mathbf{x}\right)\\
 & =\sup_{\theta}L\left(\theta|\mathbf{x}\right)\\
 & =L\left(\hat{\theta}|\mathbf{x}\right),
\end{flalign*}

\end_inset

where the second equality follows because the iterated maximization is equal
 to the unconditional maximization over 
\begin_inset Formula $\theta$
\end_inset

, which is attained at 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 Furthermore
\begin_inset Formula 
\begin{flalign*}
L\left(\hat{\theta}|\mathbf{x}\right) & =\sup_{\left\{ \theta:\tau\left(\theta\right)=\tau\left(\hat{\theta}\right)\right\} }L\left(\theta|\mathbf{x}\right)\\
 & =L^{*}\left[\tau\left(\hat{\theta}\right)|\mathbf{x}\right].
\end{flalign*}

\end_inset

Hence the string of equalities shows that 
\begin_inset Formula $L^{*}\left(\hat{\eta}|\mathbf{x}\right)=L^{*}\left(\tau\left(\hat{\theta}\right)|\mathbf{x}\right)$
\end_inset

 and that 
\begin_inset Formula $\tau\left(\hat{\theta}\right)$
\end_inset

 is the MLE of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Example
Suppose that 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\mbox{iid}}{\sim}\mbox{Exp}\left(\lambda\right)$
\end_inset

, so that the pdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula $f_{X_{i}}\left(x|\lambda\right)=\lambda e^{-\lambda x}$
\end_inset

, and the MLE of 
\begin_inset Formula $\lambda$
\end_inset

 is 
\begin_inset Formula $\hat{\lambda}=1/\bar{X}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
Find the MLE of 
\begin_inset Formula $P\left(\left\{ X>a\right\} \right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
Let 
\begin_inset Formula $\eta=P\left(\left\{ X>a\right\} \right)$
\end_inset

, so that we have 
\begin_inset Formula 
\begin{flalign*}
\eta & =P\left(\left\{ X>a\right\} \right)\\
 & =\int_{a}^{\infty}f_{X}\left(x|\lambda\right)\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\int_{a}^{c}\lambda e^{-\lambda x}\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\left[-e^{-\lambda x}\right|_{a}^{c}\\
 & =\lim_{c\rightarrow\infty}\left(-e^{-\lambda c}-\left(-e^{-\lambda a}\right)\right)\\
 & =\lim_{c\rightarrow\infty}\left(-e^{-\lambda c}+e^{-\lambda a}\right)\\
 & =0+e^{-\lambda a}\\
 & =e^{-\lambda a}.
\end{flalign*}

\end_inset

Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:invariance-mle"

\end_inset

, we have 
\begin_inset Formula $\hat{\eta}=e^{-\left(1/\bar{x}\right)a}=e^{-a/\bar{x}}$
\end_inset

.
 Note that 
\begin_inset Formula 
\begin{flalign*}
\ln\eta & =\ln e^{-\lambda a}\\
\Leftrightarrow-\lambda a & =\ln\eta\\
\Leftrightarrow\lambda & =-\frac{\ln\eta}{a},
\end{flalign*}

\end_inset

so that
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x|\eta\right) & =\left(-\frac{\ln\eta}{a}\right)e^{-\left(-\ln\eta/a\right)x}\\
 & =\left(-\frac{\ln\eta}{a}\right)e^{\left(x\ln\eta\right)/a}.
\end{flalign*}

\end_inset

Then, the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\eta\right) & =\prod_{i=1}^{n}f_{X_{i}}\left(x|\eta\right)\\
 & =\prod_{i=1}^{n}\left(-\frac{\ln\eta}{a}\right)e^{\left(x_{i}\ln\eta\right)/a}\\
 & =\left(-\frac{\ln\eta}{a}\right)^{n}e^{\sum_{i=1}^{n}\left(x_{i}\ln\eta\right)/a},
\end{flalign*}

\end_inset

so that the log-likelihood function is given by
\begin_inset Formula 
\begin{flalign*}
\ln L\left(\eta|\mathbf{x}\right) & =\ln\left[\left(-\frac{\ln\eta}{a}\right)^{n}e^{\sum_{i=1}^{n}\left(x_{i}\ln\eta\right)/a}\right]\\
 & =\ln\left[\left(-\frac{\ln\eta}{a}\right)^{n}\right]+\ln\left[e^{\sum_{i=1}^{n}\left(x_{i}\ln\eta\right)/a}\right]\\
 & =n\ln\left(-\frac{\ln\eta}{a}\right)+\sum_{i=1}^{n}\left(\frac{x_{i}\ln\eta}{a}\right)\\
 & =n\ln\left(-\frac{\ln\eta}{a}\right)+\frac{\ln\eta}{a}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\eta$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\eta}\ln L\left(\eta|\mathbf{x}\right) & =\frac{\partial}{\partial\eta}\left[n\ln\left(-\frac{\ln\eta}{a}\right)+\ln\eta\left(a\sum_{i=1}^{n}x_{i}\right)\right]\\
 & =\frac{n}{-\frac{\ln\eta}{a}}\left(-\frac{1}{\eta a}\right)+\frac{1}{\eta a}\sum_{i=1}^{n}x_{i}\\
 & =\frac{n}{\eta\ln\eta}+\frac{1}{\eta a}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{n}{\hat{\eta}\ln\hat{\eta}}+\frac{1}{\hat{\eta}a}\sum_{i=1}^{n}x_{i} & =0\\
\Leftrightarrow\frac{n}{\hat{\eta}\ln\hat{\eta}} & =-\frac{1}{\hat{\eta}a}\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\hat{\eta}\ln\hat{\eta}\sum_{i=1}^{n}x_{i} & =-n\hat{\eta}a\\
\Leftrightarrow\ln\hat{\eta} & =-\frac{na}{\sum_{i=1}^{n}x_{i}}\\
\Leftrightarrow\ln\hat{\eta} & =-\frac{a}{\bar{x}}\\
\Leftrightarrow\hat{\eta} & =e^{-a/\bar{x}},
\end{flalign*}

\end_inset

which agrees with the result obtained from the theorem.
\end_layout

\end_deeper
\begin_layout Enumerate
Find the MLE of 
\begin_inset Formula $\mbox{median}\left(X_{1},\ldots,X_{n}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
The median of a distribution is a value 
\begin_inset Formula $m$
\end_inset

 such that 
\begin_inset Formula $P\left(\left\{ Y\leq m\right\} \right)\geq\frac{1}{2}$
\end_inset

 and 
\begin_inset Formula $P\left(\left\{ Y\geq m\right\} \right)\geq\frac{1}{2}.$
\end_inset

 If 
\begin_inset Formula $Y$
\end_inset

 is continuous, 
\begin_inset Formula $m$
\end_inset

 satisfies 
\begin_inset Formula 
\[
\int_{-\infty}^{m}f\left(y\right)\mbox{d}y=\int_{m}^{\infty}f\left(y\right)\mbox{d}y=\frac{1}{2}.
\]

\end_inset


\begin_inset Formula $X$
\end_inset

 is continuous, so it follows that 
\begin_inset Formula 
\begin{flalign*}
\frac{1}{2} & =P\left(\left\{ X\leq m\right\} \right)\\
 & =F_{X}\left(m\right).
\end{flalign*}

\end_inset

The cdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
F_{X}\left(x\right) & =\int_{0}^{x}f_{X}\left(x\right)\mbox{d}x\\
 & =\int_{0}^{x}\lambda e^{-\lambda x}\mbox{d}x\\
 & =-e^{-\lambda x}\Big\rvert_{0}^{x}\\
 & =-e^{-\lambda x}-\left(-e^{0}\right)\\
 & =1-e^{-\lambda x},
\end{flalign*}

\end_inset

so it follows that 
\begin_inset Formula 
\begin{flalign*}
F_{X}\left(m\right) & =1-e^{-\lambda m}\\
 & =\frac{1}{2}\\
\Leftrightarrow e^{-\lambda m} & =\frac{1}{2}\\
\Leftrightarrow-\lambda m & =\ln\left(2^{-1}\right)\\
\Leftrightarrow-\lambda m & =-\ln2\\
\Leftrightarrow m & =\frac{\ln2}{\lambda}.
\end{flalign*}

\end_inset

Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:invariance-mle"

\end_inset

, we have 
\begin_inset Formula 
\begin{flalign*}
\hat{m} & =\frac{\ln2}{\hat{\lambda}}\\
\Leftrightarrow\hat{m} & =\frac{\ln2}{\frac{1}{\bar{x}}}\\
 & =\bar{x}\ln2.
\end{flalign*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Theorem
If an MLE is unique, then it is a function of the sufficient statistics.
\end_layout

\begin_layout Proof
Suppose that 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 is a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, whose pdf is given by 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

.
 Suppose also that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the MLE of 
\begin_inset Formula $\theta$
\end_inset

, and that the associated likelihood function is given by
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

.
 Then, we have
\begin_inset Formula 
\[
L\left(\theta|\mathbf{x}\right)=f\left(\mathbf{x}|\theta\right)=h\left(\mathbf{x}\right)g\left(T\left(\mathbf{x}\right)|\theta\right),
\]

\end_inset

where the final equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

.
 Then, maximizing 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 is equivalent to maximizing 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

, which is a function of 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

, which is a sufficient statistic.
\end_layout

\begin_layout Standard
Often, it is not possible to find the MLE analytically and we need to use
 numerical methods.
 Two commonly used methods are the Newton-Raphson algorithm and the Expectation-
Maximization (EM) algorithm.
 Both are iterative methods that produce a sequence of values 
\begin_inset Formula $\theta^{\left(0\right)},\theta^{\left(1\right)},\ldots$
\end_inset

 that, under ideal conditions, converge to the MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 It is helpful to use a good starting value 
\begin_inset Formula $\theta^{\left(0\right)}$
\end_inset

.
 Often, the method of moments estimator is a good starting value.
\end_layout

\begin_layout Subsubsection
Newton-Raphson
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $l\left(\theta|\mathbf{x}\right)=\ln L\left(\theta|\mathbf{x}\right)$
\end_inset

.
 When we maximize the log-likelihood by setting its derivative equal to
 zero, we are solving the equation
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}l\left(\theta|\mathbf{x}\right) & =0.
\end{flalign*}

\end_inset

To motivate Newton-Raphson, we will expand the derivative of 
\begin_inset Formula $l\left(\theta|\mathbf{x}\right)$
\end_inset

around 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

.
 Recall that the Taylor series expansion of 
\begin_inset Formula $f$
\end_inset

 around 
\begin_inset Formula $a$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f\left(x\right) & =\sum_{n=0}^{\infty}\frac{f^{\left(n\right)}\left(a\right)}{n!}\left(x-a\right)^{n}\\
 & =f\left(a\right)+f'\left(a\right)\left(x-a\right)+\frac{f''\left(a\right)}{2!}\left(x-a\right)^{2}+\frac{f'''\left(a\right)}{3!}\left(x-a\right)^{3}+\cdots,
\end{flalign*}

\end_inset

so that we have 
\begin_inset Formula 
\begin{flalign*}
l'\left(\hat{\theta}\right) & \approx l'\left(\theta^{\left(j\right)}\right)+l''\left(\theta^{\left(j\right)}\right)\left(\hat{\theta}-\theta^{\left(j\right)}\right).
\end{flalign*}

\end_inset

Setting 
\begin_inset Formula $l'$
\end_inset

 equal to zero and solving for 
\begin_inset Formula $\hat{\theta}$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
0 & =l'\left(\hat{\theta}\right)\\
 & \approx l'\left(\theta^{\left(j\right)}\right)+l''\left(\theta^{\left(j\right)}\right)\left(\hat{\theta}-\theta^{\left(j\right)}\right)\\
 & \approx l'\left(\theta^{\left(j\right)}\right)+\hat{\theta}l''\left(\theta^{\left(j\right)}\right)-\theta^{\left(j\right)}l''\left(\theta^{\left(j\right)}\right)\\
\Leftrightarrow\hat{\theta}l''\left(\theta^{\left(j\right)}\right) & \approx\theta^{\left(j\right)}l''\left(\theta^{\left(j\right)}\right)-l'\left(\theta^{\left(j\right)}\right)\\
\Leftrightarrow\hat{\theta} & \approx\frac{\theta^{\left(j\right)}l''\left(\theta^{\left(j\right)}\right)-l'\left(\theta^{\left(j\right)}\right)}{l''\left(\theta^{\left(j\right)}\right)}\\
\Leftrightarrow\hat{\theta} & \approx\theta^{\left(j\right)}-\frac{l'\left(\theta^{\left(j\right)}\right)}{l''\left(\theta^{\left(j\right)}\right)}.
\end{flalign*}

\end_inset

This suggests the following iterative scheme:
\begin_inset Formula 
\[
\hat{\theta}^{\left(j+1\right)}=\theta^{\left(j\right)}-\frac{l'\left(\theta^{\left(j\right)}\right)}{l''\left(\theta^{\left(j\right)}\right)}.
\]

\end_inset

We will continue to improve the estimator in this way until convergence.
\end_layout

\begin_layout Standard
In the multiparameter case, the MLE 
\begin_inset Formula $\hat{\boldsymbol{\theta}}=\left(\hat{\theta}_{1},\ldots,\hat{\theta}_{k}\right)$
\end_inset

 is a vector and the method becomes
\begin_inset Formula 
\[
\hat{\boldsymbol{\theta}}^{\left(j+1\right)}=\boldsymbol{\theta}^{\left(j\right)}-H^{-1}l'\left(\boldsymbol{\theta}^{\left(j\right)}\right),
\]

\end_inset

where 
\begin_inset Formula $l'\left(\boldsymbol{\theta}^{\left(j\right)}\right)$
\end_inset

 is a vector of first derivatives, i.e., 
\begin_inset Formula 
\[
l'\left(\boldsymbol{\theta}^{\left(j\right)}\right)=\left(\frac{\partial}{\partial\theta_{1}}l\left(\boldsymbol{\theta}\right),\ldots,\frac{\partial}{\partial\theta_{k}}l\left(\boldsymbol{\theta}\right)\right),
\]

\end_inset

and 
\begin_inset Formula $H$
\end_inset

 is the (Hessian) matrix of second partial derivatives of the log-likelihood,
 i.e., 
\begin_inset Formula 
\[
H=l''\left(\boldsymbol{\theta}\right)=\begin{bmatrix}\dfrac{\partial^{2}}{\partial\theta_{1}^{2}}l\left(\theta\right) & \cdots & \dfrac{\partial^{2}}{\partial\theta_{1}\theta_{k}}l\left(\theta\right)\\
\vdots & \ddots & \vdots\\
\dfrac{\partial^{2}}{\partial\theta_{k}\theta_{1}}l\left(\theta\right) & \cdots & \dfrac{\partial^{2}}{\partial\theta_{k}^{2}}l\left(\theta\right)
\end{bmatrix}.
\]

\end_inset

Suppose 
\begin_inset Formula $\epsilon$
\end_inset

 is sufficiently close to zero.
 Then, the convergence criterion may be 
\begin_inset Formula $\left|\theta^{\left(j+1\right)}-\theta^{\left(j\right)}\right|<\epsilon$
\end_inset

, 
\begin_inset Formula $l'\left(\theta^{\left(j+1\right)}\right)<\epsilon$
\end_inset

, or 
\begin_inset Formula $L\left(\theta^{\left(j+1\right)}\right)-L\left(\theta^{\left(j\right)}\right)<\epsilon$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 with
\begin_inset Formula 
\[
Y_{i}=\begin{cases}
0, & \mbox{if subject }i\mbox{ did not experience the event}\\
1, & \mbox{if subject }i\mbox{ experienced the event}
\end{cases}.
\]

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 fixed covariate for each subject and 
\begin_inset Formula 
\[
P\left(\left\{ Y_{i}=1\right\} \right)=\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}.
\]

\end_inset

Suppose 
\begin_inset Formula $n=5$
\end_inset

 and 
\begin_inset Formula $\left(x_{i},y_{i}\right)$
\end_inset

 are as shown below.
 Solve for the MLE of 
\begin_inset Formula $\theta$
\end_inset

 using the Newton-Raphson algorithm.
\end_layout

\begin_layout Example
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="6">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{i}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y_{i}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Example
We have 
\begin_inset Formula $Y_{i}\sim\mbox{Bernoulli}\left(p_{i}\right)$
\end_inset

, so that the pmf of 
\begin_inset Formula $Y_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{Y_{i}}\left(y_{i}|p_{i}\right) & =p_{i}^{y_{i}}\left(1-p_{i}\right)^{1-y_{i}}.
\end{flalign*}

\end_inset

We are given that 
\begin_inset Formula $P\left(\left\{ Y_{i}=1\right\} \right)=e^{\theta x_{i}}/\left(1+e^{\theta x_{i}}\right)$
\end_inset

, so we can write
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y_{i}=1\right\} \right) & =p_{i}^{1}\left(1-p_{i}\right)^{1-1}\\
 & =p_{i}\\
 & =\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}.
\end{flalign*}

\end_inset

Then, we can reparameterize 
\begin_inset Formula $p_{Y_{i}}$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
p_{Y_{i}}\left(y_{i}|\theta\right) & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(1-\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{1-y_{i}}\\
 & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(\frac{1+e^{\theta x_{i}}-e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{1-y_{i}}\\
 & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(\frac{1}{1+e^{\theta x_{i}}}\right)^{1-y_{i}}\\
 & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(\frac{1}{1+e^{\theta x_{i}}}\right)\left(\frac{1}{1+e^{\theta x_{i}}}\right)^{-y_{i}}\\
 & =\frac{\left(e^{\theta x_{i}}\right)^{y_{i}}}{\left(1+e^{\theta x_{i}}\right)^{y_{i}}}\left(1+e^{\theta x_{i}}\right)^{-1}\left(1+e^{\theta x_{i}}\right)^{y_{i}}\\
 & =e^{\theta x_{i}y_{i}}\left(1+e^{\theta x_{i}}\right)^{-1},
\end{flalign*}

\end_inset

so that the joint pmf of 
\begin_inset Formula $\mathbf{Y}=Y_{1},\ldots,Y_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{Y}}\left(\mathbf{y}|\theta\right) & =\prod_{i=1}^{n}p_{Y_{i}}\left(y_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}e^{\theta x_{i}y_{i}}\left(1+e^{\theta x_{i}}\right)^{-1}\\
 & =e^{\sum_{i=1}^{n}\theta x_{i}y_{i}}\prod_{i=1}^{n}\left(1+e^{\theta x_{i}}\right)^{-1}.
\end{flalign*}

\end_inset

Then, the log-likelihood is given by 
\begin_inset Formula 
\begin{flalign*}
l\left(\theta\right) & =\ln L\left(\theta|\mathbf{y}\right)\\
 & =\ln\left[e^{\sum_{i=1}^{n}\theta x_{i}y_{i}}\prod_{i=1}^{n}\left(1+e^{\theta x_{i}}\right)^{-1}\right]\\
 & =\ln e^{\theta\sum_{i=1}^{n}x_{i}y_{i}}+\ln\prod_{i=1}^{n}\left(1+e^{\theta x_{i}}\right)^{-1}\\
 & =\theta\sum_{i=1}^{n}x_{i}y_{i}+\sum_{i=1}^{n}\ln\left(1+e^{\theta x_{i}}\right)^{-1}\\
 & =\theta\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\ln\left(1+e^{\theta x_{i}}\right).
\end{flalign*}

\end_inset

We will take the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}l\left(\theta\right) & =\frac{\partial}{\partial\theta}\left[\theta\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\ln\left(1+e^{\theta x_{i}}\right)\right]\\
 & =\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+e^{\theta x_{i}}}\cdot x_{i}e^{\theta x_{i}}
\end{flalign*}

\end_inset

To implement Newton-Raphson algorithm, we will also need the second derivative
 with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta^{2}}l\left(\theta\right) & =\frac{\partial}{\partial\theta}\left[\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+e^{\theta x_{i}}}\cdot x_{i}e^{\theta x_{i}}\right]\\
 & =0-\sum_{i=1}^{n}\frac{\left(x_{i}^{2}e^{\theta x_{i}}\right)\left(1+e^{\theta x_{i}}\right)-\left(x_{i}e^{\theta x_{i}}\right)\left(x_{i}e^{\theta x_{i}}\right)}{\left(1+e^{\theta x_{i}}\right)^{2}}\\
 & =-\sum_{i=1}^{n}\frac{x_{i}^{2}e^{\theta x_{i}}+x_{i}^{2}e^{2\theta x_{i}}-x_{i}^{2}e^{2\theta x_{i}}}{\left(1+e^{\theta x_{i}}\right)^{2}}\\
 & =-\sum_{i=1}^{n}\frac{x_{i}^{2}e^{\theta x_{i}}}{\left(1+e^{\theta x_{i}}\right)^{2}}
\end{flalign*}

\end_inset

Thus, we must solve
\begin_inset Formula 
\begin{flalign*}
\hat{\theta} & =\theta^{\left(j\right)}-\frac{l'\left(\theta^{\left(j\right)}\right)}{l''\left(\theta^{\left(j\right)}\right)}\\
 & =\theta^{\left(j\right)}-\frac{\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+e^{\theta x_{i}}}\cdot x_{i}e^{\theta x_{i}}}{-\sum_{i=1}^{n}\frac{x_{i}^{2}e^{\theta x_{i}}}{\left(1+e^{\theta x_{i}}\right)^{2}}}\\
 & =\theta^{\left(j\right)}+\frac{\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+e^{\theta x_{i}}}\cdot x_{i}e^{\theta x_{i}}}{\sum_{i=1}^{n}\frac{x_{i}^{2}e^{\theta x_{i}}}{\left(1+e^{\theta x_{i}}\right)^{2}}}.
\end{flalign*}

\end_inset

We can implement the algorithm using the following R code.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

newtraph <- function(x, y, theta, eps = 1e-6) {
\end_layout

\begin_layout Plain Layout

	diff <- 1
\end_layout

\begin_layout Plain Layout

	theta.hat <- theta
\end_layout

\begin_layout Plain Layout

	while (diff > eps) {
\end_layout

\begin_layout Plain Layout

		eta <- theta * x
\end_layout

\begin_layout Plain Layout

		U <- sum(x * y) - sum((x * exp(eta)) / (1 + exp(eta)))
\end_layout

\begin_layout Plain Layout

		I <- sum((x^2 * exp(eta)) / (1 + exp(eta)^2))
\end_layout

\begin_layout Plain Layout

		theta.new <- theta + (U / I)
\end_layout

\begin_layout Plain Layout

		diff <- abs(theta.new - theta)
\end_layout

\begin_layout Plain Layout

		theta <- theta.new
\end_layout

\begin_layout Plain Layout

		theta.hat <- c(theta.hat, theta)
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

	return(theta.hat)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

x <- c(4.1,2.2,3.9,7.1,6.2)
\end_layout

\begin_layout Plain Layout

y <- c(0,1,0,1,1)
\end_layout

\begin_layout Plain Layout

newtraph(x, y, 0.3)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
So, we see that our estimates for 
\begin_inset Formula $\hat{\theta}$
\end_inset

 converge and 
\begin_inset Formula $\hat{\theta}\approx0.125$
\end_inset

.
\end_layout

\begin_layout Subsubsection
EM algorithm
\end_layout

\begin_layout Standard
The idea behind the EM algorithm is to iterate between taking an expectation
 and maximizing.
 Consider data 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 whose density 
\begin_inset Formula $f\left(\mathbf{y}|\theta\right)$
\end_inset

 leads to a log-likelihood function that is hard to maximize.
 Suppose we can find another random variable 
\begin_inset Formula $Z$
\end_inset

 such that 
\begin_inset Formula $f\left(\mathbf{y}|\theta\right)=\int f\left(\mathbf{y},\mathbf{z}|\theta\right)\mbox{d}z$
\end_inset

 and such that the likelihood based on 
\begin_inset Formula $f\left(\mathbf{y},\mathbf{z}|\theta\right)$
\end_inset

 is easy to maximize.
 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 is the observed data, and 
\begin_inset Formula $Z$
\end_inset

 is called the augmented (or latent, or missing) data, and 
\begin_inset Formula $L\left(\theta|\mathbf{y},\mathbf{z}\right)=f\left(\mathbf{y},\mathbf{z}|\theta\right)$
\end_inset

 is the complete-data likelihood.
 Conceptually, the EM algorithm works by filling in the missing data, maximizing
 the expectation of the complete log-likelihood, and iterating until convergence.
\end_layout

\begin_layout Standard
The EM algorithm proceeds as follows:
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(0)]
\backslash
setcounter{enumi}{0}
\end_layout

\end_inset

Pick a starting value 
\begin_inset Formula $\theta^{\left(0\right)}$
\end_inset

.
 For 
\begin_inset Formula $j=1,2,\ldots$
\end_inset

, repeat steps 1 and 2 below.
\end_layout

\begin_layout Enumerate

\series bold
E-step:
\series default
 Calculate
\begin_inset Formula 
\begin{flalign*}
J\left(\theta|\theta^{\left(j\right)}\right) & =\mbox{E}\left[\log L\left(\theta|\mathbf{y},\mathbf{z}\right)|\theta^{\left(j\right)},\mathbf{y}\right]\\
 & =\mbox{E}\left[\log f\left(y_{1},\ldots,y_{n},z_{1},\ldots,z_{n}|\theta\right)|\theta^{\left(j\right)},Y_{1}=y_{1},\ldots,Y_{n}=y_{n}\right]
\end{flalign*}

\end_inset

The expectation is over the missing data 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 treating 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

 and the observed data 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 as fixed.
\end_layout

\begin_layout Enumerate

\series bold
M-step:
\series default
 Find 
\begin_inset Formula $\theta^{\left(j+1\right)}$
\end_inset

 to maximize 
\begin_inset Formula $J\left(\theta|\theta^{\left(j\right)}\right)$
\end_inset

, i.e., take the derivative, set it equal to zero, and solve for 
\begin_inset Formula $\theta\rightarrow\theta^{\left(j+1\right)}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Mixture of normals]
\end_layout

\end_inset

Often the data distribution may arise as a mixture of normal densities.
 Consider, for instance, heights of people arising as a mixture of men's
 and women's heights.
 The density of a mixture of two Normals is
\begin_inset Formula 
\[
f\left(y|\theta\right)=\left(1-p\right)\phi\left(y|\mu_{0},\sigma_{0}\right)+p\phi\left(y|\mu_{1},\sigma_{1}\right)
\]

\end_inset

where 
\begin_inset Formula $\phi\left(y|\mu,\sigma\right)$
\end_inset

 denotes a normal density with mean 
\begin_inset Formula $\mu$
\end_inset

 and standard deviation 
\begin_inset Formula $\sigma$
\end_inset

.
 The idea is that an observation is drawn from the first normal with probability
 
\begin_inset Formula $p$
\end_inset

 and the second with probability 
\begin_inset Formula $1-p$
\end_inset

.
 However, we don't know from which Normal it was drawn.
 The parameters are 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu_{0},\sigma_{0},\mu_{1},\sigma_{1},p\right)$
\end_inset

.
\end_layout

\begin_layout Example
The incomplete-data likelihood is given by
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta}|\mathbf{y}\right)=\prod_{i=1}^{n}\left[\left(1-p\right)\phi\left(y_{i}|\mu_{0},\sigma_{0}\right)+p\phi\left(y_{i}|\mu_{1},\sigma_{1}\right)\right],
\]

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\[
\ln L\left(\boldsymbol{\theta}|\mathbf{y}\right)=\sum_{i=1}^{n}\log\left[\left(1-p\right)\phi\left(y_{i}|\mu_{0},\sigma_{0}\right)+p\phi\left(y_{i}|\mu_{1},\sigma_{1}\right)\right].
\]

\end_inset

The sum inside the logarithm makes this difficult to maximize.
 Instead, we will define an indicator variable 
\begin_inset Formula $Z_{i}$
\end_inset

 as
\begin_inset Formula 
\[
Z_{i}=\begin{cases}
0, & \mbox{if }Y_{i}\sim N\left(\mu_{0},\sigma_{0}^{2}\right)\\
1, & \mbox{if }Y_{i}\sim N\left(\mu_{1},\sigma_{1}^{2}\right)
\end{cases}.
\]

\end_inset

Then, we can write the joint density of 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 conditioned on 
\begin_inset Formula $\mathbf{Z}$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{Y}}\left(\mathbf{y}|\theta,z_{i}=j\right) & =\phi\left(y_{i}|\mu_{j},\sigma_{j}\right)\\
 & =\frac{1}{\sqrt{2\pi\sigma_{j}^{2}}}e^{-\left(y_{i}-\mu_{j}\right)^{2}/\left(2\sigma_{j}^{2}\right)}.
\end{flalign*}

\end_inset

Recall that the probability of the intersection of two events 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is equal to the probability of 
\begin_inset Formula $A$
\end_inset

 given 
\begin_inset Formula $B$
\end_inset

 multiplied by the probability of 
\begin_inset Formula $B$
\end_inset

, i.e., 
\begin_inset Formula 
\[
P\left(\left\{ A\cap B\right\} \right)=P\left(\left\{ A|B\right\} \right)P\left(\left\{ B\right\} \right).
\]

\end_inset

Then, abusing notation slightly, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y_{i}=y_{i}\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =P\left(\left\{ Y_{i}=y_{i}|Z_{i}=z_{i}\right\} \right)P\left(\left\{ Z_{i}=z_{i}\right\} \right),
\end{flalign*}

\end_inset

i.e., the joint density of 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Z}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{Y},\mathbf{Z}}\left(y_{i},z_{i}=j\right) & =f_{\mathbf{Y}}\left(y_{i}|z_{i}=j,\theta\right)P\left(\left\{ Z_{i}=j\right\} \right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $P\left(\left\{ Z_{i}=1\right\} \right)=p$
\end_inset

 and 
\begin_inset Formula $P\left(\left\{ Z_{i}=0\right\} \right)=1-p$
\end_inset

.
 Then, the complete-data likelihood is given by
\begin_inset Formula 
\begin{flalign*}
L\left(\boldsymbol{\theta}|\mathbf{y},\mathbf{z}\right) & =\prod_{i=1}^{n}f_{\mathbf{Y},\mathbf{Z}}\left(y_{i},z_{i}|\boldsymbol{\theta}\right)\\
 & =\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2\pi\sigma_{j}^{2}}}e^{-\left(y_{i}-\mu_{j}\right)^{2}/\left(2\sigma_{j}^{2}\right)}\cdot P\left(\left\{ Z_{i}=j\right\} \right)\right]\\
 & =\prod_{i=1}^{n}\left[\left[\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\cdot\left(1-p\right)\right]^{1-z_{i}}\cdot\left[\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\cdot p\right]^{z_{i}}\right],
\end{flalign*}

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\ln L\left(\boldsymbol{\theta}|\mathbf{y},\mathbf{z}\right) & =\ln\prod_{i=1}^{n}\left[\left[\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\cdot\left(1-p\right)\right]^{1-z_{i}}\cdot\left[\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\cdot p\right]^{z_{i}}\right]\\
 & =\sum_{i=1}^{n}\left\{ \left(1-z_{i}\right)\ln\left[\frac{1-p}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\right]+z_{i}\ln\left[\frac{p}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\right]\right\} .
\end{flalign*}

\end_inset

We will now find the expected value of the complete-data log likelihood
 (the E-step).
\begin_inset Formula 
\begin{flalign*}
J\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{\left(j\right)}\right) & =\mbox{E}\left[\ln L\left(\boldsymbol{\theta}|\mathbf{y},\mathbf{z}\right)|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]\\
 & =\sum_{i=1}^{n}\left\{ \mbox{E}\left[1-z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]\ln\left[\frac{1-p}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\right]+\mbox{E}\left[z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]\ln\left[\frac{p}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\right]\right\} 
\end{flalign*}

\end_inset

We will evaluate 
\begin_inset Formula $\mbox{E}\left[z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]=P\left(\left\{ z_{i}=1|y_{i},\boldsymbol{\theta}^{\left(j\right)}\right\} \right)$
\end_inset

 using Bayes' Rule.
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right] & =P\left(\left\{ z_{i}=1|y_{i},\boldsymbol{\theta}^{\left(j\right)}\right\} \right)\\
 & =\frac{f\left(y_{i}|z_{i}=1,\boldsymbol{\theta}^{\left(j\right)}\right)P\left(\left\{ Z_{i}=1\right\} \right)}{\sum_{k=0}^{1}f\left(y_{i}|Z_{i}=k,\boldsymbol{\theta}^{\left(j\right)}\right)P\left(\left\{ Z_{i}=k\right\} \right)}\\
 & =\frac{\phi\left(y_{i}|\mu_{1}^{\left(j\right)},\sigma_{1}^{\left(j\right)}\right)\cdot p^{\left(j\right)}}{p^{\left(j\right)}\phi\left(y_{i}|\mu_{1}^{\left(j\right)},\sigma_{1}^{\left(j\right)}\right)+\left(1-p^{\left(j\right)}\right)\phi\left(y_{i}|\mu_{0}^{\left(j\right)},\sigma_{0}^{\left(j\right)}\right)}\\
 & =\tau_{i}^{\left(j\right)}
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
J\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{\left(j\right)}\right) & =\sum_{i=1}^{n}\left\{ \left(1-\tau_{i}^{\left(j\right)}\right)\left[\ln\left(\frac{1-p}{\sqrt{2\pi\sigma_{0}^{2}}}\right)+\ln\left(e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\right)\right]\right.\\
 & \qquad\left.+\tau_{i}^{\left(j\right)}\left[\ln\frac{p}{\sqrt{2\pi\sigma_{1}^{2}}}+\ln\left(e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\right)\right]\right\} \\
 & =\sum_{i=1}^{n}\left\{ \left(1-\tau_{i}^{\left(j\right)}\right)\left[\ln\left(1-p\right)-\ln\left(2\pi\sigma_{0}^{2}\right)^{1/2}-\frac{\left(y_{i}-\mu_{0}\right)^{2}}{2\sigma_{0}^{2}}\right]+\tau_{i}^{\left(j\right)}\left[\ln p-\ln\left(2\pi\sigma_{1}^{2}\right)^{1/2}-\frac{\left(y_{i}-\mu_{1}\right)^{2}}{2\sigma_{1}^{2}}\right]\right\} \\
 & =\sum_{i=1}^{n}\left\{ \left(1-\tau_{i}^{\left(j\right)}\right)\left[\ln\left(1-p\right)-\frac{1}{2}\ln2\pi\sigma_{0}^{2}-\frac{\left(y_{i}-\mu_{0}\right)^{2}}{2\sigma_{0}^{2}}\right]+\tau_{i}^{\left(j\right)}\left[\ln p-\frac{1}{2}\ln2\pi\sigma_{1}^{2}-\frac{\left(y_{i}-\mu_{1}\right)^{2}}{2\sigma_{1}^{2}}\right]\right\} .
\end{flalign*}

\end_inset

We will take the derivative with respect to 
\begin_inset Formula $p$
\end_inset

 (the M-step).
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial p}J\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{\left(j\right)}\right) & =\sum_{i=1}^{n}\left\{ -\left(1-\tau_{i}^{\left(j\right)}\right)\frac{1}{1-p}+\tau_{i}^{\left(j\right)}\frac{1}{p}\right\} 
\end{flalign*}

\end_inset

(Although 
\begin_inset Formula $p$
\end_inset

 is incorporated in 
\begin_inset Formula $\tau_{i}^{\left(j\right)}$
\end_inset

, it is considered to have been evaluated there, so that 
\begin_inset Formula $\tau_{i}^{\left(j\right)}$
\end_inset

 is treated as constant when taking the deriative of 
\begin_inset Formula $J$
\end_inset

 with respect to 
\begin_inset Formula $p$
\end_inset

.) Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}\frac{\left(1-\tau_{i}^{\left(j\right)}\right)}{1-p} & =\sum_{i=1}^{n}\frac{\tau_{i}^{\left(j\right)}}{p}\\
\Leftrightarrow\frac{1}{1-p}\sum_{i=1}^{n}\left(1-\tau_{i}^{\left(j\right)}\right) & =\frac{1}{p}\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}\\
\Leftrightarrow\left(1-p\right)\sum_{i=1}^{n}\tau_{i}^{\left(j\right)} & =p\sum_{i=1}^{n}\left(1-\tau_{i}^{\left(j\right)}\right)\\
\Leftrightarrow\frac{1-p}{p} & =\frac{\sum_{i=1}^{n}\left(1-\tau_{i}^{\left(j\right)}\right)}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}\\
\Leftrightarrow\frac{1}{p}-1 & =\frac{n-\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}\\
\Leftrightarrow\frac{1}{p}-1 & =\frac{n}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}-1\\
\Leftrightarrow\frac{1}{p} & =\frac{n}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}\\
\Leftrightarrow p^{\left(j+1\right)} & =\frac{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}{n}.
\end{flalign*}

\end_inset

We can proceed similarly to find the other parameter estimates, i.e., by taking
 the derivative of 
\begin_inset Formula $J$
\end_inset

 with respect to each parameter, setting the result equal to zero, and solving
 for the parameter.
\end_layout

\begin_layout Standard
We have introduced four methods for finding MLEs.
 In order of approximate difficulty, the methods are
\end_layout

\begin_layout Enumerate
Set the derivative of the log-likelihood equal to zero and solve or 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 This is the simplest method when a closed-form solution is possible.
\end_layout

\begin_layout Enumerate
When the support depends on the parameter 
\begin_inset Formula $\theta$
\end_inset

, plot the likelihood function and determine where it attains its maximum.
\end_layout

\begin_layout Enumerate
Use the Newton-Raphson method to estimate the MLE.
\end_layout

\begin_layout Enumerate
Use the EM method to find the expectation of the complete log-likelihood
 using the observed 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and an initial estimate for 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
Introduce a latent variable 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find the complete log-likelihood 
\begin_inset Formula $L\left(\theta|\mathbf{y},\mathbf{z}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Use the E-step to estimate 
\begin_inset Formula $z^{\left(j\right)}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Use the M-step to maximize 
\begin_inset Formula $J\left(\theta|\theta^{\left(j\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection
Bayes estimators
\end_layout

\begin_layout Standard
In the classical (frequentist) approach, 
\begin_inset Formula $\theta$
\end_inset

 is considered to be unknown, but fixed.
 in the Bayesian approach, 
\begin_inset Formula $\theta$
\end_inset

 is treated as a random variable.
 A prior distribution for 
\begin_inset Formula $\theta$
\end_inset

, 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

, is specified.
 This may be non-informative or may describe expert opinion (or subjective
 belief).
 Inference is based on the posterior distribution, 
\begin_inset Formula 
\[
\pi\left(\theta|\mathbf{x}\right)=\frac{f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)}{\int f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)\mbox{d}\theta},
\]

\end_inset

which is obtained from Bayes' rule and updates the prior, 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

, given the observed data.
 The expectation of 
\begin_inset Formula $\pi\left(\theta|\mathbf{x}\right)$
\end_inset

, which is given by
\begin_inset Formula 
\[
\mbox{E}\left[\pi\left(\theta|\mathbf{x}\right)\right]=\int_{\Theta}\theta\cdot\pi\left(\theta|\mathbf{x}\right)\mbox{d}\theta,
\]

\end_inset

where 
\begin_inset Formula $\Theta$
\end_inset

 is the support of 
\begin_inset Formula $\theta$
\end_inset

, i.e., the parameter space, is called the posterior mean or the 
\shape italic
Bayes estimator
\shape default
.
 One could also use the most probable value of 
\begin_inset Formula $\theta$
\end_inset

, the 
\shape italic
posterior mode
\shape default
, as a point estimator.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mbox{Bernoulli}\left(\theta\right)$
\end_inset

 and consider 
\begin_inset Formula $\theta\sim U\left(0,1\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
Find the Bayes estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
The prior distribution is uniform on the interval 
\begin_inset Formula $\left(0,1\right)$
\end_inset

, so its density is given by
\begin_inset Formula 
\[
\pi\left(\theta\right)=\frac{1}{1-0}=1.
\]

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}},
\]

\end_inset

so that the posterior distribution is given by
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)}{\int p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)\mbox{d}\theta}\\
 & =\frac{\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\cdot1}{\int_{0}^{1}\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\cdot1\mbox{d}\theta}\\
 & =\frac{\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}}{\int_{0}^{1}\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\mbox{d}\theta}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $\alpha=\sum_{i=1}^{n}x_{i}+1$
\end_inset

 and let 
\begin_inset Formula $\beta=n-\sum_{i=1}^{n}x_{i}+1$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{\int_{0}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\mbox{d}\theta}.
\end{flalign*}

\end_inset

We recognize the integrand as the kernel of a 
\begin_inset Formula $\mbox{Beta}\left(\alpha,\beta\right)$
\end_inset

 distribution, so we write
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{B\left(\alpha,\beta\right)\int_{0}^{1}\frac{1}{B\left(\alpha,\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\mbox{d}\theta},
\end{flalign*}

\end_inset

where 
\begin_inset Formula $B\left(\alpha,\beta\right)=\Gamma\left(\alpha+\beta\right)/\left(\Gamma\left(\alpha\right)\Gamma\left(\beta\right)\right)$
\end_inset

.
 Then, the integrand is the pdf of a 
\begin_inset Formula $\mbox{Beta}\left(\alpha,\beta\right)$
\end_inset

 distribution, which integrates to 1, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{B\left(\alpha,\beta\right)\cdot1}\\
 & =\frac{1}{B\left(\alpha,\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1},
\end{flalign*}

\end_inset

which is the pdf of a 
\begin_inset Formula $\mbox{Beta}\left(\alpha,\beta\right)$
\end_inset

 random variable.
 It follows that 
\begin_inset Formula 
\[
\pi\left(\theta|\mathbf{x}\right)\sim\mbox{Beta}\left(\sum_{i=1}^{n}x_{i}+1,n-\sum_{i=1}^{n}x_{i}+1\right).
\]

\end_inset

The expected value (mean) of a 
\begin_inset Formula $\mbox{Beta}\left(\gamma,\psi\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\gamma/\left(\gamma+\psi\right)$
\end_inset

, so it follows that 
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[\pi\left(\theta|\mathbf{x}\right)\right] & =\frac{\alpha}{\alpha+\beta}\\
 & =\frac{\sum_{i=1}^{n}x_{i}+1}{\left(\sum_{i=1}^{n}x_{i}+1\right)+\left(n-\sum_{i=1}^{n}x_{i}+1\right)}\\
 & =\frac{\sum_{i=1}^{n}x_{i}+1}{n+2}
\end{flalign*}

\end_inset

is the Bayes estimator (posterior mean) for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Show that the Bayes estimator is a weighted average of the MLE and the prior
 mean.
\end_layout

\begin_deeper
\begin_layout Standard
The log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\ln L\left(\theta|\mathbf{x}\right) & =\ln\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\\
 & =\ln\theta^{\sum_{i=1}^{n}x_{i}}+\ln\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\\
 & =\left(\sum_{i=1}^{n}x_{i}\right)\ln\theta+\left(n-\sum_{i=1}^{n}x_{i}\right)\ln\left(1-\theta\right).
\end{flalign*}

\end_inset

We will take the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\ln L\left(\theta|\mathbf{x}\right) & =\frac{\partial}{\partial\theta}\left[\left(\sum_{i=1}^{n}x_{i}\right)\ln\theta+\left(n-\sum_{i=1}^{n}x_{i}\right)\ln\left(1-\theta\right)\right]\\
 & =\frac{1}{\theta}\sum_{i=1}^{n}x_{i}+\left(n-\sum_{i=1}^{n}x_{i}\right)\frac{1}{1-\theta}\cdot-1\\
 & =\frac{1}{\theta}\sum_{i=1}^{n}x_{i}-\frac{1}{1-\theta}\left(n-\sum_{i=1}^{n}x_{i}\right)
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{1}{\theta}\sum_{i=1}^{n}x_{i} & =\frac{1}{1-\theta}\left(n-\sum_{i=1}^{n}x_{i}\right)\\
\Leftrightarrow\theta\left(n-\sum_{i=1}^{n}x_{i}\right) & =\left(1-\theta\right)\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta\left(n-\sum_{i=1}^{n}x_{i}\right) & =\sum_{i=1}^{n}x_{i}-\theta\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta\left(n-\sum_{i=1}^{n}x_{i}\right)+\theta\sum_{i=1}^{n}x_{i} & =\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta\left[\left(n-\sum_{i=1}^{n}x_{i}\right)+\sum_{i=1}^{n}x_{i}\right] & =\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta n & =\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\hat{\theta} & =\bar{x}.
\end{flalign*}

\end_inset

The prior mean is given by 
\begin_inset Formula 
\[
\mbox{E}\left[\theta\right]=\frac{0+1}{2}=\frac{1}{2}.
\]

\end_inset

Suppose that 
\begin_inset Formula $a$
\end_inset

 is the weight of the MLE and 
\begin_inset Formula $1-a$
\end_inset

 is the weight of the prior mean, so that we have
\begin_inset Formula 
\begin{flalign*}
a\bar{x}+\left(1-a\right)\frac{1}{2} & =\frac{n\bar{x}+1}{n+2}\\
 & =\frac{n}{n+2}\bar{x}+\frac{1}{n+2}\\
 & =\frac{n}{n+2}\bar{x}+\left(\frac{1}{n+2}\right)\left(\frac{2}{2}\right)\\
 & =\frac{n}{n+2}\bar{x}+\left(\frac{2+n-n}{n+2}\right)\frac{1}{2}\\
 & =\frac{n}{n+2}\bar{x}+\left(1-\frac{n}{n+2}\right)\frac{1}{2}
\end{flalign*}

\end_inset

Then, it follows that 
\begin_inset Formula 
\begin{flalign*}
a & =\frac{n}{n+2}\quad\mbox{and}\quad1-a=\frac{2}{n+2}.
\end{flalign*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout --Separator--

\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are iid 
\begin_inset Formula $\mbox{Poisson}\left(\lambda\right)$
\end_inset

, i.e.,
\begin_inset Formula 
\[
f\left(\mathbf{x}|\lambda\right)=\frac{\lambda^{\sum_{i=1}^{n}x_{i}}e^{-n\lambda}}{\prod_{i=1}^{n}x_{i}!}.
\]

\end_inset

The investigator decides to use a prior distribution that captures his prior
 opinion using a gamma density (conjugate prior for Poisson) with mean of
 15 and standard deviation of 5.
 Find the posterior mean (Bayes estimator) of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
The posterior distribution is given by
\begin_inset Formula 
\begin{flalign*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\lambda\right)\pi\left(\lambda\right)}{\int f\left(\mathbf{x}|\lambda\right)\pi\left(\lambda\right)\mbox{d}\lambda}\\
 & =\frac{\left[\frac{\lambda^{\sum_{i=1}^{n}x_{i}}e^{-n\lambda}}{\prod_{i=1}^{n}x_{i}!}\right]\left[\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}\right]}{\int_{0}^{\infty}\left[\frac{\lambda^{\sum_{i=1}^{n}x_{i}}e^{-n\lambda}}{\prod_{i=1}^{n}x_{i}!}\right]\left[\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}e^{-\lambda/\beta}\right]\mbox{d}\lambda}\\
 & =\frac{\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}e^{-n\lambda-\lambda/\beta}}{\int_{0}^{\infty}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}e^{-n\lambda-\lambda/\beta}\mbox{d}\lambda}\\
 & =\frac{\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}e^{-\lambda\left(n+1/\beta\right)}}{\int_{0}^{\infty}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}e^{-\lambda\left(n+1/\beta\right)}\mbox{d}\lambda}\\
 & =\frac{\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}e^{-\lambda/\left(n+1/\beta\right)^{-1}}}{\int_{0}^{\infty}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}e^{-\lambda/\left(n+1/\beta\right)^{-1}}\mbox{d}\lambda}
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $\alpha'=\sum_{i=1}^{n}x_{i}+\alpha$
\end_inset

 and let 
\begin_inset Formula $\beta'=1/\left(n+1/\beta\right)$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{\lambda^{\alpha'-1}e^{-\lambda/\beta'}}{\int_{0}^{\infty}\lambda^{\alpha'-1}e^{-\lambda/\beta'}\mbox{d}\lambda}.
\end{flalign*}

\end_inset

We recognize the integrand as the kernel of a 
\begin_inset Formula $\Gamma\left(\alpha',\beta'\right)$
\end_inset

 distribution, so we write
\begin_inset Formula 
\[
\pi\left(\lambda|\mathbf{x}\right)=\frac{\lambda^{\alpha'-1}e^{-\lambda/\beta'}}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}\int_{0}^{\infty}\frac{1}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}}\lambda^{\alpha'-1}e^{-\lambda/\beta'}\mbox{d}\lambda}.
\]

\end_inset

Then, the integrand is the pdf of a 
\begin_inset Formula $\Gamma\left(\alpha',\beta'\right)$
\end_inset

 distribution, which integrates to 1, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{\lambda^{\alpha'-1}e^{-\lambda/\beta'}}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}\cdot1}\\
 & =\frac{1}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}}\lambda^{\alpha'-1}e^{-\lambda/\beta'},
\end{flalign*}

\end_inset

which is the pdf of a 
\begin_inset Formula $\Gamma\left(\alpha',\beta'\right)$
\end_inset

 random variable.
 It follows that 
\begin_inset Formula $\pi\left(\lambda|\mathbf{x}\right)\sim\Gamma\left(\sum_{i=1}^{n}x_{i}+\alpha,1/\left(n+1/\beta\right)\right)$
\end_inset

.
 The expected value (mean) of a 
\begin_inset Formula $\Gamma\left(\gamma,\psi\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\gamma\psi$
\end_inset

, so it follows that
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[\pi\left(\lambda|\mathbf{x}\right)\right] & =\left(\sum_{i=1}^{n}x_{i}+\alpha\right)\left(\frac{1}{n+\frac{1}{\beta}}\right)\\
 & =\frac{\sum_{i=1}^{n}x_{i}+\alpha}{n+\frac{1}{\beta}}
\end{flalign*}

\end_inset

is the Bayes estimator of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\mathcal{F}$
\end_inset

 denote the class of pdfs or pmfs 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 (indexed by 
\begin_inset Formula $\theta$
\end_inset

).
 A class 
\begin_inset Formula $\Pi$
\end_inset

 of prior distributions is a 
\shape italic
conjugate family
\shape default
 for 
\begin_inset Formula $\mathcal{F}$
\end_inset

 if the posterior distribution is in the class 
\begin_inset Formula $\Pi$
\end_inset

 for all 
\begin_inset Formula $f\in\mathcal{F}$
\end_inset

, all priors in 
\begin_inset Formula $\Pi$
\end_inset

, and all 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

, i.e., if 
\begin_inset Formula 
\[
\pi\left(\theta\right)\in\Pi\Rightarrow f\left(\theta|x\right)\in\Pi.
\]

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Standard
Sampling models from exponential families all have conjugate priors, including
 the following examples.
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sampling model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Prior & Posterior
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mbox{Binomial}\left(n,\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Beta
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mbox{Poisson}\left(\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gamma
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mbox{Exponential}\left(\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gamma
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Without a conjugate prior, it is not usually possible to obtain a closed-form
 solution for a posterior distribution.
\end_layout

\begin_layout Standard
Contemporary computational resources have had an enormous impact on Bayesian
 inference.
 The computationally difficult part of Bayesian inference is the calculation
 of the normalizing constant that makes the posterior density integrate
 to 1.
 Traditionally, such calculations were performed analytically, often using
 conjugate priors so that the integrations could be done explicitly.
 For more complex problems, Markov chain Monte Carlo methods are used to
 sample from the posterior distribution.
\end_layout

\begin_layout Section
Methods of evaluating estimators
\end_layout

\begin_layout Standard
In the previous section, we discussed techniques for finding point estimators
 of parameters.
 Since we can usually apply more than one of these methods, we are often
 faced with the task of choosing between estimators.
 What qualities should a 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 estimator have? Is it possible to find a 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\hat{\theta}$
\end_inset

?
\end_layout

\begin_layout Definition
The 
\shape italic
bias
\shape default
 of a point estimator 
\begin_inset Formula $W$
\end_inset

 of a parameter 
\begin_inset Formula $\theta$
\end_inset

 is the difference between the expected value of 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

; that is, 
\begin_inset Formula $\mbox{Bias}_{\theta}W=\mbox{E}_{\theta}\left[W\right]-\theta$
\end_inset

.
 An estimator whose bias is identically (in 
\begin_inset Formula $\theta$
\end_inset

) equal to zero is called 
\shape italic
unbiased
\shape default
 and satisfied 
\begin_inset Formula $\mbox{E}_{\theta}\left[W\right]=\theta$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 be a random sample from the pdf 
\begin_inset Formula 
\[
f\left(y|\theta\right)=\frac{2y}{\theta^{2}},\quad0\leq y\leq\theta.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the moment estimator and the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
The first moment of 
\begin_inset Formula $Y_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[Y_{i}\right] & =\int_{0}^{\theta}y\cdot f_{Y_{i}}\left(y|\theta\right)\mbox{d}y\\
 & =\int_{0}^{\theta}y\cdot\frac{2y}{\theta^{2}}\mbox{d}y\\
 & =\frac{2}{3\theta^{2}}y^{3}\Big\rvert_{0}^{\theta}\\
 & =\frac{2\theta^{3}}{3\theta^{2}}-0\\
 & =\frac{2}{3}\theta,
\end{flalign*}

\end_inset

so that we have 
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\frac{\sum_{i=1}^{n}y_{i}}{n}\\
\Leftrightarrow\frac{2}{3}\hat{\theta} & =\bar{y}\\
\Leftrightarrow\hat{\theta}_{MOM} & =\frac{3}{2}\bar{y}.
\end{flalign*}

\end_inset

The support depends on the parameter 
\begin_inset Formula $\theta$
\end_inset

, so we can write the pdf as 
\begin_inset Formula 
\begin{flalign*}
f_{Y}\left(y|\theta\right) & =\frac{2y}{\theta^{2}}I_{\left\{ 0\leq y\leq\theta\right\} }.
\end{flalign*}

\end_inset

Then, the likelihood function is given by
\begin_inset Formula 
\begin{flalign*}
L\left(\theta|\mathbf{y}\right) & =\prod_{i=1}^{n}f_{Y_{i}}\left(y_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}\frac{2y_{i}}{\theta^{2}}I_{\left\{ 0\leq y_{i}\leq\theta\right\} }\\
 & =\frac{2^{n}\prod_{i=1}^{n}y_{i}}{\theta^{2n}}I_{\left\{ 0\leq y_{\left(n\right)}\leq\theta\right\} },
\end{flalign*}

\end_inset

which we can write as 
\begin_inset Formula 
\[
L\left(\theta|\mathbf{y}\right)=\begin{cases}
\dfrac{2^{n}\prod_{i=1}^{n}y_{i}}{\theta^{2n}}, & \theta\geq y_{\left(n\right)}\\
0, & \theta<y_{\left(n\right)}
\end{cases},
\]

\end_inset

and whose graph is shown below.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(2, 4, length=2000) 
\end_layout

\begin_layout Plain Layout

plot(z, 2^4/(z^4), type="l", xlab = expression(theta), ylab=expression(paste(L,
 "(",theta,"|",bold(y),")")), yaxt="n", xaxt="n", xlim = c(1, 4), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at=2, labels=expression(y[(n)]), cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clearly, the likelihood is maximized at 
\begin_inset Formula $Y_{\left(n\right)}$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}_{MLE}=Y_{\left(n\right)}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Is either estimator unbiased?
\end_layout

\begin_deeper
\begin_layout Standard
The expected value of 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[\hat{\theta}_{MOM}\right] & =\mbox{E}\left[\frac{3}{2}\bar{Y}\right]\\
 & =\frac{3}{2}\mbox{E}\left[\bar{Y}\right]\\
 & =\frac{3}{2}\mbox{E}\left[\frac{1}{n}\sum_{i=1}^{n}Y_{i}\right]\\
 & =\frac{3}{2n}\sum_{i=1}^{n}\mbox{E}\left[Y_{i}\right]\\
 & =\frac{3}{2n}\sum_{i=1}^{n}\frac{2}{3}\theta\\
 & =\frac{3}{2n}\left(n\cdot\frac{2}{3}\theta\right)\\
 & =\theta,
\end{flalign*}

\end_inset

so it follows that 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is unbiased for 
\begin_inset Formula $\theta$
\end_inset

.
 The cdf of 
\begin_inset Formula $Y$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
F_{Y}\left(y\right) & =\int_{0}^{y}f_{Y}\left(y|\theta\right)\mbox{d}t\\
 & =\int_{0}^{y}\frac{2t}{\theta^{2}}\mbox{d}t\\
 & =\frac{t^{2}}{\theta^{2}}\Bigr\rvert_{0}^{y}\\
 & =\frac{y^{2}}{\theta^{2}}-0\\
 & =\frac{y^{2}}{\theta^{2}},
\end{flalign*}

\end_inset

so that the pdf of 
\begin_inset Formula $\hat{\theta}_{MLE}=Y_{\left(n\right)}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f_{Y_{\left(n\right)}}\left(y|\theta\right) & =n\left[F_{y}\left(y\right)\right]^{n-1}f_{Y}\left(y\right)\\
 & =n\left(\frac{y^{2}}{\theta^{2}}\right)^{n-1}\left(\frac{2y}{\theta^{2}}\right)\\
 & =n\frac{y^{2\left(n-1\right)}}{\theta^{2\left(n-1\right)}}\left(\frac{2y}{\theta^{2}}\right)\\
 & =\frac{2ny^{2n-1}}{\theta^{2n}}.
\end{flalign*}

\end_inset

Then, the expected value of 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[\hat{\theta}_{MLE}\right] & =\mbox{E}\left[Y_{\left(n\right)}\right]\\
 & =\int_{0}^{\theta}y\cdot f_{Y_{\left(n\right)}}\left(y\right)\mbox{d}y\\
 & =\int_{0}^{\theta}y\cdot\frac{2ny^{2n-1}}{\theta^{2n}}\mbox{d}y\\
 & =\frac{2n}{\theta^{2n}}\int_{0}^{\theta}y^{2n}\mbox{d}y\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{1}{2n+1}y^{2n+1}\right|_{0}^{\theta}\\
 & =\frac{2n}{\theta^{2n}}\left(\frac{\theta^{2n+1}}{2n+1}-0\right)\\
 & =\frac{2n}{2n+1}\theta.
\end{flalign*}

\end_inset

We see that 
\begin_inset Formula $\mbox{E}\left[\hat{\theta}_{MLE}\right]$
\end_inset

 is not equal to 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

 is a biased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Construct an unbiased estimator based on the MLE.
\end_layout

\begin_deeper
\begin_layout Standard
An unbiased estimator 
\begin_inset Formula $\tilde{\lambda}$
\end_inset

 for 
\begin_inset Formula $\lambda$
\end_inset

 satisfies the condition that 
\begin_inset Formula $\mbox{E}\left[\tilde{\lambda}\right]=\lambda$
\end_inset

 for all 
\begin_inset Formula $\lambda$
\end_inset

.
 So, to construct an unbiased estimator 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 based on the MLE 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

, we need to find a constant such that 
\begin_inset Formula $\mbox{E}\left[\hat{\theta}_{MLE}\right]=\theta$
\end_inset

.
 We see that
\begin_inset Formula 
\[
\tilde{\theta}=\frac{2n+1}{2n}\hat{\theta}_{MLE}=\frac{2n+1}{2n}Y_{\left(n\right)}
\]

\end_inset

satisfies the condition that 
\begin_inset Formula 
\[
\mbox{E}\left[\tilde{\theta}\right]=\mbox{E}\left[\frac{2n+1}{2n}Y_{\left(n\right)}\right]=\frac{2n+1}{2n}\mbox{E}\left[Y_{\left(n\right)}\right]=\frac{2n+1}{2n}\left(\frac{2n}{2n+1}\theta\right)=\theta,
\]

\end_inset

so it follows that 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is an unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

 based on the MLE.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
The precision of an estimator is another important property beside unbiasedness.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 be two unbiased estimators for a parameter 
\begin_inset Formula $\theta$
\end_inset

.
 If 
\begin_inset Formula $\mbox{Var}\left(\hat{\theta}_{1}\right)<\mbox{Var}\left(\hat{\theta}_{2}\right)$
\end_inset

, we say that 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 is 
\shape italic
more efficient
\shape default
 than 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

.
 The relative efficiency of 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 with respect to 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 is the ratio 
\begin_inset Formula $\mbox{Var}\left(\hat{\theta}_{2}\right)/\mbox{Var}\left(\hat{\theta}_{1}\right)$
\end_inset

.
\end_layout

\begin_layout Example
Consider the two unbiased estimators from the example above.
 Which estimator is more efficient?
\end_layout

\begin_layout Example
The variance of 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(\hat{\theta}_{MOM}\right) & =\mbox{Var}\left(\frac{3}{2}\bar{Y}\right)\\
 & =\mbox{Var}\left(\frac{3}{2n}\sum_{i=1}^{n}Y_{i}\right)\\
 & =\frac{9}{4n^{2}}\mbox{Var}\left(\sum_{i=1}^{n}Y_{i}\right)\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\mbox{Var}\left(Y_{i}\right)
\end{flalign*}

\end_inset

The final equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:variance-of-ind-rvs"

\end_inset

, i.e., the variance of the sum of independent random variables is equal to
 the sum of their variances.
 To find the variance of 
\begin_inset Formula $Y_{i}$
\end_inset

, we will need to find its second moment.
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[Y^{2}\right] & =\int_{0}^{\theta}y^{2}\cdot f_{Y}\left(y\right)\mbox{d}y\\
 & =\int_{0}^{\theta}y^{2}\frac{2y}{\theta^{2}}\mbox{d}y\\
 & =\frac{1}{2\theta^{2}}y^{4}\Big\rvert_{0}^{\theta}\\
 & =\frac{\theta^{4}}{2\theta^{2}}-0\\
 & =\frac{\theta^{2}}{2}
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(\hat{\theta}_{MOM}\right) & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\mbox{Var}\left(Y_{i}\right)\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\mbox{E}\left[Y^{2}\right]-\left(\mbox{E}\left[Y\right]\right)^{2}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\frac{\theta^{2}}{2}-\left(\frac{2}{3}\theta\right)^{2}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\frac{\theta^{2}}{2}-\frac{4\theta^{2}}{9}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\frac{9\theta^{2}}{18}-\frac{8\theta^{2}}{18}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\frac{\theta^{2}}{18}\\
 & =\frac{9}{4n^{2}}\cdot n\frac{\theta^{2}}{18}\\
 & =\frac{\theta^{2}}{8n}.
\end{flalign*}

\end_inset

The variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(\tilde{\theta}\right) & =\mbox{Var}\left(\frac{2n+1}{2n}\hat{\theta}_{MLE}\right)\\
 & =\left(\frac{2n+1}{2n}\right)^{2}\mbox{Var}\left(\hat{\theta}_{MLE}\right)\\
 & =\frac{\left(2n+1\right)^{2}}{4n^{2}}\mbox{Var}\left(\hat{\theta}_{MLE}\right).
\end{flalign*}

\end_inset

We will now find the second moment of 
\begin_inset Formula $\hat{\theta}_{MLE}=Y_{\left(n\right)}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[Y_{\left(n\right)}^{2}\right] & =\int_{0}^{\theta}y^{2}\cdot f_{Y_{\left(n\right)}}\left(y\right)\mbox{d}y\\
 & =\int_{0}^{\theta}y^{2}\cdot\frac{2ny^{2n-1}}{\theta^{2n}}\mbox{d}y\\
 & =\frac{2n}{\theta^{2n}}\int_{0}^{\theta}y^{2n+1}\mbox{d}y\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{1}{2n+2}y^{2n+2}\right|_{0}^{\theta}\\
 & =\frac{2n}{\theta^{2n}}\left(\frac{\theta^{2n+2}}{2n+2}-0\right)\\
 & =\frac{2n\theta^{2}}{2n+2}\\
 & =\frac{n\theta^{2}}{n+1}
\end{flalign*}

\end_inset

Then, the variance of 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(\hat{\theta}_{MLE}\right) & =\mbox{Var}\left(Y_{\left(n\right)}\right)\\
 & =\mbox{E}\left[Y_{\left(n\right)}^{2}\right]-\left(\mbox{E}\left[Y_{\left(n\right)}\right]\right)^{2}\\
 & =\frac{n\theta^{2}}{n+1}-\left(\frac{2n\theta}{2n+1}\right)^{2}\\
 & =\frac{n\theta^{2}}{n+1}-\frac{4n^{2}\theta^{2}}{\left(2n+1\right)^{2}}\\
 & =\frac{\theta^{2}\left[n\left(2n+1\right)^{2}-4n^{2}\left(n+1\right)\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{\theta^{2}\left[n\left(4n^{2}+4n+1\right)-4n^{3}-4n^{2}\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{\theta^{2}\left[4n^{3}+4n^{2}+n-4n^{3}-4n^{2}\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{n\theta^{2}}{\left(n+1\right)\left(2n+1\right)^{2}},
\end{flalign*}

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(\tilde{\theta}\right) & =\frac{\left(2n+1\right)^{2}}{4n^{2}}\mbox{Var}\left(\hat{\theta}_{MLE}\right)\\
 & =\frac{\left(2n+1\right)^{2}}{4n^{2}}\left(\frac{n\theta^{2}}{\left(n+1\right)\left(2n+1\right)^{2}}\right)\\
 & =\frac{\theta^{2}}{4n\left(n+1\right)}.
\end{flalign*}

\end_inset

If 
\begin_inset Formula $\mbox{Var}\left(\hat{\theta}_{MOM}\right)<\mbox{Var}\left(\tilde{\theta}\right)$
\end_inset

, then 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is more efficient than 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(\hat{\theta}_{MOM}\right) & <\mbox{Var}\left(\tilde{\theta}\right)\\
\Leftrightarrow\frac{\theta^{2}}{8n} & <\frac{\theta^{2}}{4n\left(n+1\right)}\\
\Leftrightarrow8n & >4n\left(n+1\right)\\
\Leftrightarrow8n-4n\left(n+1\right) & >0\\
\Leftrightarrow4n\left(2-\left(n+1\right)\right) & >0\\
\Leftrightarrow4n\left(2-n-1\right) & >0\\
\Leftrightarrow4n\left(1-n\right) & >0\\
\Leftrightarrow4n-4n^{2} & >0
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

, so it follows that 
\begin_inset Formula $4n-4n^{2}<0$
\end_inset

, so it cannot be the case that 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is more efficient than 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 Therefore, 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is more efficient than 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

.
 The relative efficiency of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 with respect to 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is given by 
\begin_inset Formula 
\[
\frac{\mbox{Var}\left(\hat{\theta}_{MOM}\right)}{\mbox{Var}\left(\tilde{\theta}\right)}=\frac{\theta^{2}/\left(8n\right)}{\theta^{2}/\left(4n\left(n+1\right)\right)}=\frac{4n\left(n+1\right)}{8n}=\frac{n+1}{2}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Mean squared error
\end_layout

\begin_layout Definition
The 
\shape italic
mean squared error
\shape default
 (MSE) of an estimator 
\begin_inset Formula $W$
\end_inset

 of a parameter 
\begin_inset Formula $\theta$
\end_inset

 is the function of 
\begin_inset Formula $\theta$
\end_inset

 defined by 
\begin_inset Formula $\mbox{E}_{\theta}\left[\left(W-\theta\right)^{2}\right]$
\end_inset

.
\end_layout

\begin_layout Standard
Any increasing function of 
\begin_inset Formula $\left|W-\theta\right|$
\end_inset

 would serve to measure the goodness of an estimator, but MSE has at least
 two advantages over other distance measures.
 First, it is analytically tractable (absolute value would lead to discontinuiti
es).
 Second, it has the interpretation
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[\left(W-\theta\right)^{2}\right] & =\mbox{E}\left[\left(W-\mbox{E}\left[W\right]+\mbox{E}\left[W\right]-\theta\right)^{2}\right]\\
 & =\mbox{E}\left[\left(W-\mbox{E}\left[W\right]\right)^{2}+2\left(W-\mbox{E}\left[W\right]\right)\left(\mbox{E}\left[W\right]-\theta\right)+\left(\mbox{E}\left[W\right]-\theta\right)^{2}\right]\\
 & =\mbox{E}\left[\left(W-\mbox{E}\left[W\right]\right)^{2}\right]+2\mbox{E}\left[\left(W-\mbox{E}\left[W\right]\right)\left(\mbox{E}\left[W\right]-\theta\right)\right]+\mbox{E}\left[\left(\mbox{E}\left[W\right]-\theta\right)^{2}\right]\\
 & =\mbox{Var}\left(W\right)+2\mbox{E}\left[W\mbox{E}\left[W\right]-\theta W-\left(\mbox{E}\left[W\right]\right)^{2}+\theta\mbox{E}\left[W\right]\right]+\mbox{E}\left[\left(\mbox{Bias}\left(W\right)\right)^{2}\right]\\
 & =\mbox{Var}\left(W\right)+2\left[\mbox{E}\left[W\mbox{E}\left[W\right]\right]-\mbox{E}\left[\theta W\right]-\mbox{E}\left[\left(\mbox{E}\left[W\right]\right)^{2}\right]+\mbox{E}\left[\theta\mbox{E}\left[W\right]\right]\right]+\left[\mbox{Bias}\left(W\right)\right]^{2}\\
 & =\mbox{Var}\left(W\right)+2\left[\left(\mbox{E}\left[W\right]\right)^{2}-\theta\mbox{E}\left[W\right]-\left(\mbox{E}\left[W\right]\right)^{2}+\theta\mbox{E}\left[W\right]\right]+\left[\mbox{Bias}\left(W\right)\right]^{2}\\
 & =\mbox{Var}\left(W\right)+2\left(0\right)+\left[\mbox{Bias}\left(W\right)\right]^{2}\\
 & =\mbox{Var}\left(W\right)+\left[\mbox{Bias}\left(W\right)\right]^{2}.
\end{flalign*}

\end_inset

If 
\begin_inset Formula $W$
\end_inset

 is unbiased, 
\begin_inset Formula $\mbox{MSE}\left[W\right]=\mbox{Var}\left(W\right)$
\end_inset

.
 An estimator that has good MSE properties has small combined variance and
 bias.
 Controlling bias does not guarantee that MSE is controlled.
 There is often a bias-variance trade-off, such that a small increase in
 bias can be traded for a larger decrease in variance, resulting in an improveme
nt in MSE.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid 
\begin_inset Formula $\mbox{Bernoulli}\left(\theta\right)$
\end_inset

.
 Consider the statistics
\begin_inset Formula 
\[
T_{1}=\frac{\sum_{i}X_{i}+1}{n+2}\quad\mbox{and}\quad T_{2}=\frac{\sum_{i}X_{i}}{n}.
\]

\end_inset

Find the MSE for 
\begin_inset Formula $T_{1}$
\end_inset

 and 
\begin_inset Formula $T_{2}$
\end_inset

.
\end_layout

\begin_layout Example
Noting that the expected value of a 
\begin_inset Formula $\mbox{Bernoulli}\left(p\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $p$
\end_inset

, we will begin by finding the expected value of 
\begin_inset Formula $T_{1}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[T_{1}\right] & =\mbox{E}\left[\frac{\sum_{i}X_{i}+1}{n+2}\right]\\
 & =\frac{1}{n+2}\mbox{E}\left[\sum_{i=1}^{n}X_{i}+1\right]\\
 & =\frac{1}{n+2}\left(\sum_{i=1}^{n}\mbox{E}\left[X_{i}\right]+1\right)\\
 & =\frac{1}{n+2}\left(\sum_{i=1}^{n}\theta+1\right)\\
 & =\frac{n\theta+1}{n+2}.
\end{flalign*}

\end_inset

Then, the bias of 
\begin_inset Formula $T_{1}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{Bias}\left(T_{1}\right) & =\mbox{E}\left[T_{1}\right]-\theta\\
 & =\frac{n\theta+1}{n+2}-\frac{\theta\left(n+2\right)}{n+2}\\
 & =\frac{n\theta+1-n\theta-2\theta}{n+2}\\
 & =\frac{1-2\theta}{n+2}.
\end{flalign*}

\end_inset

Noting that the variance of a 
\begin_inset Formula $\mbox{Bernoulli}\left(p\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $p\left(1-p\right)$
\end_inset

, it follows that the variance of 
\begin_inset Formula $T_{1}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(T_{1}\right) & =\mbox{Var}\left(\frac{\sum_{i}X_{i}+1}{n+2}\right)\\
 & =\left(\frac{1}{n+2}\right)^{2}\mbox{Var}\left(\sum_{i=1}^{n}X_{i}+1\right)\\
 & =\frac{1}{\left(n+2\right)^{2}}\mbox{Var}\left(\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{\left(n+2\right)^{2}}\sum_{i=1}^{n}\mbox{Var}\left(X_{i}\right)\\
 & =\frac{1}{\left(n+2\right)^{2}}\sum_{i=1}^{n}\theta\left(1-\theta\right)\\
 & =\frac{n\theta\left(1-\theta\right)}{\left(n+2\right)^{2}},
\end{flalign*}

\end_inset

where the fourth equality follows from the independence of the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

.
 Then, the MSE for 
\begin_inset Formula $T_{1}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{MSE}\left(T_{1}\right) & =\mbox{Var}\left(T_{1}\right)+\left[\mbox{Bias}\left(T_{1}\right)\right]^{2}\\
 & =\frac{n\theta\left(1-\theta\right)}{\left(n+2\right)^{2}}+\left[\frac{1-2\theta}{n+2}\right]^{2}\\
 & =\frac{n\theta\left(1-\theta\right)+\left(1-2\theta\right)^{2}}{\left(n+2\right)^{2}}\\
 & =\frac{n\theta-n\theta^{2}+\left(1-4\theta+4\theta^{2}\right)}{\left(n+2\right)^{2}}\\
 & =\frac{\theta^{2}\left(4-n\right)-\theta\left(4-n\right)+1}{\left(n+2\right)^{2}}.
\end{flalign*}

\end_inset

The expected value of 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{E}\left[T_{2}\right] & =\mbox{E}\left[\frac{1}{n}\sum_{i=1}^{n}X_{i}\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\mbox{E}\left[X_{i}\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\theta\\
 & =\frac{n\theta}{n}\\
 & =\theta,
\end{flalign*}

\end_inset

so that we have 
\begin_inset Formula 
\[
\mbox{Bias}\left(T_{2}\right)=\mbox{E}\left[T_{2}\right]-\theta=\theta-\theta=0.
\]

\end_inset

The variance of 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\mbox{Var}\left(T_{2}\right) & =\mbox{Var}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{n^{2}}\mbox{Var}\left(\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{n^{2}}\sum_{i=1}^{n}\mbox{Var}\left(X_{i}\right)\\
 & =\frac{1}{n^{2}}\sum_{i=1}^{n}\theta\left(1-\theta\right)\\
 & =\frac{n\theta\left(1-\theta\right)}{n^{2}}\\
 & =\frac{\theta\left(1-\theta\right)}{n},
\end{flalign*}

\end_inset

so that the MSE for 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\[
\mbox{MSE}\left(T_{2}\right)=\mbox{Var}\left(T_{2}\right)+\left[\mbox{Bias}\left(T_{2}\right)^{2}\right]=\frac{\theta\left(1-\theta\right)}{n}+0=\frac{\theta\left(1-\theta\right)}{n}.
\]

\end_inset


\end_layout

\begin_layout Remark
The MSEs may differ based on how large 
\begin_inset Formula $n$
\end_inset

 is, i.e., one may be superior for certain ranges of 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Subsection
Best unbiased estimators
\end_layout

\begin_layout Standard
A common way to make the problem of finding a 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 estimator tractable is to limit the class of estimators to unbiased estimators.
\end_layout

\begin_layout Definition
An estimator 
\begin_inset Formula $W^{*}$
\end_inset

 is a 
\shape italic
best unbiased estimator
\shape default
 of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

 if it satisfies 
\begin_inset Formula $\mbox{E}_{\theta}\left[W^{*}\right]=\tau\left(\theta\right)$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

 and, for any other estimator 
\begin_inset Formula $W$
\end_inset

 with 
\begin_inset Formula $\mbox{E}_{\theta}\left[W\right]=\tau\left(\theta\right)$
\end_inset

, we have 
\begin_inset Formula $\mbox{Var}_{\theta}\left(W^{*}\right)\leq\mbox{Var}_{\theta}\left(W\right)$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

.
 
\begin_inset Formula $W^{*}$
\end_inset

 is also called a 
\shape italic
uniform minimum variance unbiased estimator
\shape default
 (UMVUE) of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Cramr-Rao Inequality]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be a sample with pdf 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

, and let 
\begin_inset Formula $W\left(\mathbf{X}\right)=W\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 be any estimator satisfying
\begin_inset Formula 
\[
\frac{\mbox{d}}{\mbox{d}\theta}\E_{\theta}\left[W\left(\mathbf{X}\right)\right]=\int_{\mathcal{X}}\frac{\partial}{\partial\theta}\left[W\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta\right)\right]\mbox{d}x\quad\mbox{and}\quad\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)<\infty.
\]

\end_inset

Then
\begin_inset Formula 
\[
\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\frac{\mbox{d}}{\mbox{d}\theta}\E_{\theta}\left[W\left(\mathbf{X}\right)\right]\right]^{2}}{\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}=\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}
\]

\end_inset

with Fisher information
\begin_inset Formula 
\[
I=\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]=-\E_{\theta}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right].
\]

\end_inset

(This is Theorem 7.3.9 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
Under independence, 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}f_{i}\left(x_{i}|\theta\right)$
\end_inset

.
\end_layout

\end_body
\end_document
